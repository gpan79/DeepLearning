{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_House_Price_Prediction_RandomizedSearchCV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNN1eSQo2b4a356qAQKbmmv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gpan79/DeepLearning/blob/main/ANN_House_Price_Prediction_RandomizedSearchCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb8N_UBgwoWV",
        "outputId": "5fcc1e2d-7ec9-4424-c924-6d5abc80ce9c"
      },
      "source": [
        "import sklearn\n",
        "if (sklearn.__version__ != '0.24.2'):\n",
        "  !pip uninstall scikit-learn -y\n",
        "  !pip install -U scikit-learn\n",
        "else:\n",
        "  print(\"Scikit-Learn updated to last version: \", sklearn.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scikit-Learn updated to last version:  0.24.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ6Wd7aUq9X1"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import mean_absolute_percentage_error, explained_variance_score\n",
        "#from sklearn.metrics import mean_squared_error, explained_variance_score\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from datetime import datetime\n",
        "import pytz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaaTFGxHutXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe3800e-6449-4027-f2a1-883f1097486b"
      },
      "source": [
        "# Verifica Accelerazione GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q24xjxJuuQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7857ea6c-1458-45c1-8977-f462ac6c5109"
      },
      "source": [
        "# Monta Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR_IQAEG0aj9",
        "outputId": "63530f81-fb40-4dc4-cacb-15375f394905"
      },
      "source": [
        "# Save filename\n",
        "# pytz TimeZone Localization\n",
        "tz_EuRome = pytz.timezone('Europe/Rome')\n",
        "datetime_EuRome = datetime.now(tz_EuRome)\n",
        "current_time = datetime_EuRome.strftime(\"%m-%d-%Y_%H-%M\")\n",
        "#print(current_time)\n",
        "saveFileName = \"model_saved_\" + current_time + \".h5\"\n",
        "pathSaveModel = F\"/content/gdrive/My Drive/MaBiDa_2021/saved_models/{saveFileName}\"\n",
        "print(\"Best Model will be saved as file: \", pathSaveModel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Model will be saved as file:  /content/gdrive/My Drive/MaBiDa_2021/saved_models/model_saved_05-16-2021_23-50.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rftksAkfvIw_"
      },
      "source": [
        "# Importa il dataset\n",
        "from google.colab import files\n",
        "import io\n",
        "import os\n",
        "\n",
        "df = pd.read_csv('/content/gdrive/My Drive/MaBiDa_2021/dataset/kc_house_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "EB_UXrooxfnB",
        "outputId": "e565892d-89e4-40d5-9d15-5fe043575853"
      },
      "source": [
        "print(\"Features: \", df.columns.values)\n",
        "\n",
        "# Visualizza una preview dei dati\n",
        "print(df.head())\n",
        "\n",
        "# Cancella righe con dati mancanti o NaN\n",
        "df.dropna(axis=0, how='any')\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "ax[0].hist(df['price'], bins=100, color='#12947e', edgecolor='white')\n",
        "ax[1].scatter(df['price'], df['sqft_living'], color='#12947e', edgecolors='white')\n",
        "ax[0].set(xlabel='House Price (Millions US Dollars)', ylabel='', title='House Price Distribution')\n",
        "ax[1].set(xlabel='House Price (Millions US Dollars)', ylabel='Sqft Living',\n",
        "          title='Scatter Plot: House Price VS Square Foots')\n",
        "ax[0].grid(color='#b8bfbd', linestyle='-.', linewidth=0.7)\n",
        "ax[1].grid(color='#b8bfbd', linestyle='-.', linewidth=0.7)\n",
        "plt.show(block=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  ['id' 'date' 'price' 'bedrooms' 'bathrooms' 'sqft_living' 'sqft_lot'\n",
            " 'floors' 'waterfront' 'view' 'condition' 'grade' 'sqft_above'\n",
            " 'sqft_basement' 'yr_built' 'yr_renovated' 'zipcode' 'lat' 'long'\n",
            " 'sqft_living15' 'sqft_lot15']\n",
            "           id             date     price  ...     long  sqft_living15  sqft_lot15\n",
            "0  7129300520  20141013T000000  221900.0  ... -122.257           1340        5650\n",
            "1  6414100192  20141209T000000  538000.0  ... -122.319           1690        7639\n",
            "2  5631500400  20150225T000000  180000.0  ... -122.233           2720        8062\n",
            "3  2487200875  20141209T000000  604000.0  ... -122.393           1360        5000\n",
            "4  1954400510  20150218T000000  510000.0  ... -122.045           1800        7503\n",
            "\n",
            "[5 rows x 21 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzsAAAGDCAYAAAAMKQ4eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9e3xcV3mo/bySRjdLlixZtqSRZMmWHV+TkARI27TwUQiQFkJ7TjiQHC6nEMpHaOkplEJLCZfC6YUPeoNSUlLCIeESoC2loSRQWpoWBxwT32+yJVuW5Zts3SxZGnve74+9NZ6RR9LIXvLM2n6f329+Gq21197PrL1n73nXWnttUVUMwzAMwzAMwzCiRlG+BQzDMAzDMAzDMBYCC3YMwzAMwzAMw4gkFuwYhmEYhmEYhhFJLNgxDMMwDMMwDCOSWLBjGIZhGIZhGEYksWDHMAzDMAzDMIxIYsGOYcyAiPyeiPxtHrbbJiKjIlLsaH2fFZE/CN+/WESOulhvuL6fF5F9rtZnGIaxkIjIh0TkS/n28AkR2SUiL863h2FcKRbsGE4QkR4Reem0tDeLyNP5cppO6HMxDCSGReQ5EfnlmZZX1Y+r6lsX0GFURLpF5O9EZE3ado+oapWqXsxhXXPWr6q+XVU/6shfRaQzbd3/oao3uFi3YRiFg4jcISL/JSJDInJGRP5TRJ5/leu87JwlIl8QkT+8OtvLtvMFEZkMz7FnROQpEVl7Beu57Lo2x/JZG5NE5N9ExOm15GoIfc6H9XNaRL4pIk0zLa+qG1T13xxu/7Mi8sUs6TeJyISI1IlIrYg8LCLHRWRERPaLyPtmWedbRGRvuOwJEXlCRKpdOS8U4TE2nvabYFREmq9ynRnXacOCHeP640eqWgXUAp8HviYiS6YvJCIl18ChBngpMA48KyIbXW/IVe+QYRjXDyKyGPg28JdAHRAHPgxM5NMrG7Ocq/8kPM+2ACeBL1wzKT94Z1g/awiuh5+avsACXgcfAX5VRBZNS38D8G1VPRP6VAHrCK6Vrwa6sq1MRF4EfBx4vapWh2W+ukDuM3IV9fWqsIFz6nXMqZhhwY5x7RCRdWGL0mDYLf7qtLyMlq/0FkAJ+JSInAx7ZHZMBQYiUiYinxCRI2FrzmdFpGIuF1VNAg8DFcCqcGjD10XkSyIyDLx5+nCHtJbOQRHpFZE3X6XDRVU9qKrvAP4d+FC4vvawZaYkrS4OhS1W3SJyn4isAz4L/EzYEjQYLvsFEfnrsFXrHPD/ZGs5lWCI3umwVem+HPfDD8PkbeE2/8f0lsw59vEXROTTIvLP4Wd5RkRWzVVPhmFcc9YAqOqXw/PUuKo+qarbpxYQkftFZE/4Xd4tIreE6e8TkYNp6b8Spl92zhKRtwH3Ae8N0/4pXLZZRL4hIqfCc95vpm33snP1bB9EVceAx4CsjUki8urwXDUYnrvWhen/F2gD/il0e++VVWXWbd4vIl0S9Dp9S8KW/Onn/jAtdU4WkU4R+XcJettOi8hX05ZbK0EP1hkR2Scir83FJQwsvkFYP+E14XdFZDtwTkRKJK2HS0SKw+vH1D5+VkRa5+Ogqj8C+oD/luZfDNwLTPX4PB94TFXPqmpSVfeq6tdn+BjPJ2hE/OnUZ1LVR1R1JFx3fVjPwyLyYxH5aNp1ba46XyUi/yoiA2GdPyoitWnLZquv2+XSb4VtcgVDACX4XfFnInIsfP2ZiJSl5c90DGW7Ti8VkW+HPmdE5D9E5Lr6/X9dfVgjf4hIDPgn4ElgGfAbwKMikssQqDuBXyC4ANcArwUGwrw/CtNvBjoJWiA/mINPCfBWYBQ4ECbfDXydoJXr0WnLrwC+Q9DS2RBu77mrcZjGN4Gfz+K5CPgL4JVhi9XPAs+p6h7g7YS9RKpam1bsXuBjQDWQbZhbI7A09HwT8Llc9oOq/kL49qZwmxktZznu49cRtBAvIWil+9hc2zUM45qzH7goIo+IyCtlWu+3iNxD0DjzRmAxQav71Dn5IMG5rIbgu/4lEWnKds5S1c8RnGv/JEx7Vfgj7J+AbQTnqF8EfktEXp6mMOO5ejoiUkUQUP00S94a4MvAbxGc158gCG5KVfUNwBEutbr/SVhmu4jcO1cFzuLzEuD/EFzHmoDDwFdyLP5RgvPrEoIeq78M17kIeIogqFtGcJ79jIisz8FnKUHQkV4/rwd+CahV1QvTivx2mH8Xwb7/NWDsChy+SHD8TPFSIEawDwA2Ax8Tkf8lIqvn+BjPAC8XkQ+LyM+lBwUhnwbOE9T3r4WvXBGC/dVM0GPUStgwmUaqvoDlwD8Df0jQK/oe4Bsi0jCPbQL8PnA7we+Km4AXAB+A2Y+hGa7T7waOEhzjy4HfA3SePl5jwY7hkn8IWw4GJehp+Exa3u0EXdJ/pKqTqvqvBMMkXp/DehMEP9zXAqKqe1S1X0QEeBvwv8OWnBGCruzXzbKu20O34+G2f0VVh8K8H6nqP4StSOPTyt0LfC9s6Uyo6oCqPneFDtk4RnBizEYS2CgiFarar6q75ljXP6rqf4af4/wMy/yBqk6o6r8TnJhzagWcg1z28d+r6o/DC+ijBCdywzAKCFUdBu4g+EH0EHAqbD1eHi7yVoIA5Sca0KWqh8Oyj6vqsfD881WCxqQXzGPzzwcaVPUj4XnkUOiQfk6d7Vw9xXvCc30XwXnpzVmW+R/AP6vqU6qaAD5B0Nv/szPJqeqNqvrYLP7N6dfB0OGOtPz7gIdVdauqTgDvJ+jtap9lnVMkgBVAs6qeV9WpxqxfBnpU9e9U9ULYw/EN4J5Z1vUXods2oJ8giEnlqWrvDHX7VuADqrov3PfbVHXgChz+L/AiEWkJ/38jQU9OIvz/NwiuEe8Edoe9GK/MtiJV/Q/gV4FbCK5nAyLyybAXqpggmPugqp5T1Z0Ew+hyIjy2nwqvl6eATwIvmrZYen39T+AJVX0iPD6fArYQBIczkf7b6R/CtPuAj6jqyXC7HyYY5jeVN59jKEEQFK0If7/8h6pasGMYV8hrwta62rCn4R1pec1ArwbDx6Y4TNByNyvhj+a/ImidOSkin5NgTHkDUElwv8vUReVfwvSZ2Bz6LVXV21X1e2l5vbOUayVosZzOlThkIw6cmZ6oqucILshvB/olGAI21422s30OgLPheqc4TLB/rpZc9vHxtPdjBD9CDMMoMMJGpTeragvBEKdm4M/C7JnOh4jIGyWY/GXqfLiRoCc5V1YwLWAgaIlenrbMXOc4gE+E5/pGVX21qmbzbSY4RwGp4c295HBdmoVj6dfB8FqY3sM+fZujBL1iuWzzvQQ9DT+WYOjdVA/FCuCF0+rsPoJe/Jn4zdAvrqr3hT+op7iSa+G8HFT1CPBD4H+GvW+v4dIQNjQYOvlxVb0VqAe+BjwuIlkbBVX1O6r6KoJGw7sJgtu3ElyLS6Z9psOXrWAGRGS5iHxFRPokGDb5JS4/ntPXvQK4J0uwO+MEEGT+dnpNmJZxnJB5nZ7vMfSnBEH/kxIMiZ9xooeoYsGOca04BrROGyfaRjBuF+AcQdAwRcYJUlX/IjzprScYMvY7wGmCm/s3pJ0oajS46fJKmK2loxfIdn+JK4dfAf4jq5Tqd1X1ZQQny70ErZyz+c7VYrNEMm8MbSPYPzDHfpiDufaxYRgeoqp7CW7wn7rvJev5MBzu+xBBa3x9+EN/J8EPdMh+bpqe1gt0TwsYqlX1rlnKXCnHCH6cTvkLwY/5qXPWQrR+T9/mIoIf830E51+Y4RysqsdV9X5VbQZ+nWCYWCdBnf37tDqrUtX/9wodr+RaeCUOjxD0Vvw3gn3+bFaZoKfx48AioGNW8aA35fvAvxIcr6eACwT7dYq2tPez1nm4XQU2qepigp4bIZP0+uoF/u+0elikqn80m3cWMo4TMq/Tsx1Dl6GqI6r6blVdSTDk9LdF5Bfn6eM1FuwY14pnCFry3ysiMQlu2HsVl8YqP0cwO0tlePJ+y1RBEXm+iLwwvCfkHMHY22TYCvcQ8CkRWRYuG582ttsVjwIvFZHXSnADYr2I3Hw1DmEXe4eI/CXwYoJu6unLLBeRu8OT2QTBPUZTPScngBYRKb2Cz/NhESkVkZ8nGH7weJg+435I2+bKGdY51z42DMMDJLjR/N1TQ4wkuAH99QT3UQD8LcEwsVsloDMMdBYR/PA7FZb7X2RODJDtnDX9nPJjYESCm74rwvPkRrnKaa9n4GvAL4nIL4bXl3cTnGf/awY3F3wZ+F8icrME95Z8HHhGVXvC3pU+gt6O4rDnJhVYiMg9acO+zhLUdZJguPAaEXlDeO6NhdfNdY7dIdj3HxWR1eG+v1FE6q/Q4RsEP+I/zLShZSLyB2H5UhEpB94FDAKXPdctvEa+TkSWhE4vIBhqtlmDRzh8E/hQeF1bT3CvKgBz1TnBEPpRYEhE4gQNrbPxJeBVIvLycH3lEkzk0zJHuel8GfiAiDRIcF/VB8N1T+VlPYbC/IzjVkR+OfyOCjAEXOTS74jrAgt2jGuCqk4S/PB9JUFvyGeAN4YthhBMMzlJ8CV9hMybThcTBBRnCbpuBwi6ZQF+l6B7dnPYxfw9wPlzX8Iu97sILoZnCIKCm67Q4WdEZBQYBv6N4PM9X1V3ZFm2iGAs9bFwuy8CplrK/hXYBRwXkdPz+DjHCeryGEE9vz3H/QDBjZmPhN3zGff55LCPDcPwgxHghcAzEszquJmgh+bdENyXQzC5yGPhsv8A1KnqbuD/A35EcA7ZBPxn2nqznbM+D6wPzyn/EP44/WWC+/m6Cc4lf0sw4YFTVHUfQUv9X4bbeRXBhAST4SL/h+AH56CIvAdSD9i8L+sKc9vm94A/IPih30/wwzr9fqT7CX5QDwAbuBR4QXA/0zPh9eNbwLtU9ZAG94reGa7nGME5/o+B6Tfqu+CTBEHikwTXsM8DFVfiEA6n/gbBZAvTrzUK/B3BfjkGvAz4pXDI1nTOEtTbgdDpS8CfqurUOt9JMGT6OEEP5d9NKz9bnX+Y4F6gIYL7gb450+cJP1MvwTC63yMI+nvDdc/39/YfEtzrsx3YAWwN03I5hj5E5nV6NcHvklGC7+ZnVPUH8/TxGrnO7lEyDMMwDMMwrlMkeGzEW1X1jrmWNaKB9ewYhmEYhmEYhhFJLNgxDMMwDMMwDCOS2DA2wzAMwzAMwzAiifXsGIZhGIZhGIYRSSzYMQzDMAzDMAwjkpTkW2A2li5dqu3t7fnWMAzDuO559tlnT6tqQ749ChG7VhmGYeSfma5TBR3stLe3s2XLlnmV6erpprN91gfsFgy+uJqne3xxNU/3+OI63VNEDudRZ8rhYYJnsJxU1Y3T8t4NfAJoUNXT4QP0/pzg+VhjwJtVdWu47JuAD4RF/1BVHwnTbyV4DkcF8ATBc0zmvLE1ytcq83SPL67m6R5fXH31nOk6VdATFNx222063wvIxYsXKS4uXiAjt/jiap7u8cXVPN3ji+t0TxF5VlVvy6MSIvILBA/G+2J6sCMirQQPnlwL3BoGO3cBv0EQ7LwQ+HNVfaGI1BE8rO82ggcXPhuWOSsiPwZ+E3iGINj5C1X9zlxeUb5Wmad7fHE1T/f44uqr50zXqcjds7Nt1858K+SML67m6R5fXM3TPb64FqKnqv4QOJMl61PAewmClynuJgiKVFU3A7Ui0gS8HHhKVc+o6lngKeAVYd5iVd0c9uZ8EXjNQn2WQqzfbJine3xxNU/3+OIaNc+CHsZmGIZhGLMhIncDfaq6LRi5liIO9Kb9fzRMmy39aJb0mbb7NuBtAPGWOFu3b0vP5JZNN9LV082KllZ27N6VUbY13sL5iQlGz53j5MBpBgcHU3nl5eWsWdVJb18fDfV17D94MKPsmlWrODVwhtZ4nP0Huzh//nwqr7a2lmX1Sxk5N0pJcQm9fUczym5av4HDR3vpbO9g647tkDayo3H5ckpjpRQXFzN6bpRTp08DMDQyzLbdu7hp/Qa6erppaWpm9769Gettb2tjbHyc+iV1HO0/xsjISCpv0aJFtLe2ceLUSRZXV3Oopyej7NrVazh24jid7R3s2LubxGQilVdfV0dtTQ0TE5MkNcmx/v6Msjdv3MShI4fpaG1jaGQ4Yx/Em5oBqCgv58zgIGfOXoqRS8tKWb/6Brp7j9DYsIx9XQcy1ruqo4PBoSGaljdy6HAPY2NjqbzFixfTvLyRs0ODlJeVc7j3SEbZDWvX0Xusj872Dp7btZPkxYupvGUNDVRWVDCZSNB3vJ8TJ0+m8qSoiOdt3ERXTzdt8RZ27tmdsd62llYmE5PUVC/m+KmTDA0NpfIqKiro7FhJX38/dbW1dHUfyii7ZlUnJwdOsyLewt6uA0xMTKTyltQuoaG+jtGxMYqkiKPH+lJ5QyPDXLh4kZ7eI8Exk36MA02NjZQUlxCLlTA8MsLpgYFUXiwWY9O69XT1dBNvbGLP/n0ZZTtWtDN6bpSG+qX09h1lZHQ0lVddVUVrvIVTA6epWlRF9+GejLLr1txA3/H+4JjZsztj3y+tr2dxdTWJxAUuXLxA//HjGWVvufEmunq6aW9tY/u0H8otzXGSmqSqspJTA2c4O3g2lVdWVsbaztUc7jvKsvql7D/YlVG2s2MlZwYHiTc10dV9iPHx8VReTU0NjQ3LGBoZZjIxeVk9bly3niN9R+ls7+CnO3egyWQqb/myZZSXlSMCY+PjnDx1KpVXVFzMzRs20tXTTWtznF1792Ssd0VrG+cnzrOkppZjJ44zPDycyqusrGTlinb6TxyntqaGg93dGWUvXrzIwcM9dLS2sfvAPiYnJlN5dUvqqKutZTw89/T1H8soe9OGjXT3HmFl2wqe27kjI6+5qYkiKaKsrJTBoSEGzlz6XsZKY2xaGxwzzcsb2Xtgf0bZle3tDI+MsLxhGT29Rzh37lxq31dXV9MSfuezoqoF+7r11lt1vjy77bl5l8kXvriap3t8cTVP9/jiOt0T2KIFcF0A2oGd4ftKgiFnNeH/PcDS8P23gTvSyn2fYOjae4APpKX/QZh2G/C9tPSfB76di1OUr1Xm6R5fXM3TPb64+uo503UqcsPYDMMwjOuGVUAHsE1EeoAWYKuINAJ9QGvasi1h2mzpLVnSDcMwDI+JXLATn60bq8DwxdU83eOLq3m6xxdXHzxVdYeqLlPVdlVtJxh6douqHge+BbxRAm4HhlS1H/gucKeILBGRJcCdwHfDvGERuT2cye2NwD8ulLsP9QvmuRD44mqe7vHFNWqekQt2DMMwjGgiIl8GfgTcICJHReQtsyz+BHAI6AIeAt4BoKpngI8CPwlfHwnTCJf527DMQWDOmdgMwzCMwiZyExRUlJfnWyFnfHE1T/f44mqe7vHFtRA9VfX1c+S3p71X4IEZlnsYeDhL+hZg4+Ul3FOI9ZsN83SPL67m6R5fXKPmGbmenTNps9oUOr64mqd7fHE1T/f44uqLp6/4Ur/m6R5fXM3TPb64Rs0zesHO2WyPYChMfHE1T/f44mqe7vHF1RdPX/GlfqPmmdQko5MTJFXDv8m5CzkmanWab3zxBH9co+YZuWFshmEYhmEY00lqktPjY9z/5ONs7j/C7U1tPHTnPSytqKRIItf2axhGiH27DcMwDMOIPGOJBPc/+ThP9/VwIZnk6b4e7n/yccYSibkLG4bhLZELdkrLSvOtkDO+uJqne3xxNU/3+OLqi6ev+FK/UfKsjJWyuf9IRtrm/iNUxq7tZ4xSnRYCvniCP65R85RgwprC5LbbbtMtW7bMq0wymaSoaH4x3PkLCcpLYjP+v1BciWs+ME/3+OJqnu7xxXW6p4g8q6q35VGpYLlW16p8ECXP0ckJ7nviMZ7u60ml3RFv59G77qWqtGyBDS8RpTotBHzxBH9cffWc6TpV+J9knnT3Hpl7oWmUl8So//SDqde1CHTgylzzgXm6xxdX83SPL66+ePqKL/UbJc/KWIyH7ryHO+LtlBQVcUe8nYfuvIfK2LW55k8RpTotBHzxBH9co+YZuQkKGhuW5VshZ3xxNU/3+OJqnu7xxdUXT1/xpX6j5FkkRSytqOTRu+6lMlbKWGKSyljsmk9OEKU6LQR88QR/XKPmGbmenX1dB/KtkDO+uJqne3xxNU/3+OLqi6ev+FK/UfMskiKqSssoEgn/XvufQVGr03zjiyf44xo1z8gFO4ZhGIZhGIZhGGDBjmEYhmEYhmEYEcWCHcMwDMMwDMMwIknkgp1VHR35VsgZX1zN0z2+uJqne3xx9cXTV3ypX/N0jy+u5ukeX1yj5hm5YGdwaCjfCjnji6t5uscXV/N0jy+uvnj6ii/1a57u8cXVPN3ji2vUPCMX7DQtb8y3Qs744mqe7vHF1Tzd44urL56+4kv9mqd7fHE1T/f44ho1z8gFO4cO9+RbIWd8cTVP9/jiap7u8cXVF09f8aV+zdM9vriap3t8cY2aZ+SCnbGxsXwr5IwvrubpHl9czdM9vrj64ukrvtSvebrHF1fzdI8vrlHzjFywYxiGYRiGYRiGARbsGIZhGIZhGIYRUSIX7CxevDjfCjnji6t5uscXV/N0jy+uvnj6ii/1a57u8cXVPN3ji2vUPCMX7DQ7mEHi/IVE1veuceF6LTBP9/jiap7u8cXVF09f8aV+zdM9vriap3t8cY2aZ+SCnbNDg1e9jvKSGPWffpD6Tz9IeUnMgVV2XLheC8zTPb64mqd7fHH1xdNXfKlf83SPL67m6R5fXKPmGblgp7ysPN8KOeOLq3m6xxdX83SPL66+ePqKL/Vrnu7xxdU83eOLa9Q8IxfsHO49ktNyCzk8LVdydc035ukeX1zN0z2+uPri6Su+1K95uscXV/N0jy+uUfOMXLCTK+lD1QzDMAzDMAzDiB7XbbBjGIZhGIZhGEa0sWDHMAzDMAzDMIxIErlgZ8PadflWyBlfXM3TPb64mqd7fHH1xdNXfKlf83SPL67m6R5fXKPmOWewIyLlIvJjEdkmIrtE5MNheoeIPCMiXSLyVREpDdPLwv+7wvz2tHW9P0zfJyIvv6JPNge9x/oWYrULgi+u5ukeX1zN0z2+uPri6Su+1K95uscXV/N0jy+uUfPMpWdnAniJqt4E3Ay8QkRuB/4Y+JSqdgJngbeEy78FOBumfypcDhFZD7wO2AC8AviMiBTn/IlypLO9w/UqFwxfXM3TPb64mqd7fHH1xdNXfKlf83SPL67m6R5fXKPmOWewowGj4b+x8KXAS4Cvh+mPAK8J398d/k+Y/4siImH6V1R1QlW7gS7gBTlZzoPndu10vcoFwxdX83SPL67m6R5fXH3x9BVf6tc83eOLq3m6xxfXqHnmdM+OiBSLyHPASeAp4CAwqKoXwkWOAvHwfRzoBQjzh4D69PQsZZyRvHjR9SoXDF9czdM9vriap3t8cfXF01d8qV/zdI8vrubpHl9co+ZZkstCqnoRuFlEaoG/B9ZeudrsiMjbgLcBxFvibN2+LT2TWzbdSFdPNytaWtmxe1dG2dZ4C+cnJhg9d46TA6cZHBxM5ZWXl7NmVSe9fX001Nex/+BB3tO2EYCt27cRL6vk5qo6vnfmGLv378vIq62tZVn9UkbOjVJSXEJv39GM7W5av4HDR3vpbO9g647toJrKa1y+nNJYKcXFxYyeG+XU6dOpvOHREQC6erppaWpm9769Gettb2tjbHyc+iV1HO0/xsjISCpv0aJFtLe2ceLUSRZXV3Oopyej7NrVazh24jid7R3s2LubxOSlh6jW19VRW1PDxMQkSU1yrL8/o+zNGzdx6MhhOlrb2LZrJ0Mjw6n9EG9qBqCivJwzg4OcOXsmVa60rJT1q2+gu/cIjQ3L2Nd1IGO9qzo6GBwaoml5I4cO9zA2NpbKW7x4Mc3LGzk7NEh5WfllD4rasHYdvcf66Gzv4LldOzMO8GUNDVRWVDCZSNB3vJ8TJ0+m8qSoiOdt3ERXTzdt8RZ27tmdsd62llYmE5PUVC/m+KmTDA0NpfIqKiro7FhJX38/dbW1dHUfyii7ZlUnJwdOsyLewt6uA0xMTKTyltQuoaG+jtGxMYqkiKPTxpWqKl093cExk36MA02NjZQUlxCLlTA8MsLpgYFUXiwWY9O69XT1dBNvbGLP/n0ZZTtWtDN6bpSG+qX09h1lZHQ0lVddVUVrvIVTA6epWlRF9+GejLLr1txA3/H+4JjZs5tEIpHa90vr61lcXU0icYELFy/Qf/x4RtlbbryJrp5u2lvb2D6tpaWlOU5Sk1RVVnJq4AxnB8+m8srKyljbuZrDfUdZVr+U/Qe7Msp2dqzkzOAg8aYmuroPMT4+nsqrqamhsWEZQyPDTCYmL6vHjevWc6TvKJ3tHfx05w40mUzlLV+2jPKyckRgbHyck6dOpfKKiou5ecNGunq6aW2Os2vvnoz1rmht4/zEeZbU1HLsxHGGh4dTeZWVlaxc0U7/iePU1tRwsLs7o+wNnas5Nz5GMplk94F9TE5MpvLqltRRV1vL+PnzAPT1H8soe9OGjXT3HmFl2wqe27kjI6+5qYkiKaKsrJTBoSEGzlz6XsZKY2xaGxwzzcsb2Xtgf0bZle3tDI+MsLxhGT29Rzh37hwAQyPDHOg+REtTMwNp33PDMAzD8ApVndcL+CDwO8BpoCRM+xngu+H77wI/E74vCZcT4P3A+9PWk1puptett96q8+XZbc/lvGzdX31Q6/7qgxnvp/+/kMzHNZ+Yp3t8cTVP9/jiOt0T2KLzvF5cL6+FvlblE/N0jy+u5ukeX1x99ZzpOpXLbGwNYY8OIlIBvAzYA/wA+O/hYm8C/jF8/63wf8L8fw0FvgW8LpytrQNYDfx4/uHZ7CxraHC9ygXDF1fzdI8vrubpHl9cffH0FV/q1zzd44urebrHF9eoeeZyz04T8AMR2Q78BHhKVb8N/C7w2yLSRXBPzufD5T8P1Ifpvw28D0BVdwFfA3YD/wI8oMHwOKdUVlS4XuWC4YurebrHF1fzdI8vroXoKSIPi8hJEdmZlvanIrJXRLaLyN9PNc6FeVkfdyAirwjTukTkfWnpWR+psBAUYv1mwzzd44urebrHF9dr7ZnUJKOTEyRVw7/Jucx+P50AACAASURBVAuRu2cus7FtV9XnqeqNqrpRVT8Sph9S1Reoaqeq3qOqE2H6+fD/zjD/UNq6Pqaqq1T1BlX9Tk6G8yTtdpmCxxdX83SPL67m6R5fXAvU8wsEjy5I5ylgo6reCOwnGDI94+MOwkcefBp4JbAeeH24LMz8SAXnFGj9XoZ5uscXV/N0jy+u19IzqUlOj49x3xOP0fTZj3DfE49xenwsp4AnV8+cZmPzifMT5/OtkDO+uJqne3xxNU/3+OJaiJ6q+kPgzLS0J/XSzKCbgZbw/UyPO3gB0BU22E0CXwHuDh+RMNMjFZxTiPWbDfN0jy+u5ukeX1yvpedYIsH9Tz7O0309XEgmebqvh/uffJyxRGLOsrl6Ri7YSZ+Jq9DxxdU83eOLq3m6xxdXXzyn8WvA1KiBmR53MFN6PTM/UsE5vtSvebrHF1fzdI8vrtfSszJWyub+zBl4N/cfoTI29yjiXD1zmnraMAzDMAoZEfl94ALw6DXa3jV7TEI6a1at4tTAGVrjcfYf7OL8+UstmwvxmIShkWG27d7FTes3FPRjEtIfkQD2mIR0rvQxCUMjw1y4eJGe3iMF/ZiE9H1vj0m4xNU8JuHixYscPNxDR2vbgj8m4YIm+UjnrRweHeZvju3jVxpWcL48xpbtz1Eil/pksj0mYWrfV1dX0xJ+57OSbYq2QnnZ1NOFgXm6xxdX83SPL66FOvU00A7snJb2ZuBHQGVaWtbHHZD2qIT05QgekZD1kQpzvWzq6fzji6eqP67m6R5fXK+l58XkRT1xbkRf/fcP67LPfEhf/fcP64lzI3oxeXHOsrlepyLXsyNF/ozM88XVPN3ji6t5uscXV288RV4BvBd4kaqOpWV9C3hMRD4JNHPpcQcCrA4fgdBHMInBvaqqIjL1SIWvkPlIBffevtSveTrHF1fzdI8vrtfSs0iKWFpRyaN33UtlrJSxxCSVsRhFMrdDrp6iBTw1xG233aZbtmxZsPXXf/pBAAYe+HDq/fT/Bx748IJt3zAMwxdE5FlVvS3PDl8GXgwsBU4ADxL0ypQBU2NoNqvq28Plf5/gPp4LwG9pOAuoiNwF/BlQDDysqh8L01cSBDp1wE+B/6nhTKOzsdDXKsMwDGNuZrpO+RFizoOunu65FyoQfHE1T/f44mqe7vHFtRA9VfX1qtqkqjFVbVHVz2vwmINWVb05fL09bfmsjztQ1SdUdU2Y97G09KyPVFgICrF+s2Ge7vHF1Tzd44tr1DwjF+y0xVvmXqhA8MXVPN3ji6t5uscXV188fcWX+jVP9/jiap7u8cU1ap6RC3amz2BSyPjiap7u8cXVPN3ji6svnr7iS/2ap3t8cTVP9/jiGjXPyAU7hmEYhmEYhmEYYMGOYRiGYRiGYRgRxYIdwzAMwzAMwzAiSeSCnbaW1nwr5IwvrubpHl9czdM9vrj64ukrvtSvebrHF1fzdI8vrlHzjFywM5mYzLdCzvjiap7u8cXVPN3ji6svnr7iS/2ap3t8cTVP9/jiGjXPyAU7NdWL862QM764mqd7fHE1T/f44uqLp6/4Ur/m6R5fXM3TPb64Rs0zcsHO8VMn862QM764mqd7fHE1T/f44uqLp6/4Ur/m6R5fXM3TPb64Rs0zcsHO0NBQvhVyxhdX83SPL67m6R5fXH3x9BVf6tc83eOLq3m6xxfXqHlGLtgxDMMwDMMwDMMAC3YMwzAMwzAMw4gokQt2Kioq8q2QM764mqd7fHE1T/f44uqLp6/4Ur/m6R5fXM3TPb64Rs0zcsFOZ8fKfCvkjC+u5ukeX1zN0z2+uPri6Su+1K95uscXV/N0jy+uUfOMXLDT19+fb4Wc8cXVPN3ji6t5uscXV188fcWX+jVP9/jiap7u8cU1ap6RC3bqamvzrZAzvriap3t8cTVP9/ji6ounr/hSv+bpHl9czdM9vrhGzTNywU5X96F8K+SML67m6R5fXM3TPb64+uLpK77Ur3m6xxdX83SPL65R84xcsGMYhmEYhmEYhgEW7BiGYRiGYRiGEVEs2DEMwzAMwzAMI5JELthZs6oz3wo544urebrHF1fzdI8vrr54+oov9Wue7vHF1Tzd44tr1DwjF+ycHDidb4Wc8cXVPN3ji6t5uscXV188fcWX+jVP9/jiap7u8cU1ap6RC3ZWxFvyrZAzvriap3t8cTVP9/ji6ounr/hSv+bpHl9czdM9vrhGzTNywc7ergP5VsgZX1zN0z2+uJqne3xx9cXTV3ypX/N0jy+u5ukeX1yj5hm5YGdiYiLfCjnji6t5uscXV/N0jy+uvnj6ii/1a57u8cXVPN3ji2vUPCMX7BiGYRiGYRiGYYAFO4ZhGIZhGIZhRJTIBTtLapfkWyFnfHE1T/f44mqe7vHF1RdPX/Glfs3TPb64mqd7fHGNmmfkgp2G+rp8K+SML67m6R5fXM3TPb64+uLpK77Ur3m6xxdX83SPL65R84xcsDM6NpZvhZzxxdU83eOLq3m6xxdXXzx9xZf6NU/3+OJqnu7xxTVqnpELdorEn4/ki6t5uscXV/N0jy+uvnj6ii/1a57u8cXVPN3ji2vUPP34NPPg6LG+fCvkjC+u5ukeX1zN0z2+uPri6Su+1K95uscXV/N0jy+uUfOcM9gRkVYR+YGI7BaRXSLyrjD9QyLSJyLPha+70sq8X0S6RGSfiLw8Lf0VYVqXiLzvCj6XYRiGYRiGYRhGTpTksMwF4N2qulVEqoFnReSpMO9TqvqJ9IVFZD3wOmAD0Ax8T0TWhNmfBl4GHAV+IiLfUtXdLj6IYRiGYRiGYRhGOnMGO6raD/SH70dEZA8Qn6XI3cBXVHUC6BaRLuAFYV6Xqh4CEJGvhMtasGMYhmEYhmEYhnNy6dlJISLtwPOAZ4CfA94pIm8EthD0/pwlCIQ2pxU7yqXgqHda+guzbONtwNsA4i1xtm7flp7JLZtupKunmxUtrezYvSujbGu8heXLljF67hwnB04zODiYyisvL2fNqk56+/poqK9j/8GDvKdtIwBbt28jXlbJzVV1fO/MMXbv35eRV1tby7L6pYycG6WkuITevqMZ2920fgOHj/bS2d7B1h3bQTWV17h8OaWxUoqLixk9N8qp06dTeUXFxQB09XTT0tTM7n17M9bb3tbG2Pg49UvqONp/jJGRkVTeokWLaG9t48SpkyyuruZQT09G2bWr13DsxHE62zvYsXc3iclEKq++ro7amhomJiZJapJj/f0ZZW/euIlDRw7T0drGtl07SaKp/RBvagagorycM4ODnDl7JlWutKyU9atvoLv3CI0Ny9jXdSBjvas6OhgcGqJpeSOHDvcwljaLxuLFi2le3sjZoUHKy8o53Hsko+yGtevoPdZHZ3sHz+3aSfLixVTesoYGKisqaInH6Tvez4mTJ1N5UlTE8zZuoqunm7Z4Czv3ZMbWbS2tTCYmqalezPFTJxkaGkrlVVRU0Nmxkr7+fupqa+nqPpRRds2qTk4OnGZFvIW9XQeYmJhI5S2pXUJDfR2jY2MUSdFl40o3rF1HV093cMykH+NAU2MjJcUlxGIlDI+McHpgIJUXi8XYtG49XT3dxBub2LN/X0bZjhXtjJ4bpaF+Kb19RxkZHU3lVVdV0Rpv4dTAaaoWVdF9uCej7Lo1N9B3vD84ZvbsJpFIpPb90vp6FldXk0hc4MLFC/QfP55R9pYbb6Krp5v21ja279qZkdfSHCepSaoqKzk1cIazg2dTeWVlZaztXM3hvqMsq1/K/oNdGWU7O1ZyZnCQeFMTXd2HGB8fT+XV1NTQ2LCMoZFh4s3Nl9XjxnXrOdJ3lM72Dn66cweaTKbyli9bRnlZOSIwNj7OyVOnUnlFxcXcvGEjXT3dtDbH2bV3T8Z6V7S2cX7iPEtqajl24jjDw8OpvMrKSlauaKf/xHFqa2o42N2dUfaGztVUV1eTTCbZfWAfkxOTqby6JXXU1dYyfv48AH39xzLK3rRhI929R1jZtoLndu7IyGtuaqJIiigrK2VwaIiBM5e+l7HSGJvWBsdM8/JG9h7Yn1F2ZXs7wyMjLG9YRk/vEc6dOwdAEuVA9yFampoZSPueG264ccPGfCvkhHm6xxdX83SPL65R8xRN+2E+64IiVcC/Ax9T1W+KyHLgNKDAR4EmVf01EfkrYLOqfiks93ngO+FqXqGqbw3T3wC8UFXfOdM2b7vtNt2yZUtOflNM/YDMhfpPPwjAwAMfTr2f/v/AAx+e1/bnw3xc84l5uscXV/N0jy+u0z1F5FlVvS2PSojIw8AvAydVdWOYVgd8FWgHeoDXqupZERHgz4G7gDHgzaq6NSzzJuAD4Wr/UFUfCdNvBb4AVABPAO/SHC6SC32tyifm6R5fXM3TPb64dvV0s3LFCsYSCSpjpYwlJqmMxQpulrZcr1M5WYtIDPgG8KiqfhNAVU+o6kVVTQIPcWmoWh/Qmla8JUybKd0pPhxEU/jiap7u8cXVPN3ji2uBen4BeMW0tPcB31fV1cD3w/8BXgmsDl9vA/4aUsHRgwQjC14APCgiU4/h/mvg/rRy07fljAKt38swT/f44mqe7vHFdeWKFZweH+O+Jx6j6bMf4b4nHuP0+BhJTc5d+BqSa33mMhubAJ8H9qjqJ9PSm9IW+xVgauzKt4DXiUiZiHQQXDB+DPwEWC0iHSJSSjCJwbdyspwH04eyFDK+uJqne3xxNU/3+OJaiJ6q+kNg+pi6u4FHwvePAK9JS/+iBmwGasPr1suBp1T1TDj0+ingFWHeYlXdHPbmfDFtXc4pxPrNhnm6xxdX83SPL65btm/j/icf5+m+Hi4kkzzd18P9Tz7OWCIxd+FrSK71mcs9Oz8HvAHYISLPhWm/B7xeRG4mGMbWA/w6gKruEpGvEUw8cAF4QFUvAojIO4HvAsXAw6qaedONYRiGYcyP5eFEOgDHgeXh+ziX3ycanyP9aJZ0wzCMyJHU5IzD1IqliM39mfdOb+4/QmWsNB+qV00us7E9DUiWrCdmKfMx4GNZ0p+YrZxhGIZhXCmqqiKS242oV8nVTqZzfmIi58l00lmzahWnBs7QGo+z/2AX58MJLYAFmUxnaGSYbbt3cdP6DQU9mc7QyHDGPijkyXQmEwkvJtMZGhnmwsWL9PQeKejJdNL3faFPpjOZmCyIyXQqKiuoaWjgM//5fZ46cZhfa7mBG5c2UVpcjCBMXkjwQPtG/vLQdn6teQ21JaXUlVewZftzLKtbWjCT6Uzt++rqalrC73w2cp6gIB9cyU2fW7dv45Ybb8pp2XxPUDAf13xinu7xxdU83eOL63TPQpigIPRoB76dNkHBPuDFqtofDkX7N1W9QUT+Jnz/5fTlpl6q+uth+t8A/xa+fqCqa8P016cvNxsLfa3KJ+bpHl9czdM9heI6OjnBfU88xtN9Pam0O+LtPHrXvVSVlvHs9udo7ezk/icfZ3P/EW5vauOhO+9haUVlQU1SkOt1al5TT/tAU2NjvhVyxhdX83SPL67m6R5fXH3xJLj3803AH4V//zEt/Z3hM91eCAyFAdF3gY+nTUpwJ/B+VT0jIsMicjvB4xXeCPzlQkn7Ur/m6R5fXM3TPYXiWhkrnXWYWnNjE0srKnn0rnsLeja2XOuzsKwdUFLsT/zmi6t5uscXV/N0jy+uhegpIl8GfgTcICJHReQtBEHOy0TkAPDS8H8IhkwfAroIZgx9B4CqniF4XMJPwtdHwjTCZf42LHOQS49NcE4h1m82zNM9vriap3sKxXUsMcntTW0Zabc3tTGWCJ79VlJcQpEUUVVaRpFI+LfwQoZc67Mwat0hsZg/H8kXV/N0jy+u5ukeX1wL0VNVXz9D1i9mWVaBB2ZYz8PAw1nStwDX5Gl6hVi/2TBP9/jiap7uKRTXyliMh+6857JhapWxGFA4nnORq2fhhWlXyXDajZGFji+u5ukeX1zN0z2+uPri6Su+1K95uscX12vlmdQko5MTJFXDv/N7losv9QmF41okRalhav1v/yCP3nVvxv04heI5F7l6Ri7YSZ8NpNDxxdU83eOLq3m6xxdXXzx9xZf6NU/3+OJ6LTyTmrzqh1f6Up9QWK6zDVMrJM/ZyNUzcsGOa85fSGR9bxiGYRiGYVw5Y4mEFw+vNPzGj0F5eaS8JHZNpqE2DMMwDMO4nphrVjDDcEHkenZi4c1VPuCLq3m6xxdX83SPL66+ePqKL/Vrnu7xxfVaeM41K1gu+FKf4I9r1DwjF+xsWrc+3wo544urebrHF1fzdI8vrr54+oov9Wue7vHF9Vp4Ts0Kdke8nZKiIu6It2fMCpYLvtQn+OMaNc/IBTtdPd35VsgZX1zN0z2+uJqne3xx9cXTV3ypX/N0jy+u18JzrlnBcsGX+gR/XKPmGbl7duKNTflWyBlfXM3TPb64mqd7fHH1xdNXfKlf83SPL67XynNqVjAg9Xc++FKf4I9r1Dwj17OzZ/++fCvkjC+u5ukeX1zN0z2+uPri6Su+1K95uscXV/N0jy+uUfOMXLBjGIZhGIZhGIYBFuwYhmEYhmEYRgZJTTI6OUFSNfyb+4NOjcIicvfsGIZhGIZhGMaVktQkp8fHuP/Jx9ncf4Tbm9p46M575j15glEYRG6Pdaxoz7dCzvjiap7u8cXVPN3ji6svnr7iS/2ap3t8cb2ePccSCe5/8nGe7uvhQjLJ03093P/k44wlEle13uu5TheCXD0jF+yMnhvNt0LO+OJqnu7xxdU83eOLqy+evuJL/Zqne3xxvZ49K2OlbO4/kpG2uf8IlbHSq1rv9VynC0GunpELdhrql+ZbIWd8cTVP9/jiap7u8cXVF09f8aV+zdM9vrhez55jiUlub2rLSLu9qY2xxORVrfd6rtOFIFfPyAU7vX1H862QM764mqd7fHE1T/f44uqLp6/4Ur/m6R5fXK9nz8pYjIfuvIc74u2UFBVxR7ydh+68h8pY7KrWez3X6UKQq2fkJigYGfWj6w38cTVP9/jiap7u8cXVF09f8aV+zdM9vrhez55FUsTSikoeveteKmOljCUmqYzFrnpyguu5TheCXD0jF+wYhmEYhmEYxtVQJEVUlZYBpP4afhK5YWyGYRiGYRiGYRgQwWCnuqoq3wo544urebrHF1fzdI8vrr54+oov9Wue7vHF1Tzd44tr1DwjF+y0xlvyrZAzvriap3t8cTVP9/ji6ounr/hSv+bpHl9czdM9vrhGzTNywc6pgdP5VsgZX1zN0z2+uJqne3xx9cXTV3ypX/O8MpKaZHRygqRq+DeZyis015kwT/f44ho1z8hNUFC1yI+uN/DH1Tzd44urebrHF1dfPH3Fl/o1z/mT1CSnx8e4/8nH2dx/hNub2njozntYWlEZ3PReQK6zYZ7u8cU1ap6R69npPtyTb4Wc8cXVPN3ji6t5uscXV188fcWX+jXP+TOWSHD/k4/zdF8PF5JJnu7r4f4nH2cskQAKy3U2zNM9vrhGzTNywY5hGIZhGEa+qIyVsrn/SEba5v4jVMZK82RkGNc3FuwYhmEYhmE4Yiwxye1NbRlptze1MZaYzJORYVzfWLBjGIZhGIbhiMpYjIfuvIc74u2UFBVxR7ydh+68h8pYLN9qhnFdErkJCtatuSHfCjnji6t5uscXV/N0jy+uvnj6ii/1a57zp0iKWFpRyaN33UtlrJSxxCSVsRhFErQvF5LrbJine3xxjZpn5Hp2+o7351shZ3xxNU/3+OJqnu7xxdUXT1/xpX7N88ookiKqSssoEgn/Xvq5VWiuM2Ge7vHFNWqekQt2Ots78q2QM764mqd7fHE1T/f44uqLp6/4Ur/m6R5fXM3TPb64Rs0zcsHOjj27F2zd5y8ksr6/UhbS1SXm6R5fXM3TPb64+uLpK77Ur3m6xxdX83SPL65R84zcPTuJxNUHITNRXhKj/tMPAjDwwIeven0L6eoS83SPL67m6R5fXH3x9BVf6tc83eOLq3m6xxfXqHlGrmfHMAzDMAzDMAwDLNgxDMMwDMMwDCOizBnsiEiriPxARHaLyC4ReVeYXiciT4nIgfDvkjBdROQvRKRLRLaLyC1p63pTuPwBEXnTQnygpfX1C7HaBcEXV/N0jy+u5ukeX1x98fQVX+rXPN3ji6t5uscX16h55tKzcwF4t6quB24HHhCR9cD7gO+r6mrg++H/AK8EVoevtwF/DUFwBDwIvBB4AfDgVIDkksXV1a5XuWD44mqe7vHF1Tzd44urL56+4kv9mqd7fHE1T/f44ho1zzmDHVXtV9Wt4fsRYA8QB+4GHgkXewR4Tfj+buCLGrAZqBWRJuDlwFOqekZVzwJPAa/I/SPlRiJxwfUqFwxfXM3TPb64mqd7fHH1xXMKEfnf4eiDnSLyZREpF5EOEXkmHGnwVREpDZctC//vCvPb09bz/jB9n4i8fKF8falf83RPrq5JTTI6OUFSNfybXGCzTHyp03TPXOssX3XrY50WMrl6zuuenfCC8DzgGWC5qk49zec4sDx8Hwd604odDdNmSnfKhYt+7CDwx9U83eOLq3m6xxdXXzwBRCQO/CZwm6puBIqB1wF/DHxKVTuBs8BbwiJvAc6G6Z8KlyMctfA6YANBY9xnRKR4IZx9qV/zdE8urklNcnp8jPueeIymz36E+554jNPjY9c04PGlTqc8c62zfNatb3Va6OTqmfPU0yJSBXwD+C1VHRaRVJ6qqojofCVn2M7bCIa/EW+Js3X7tvRMbtl0I1093axoaWXH7l0ZZVvjLXQfPkz1oipODpxmcHAwlVdeXs6aVZ309vXRUF/H/oMHeU/bRgC2bt9GvKySm6vq+N6ZY+zevy8j71VLW3l2+DT9J09wY9US7qzL9Nq0fgOHj/bS2d7B1h3bQS9VRePy5ZTGSikuLmb03CinTp9O5Q2PjtC0bDldPd20NDWze9/ejM/T3tbG2Pg49UvqONp/jJGRkVTeokWLaG9t48SpkyyuruZQT09G2bWr13DsxHE62zvYsXc3iclL0/PV19VRW1PDxMQkSU1yrD/zCbQ3b9zEoSOH6WhtY9uunQyNDNN//DgA8aZmACrKyzkzOMiZs2dS5UrLSlm/+ga6e4/Q2LCMfV0HMta7qqODwaEhmpY3cuhwD2NjY6m8xYsX07y8kbNDg5SXlXO490hG2Q1r19F7rI/O9g6e27WT5MWLqbxlDQ1UVlRwuLeXZDLJiZMnU3lSVMTzNm6iq6ebtngLO6fNyd7W0spkYpKa6sUcP3WSoaGhVF5FRQWdHSvp6++nrraWru5DGWXXrOrk5MBpVsRb2Nt1gImJiVTektolNNTXMTo2RpEUcfRYX0ZZVeXc2FhwzKQf40BTYyMlxSXEYiUMj4xwemAglReLxdi0bj1dPd3EG5vYs39fRtmOFe2MnhuloX4pvX1HGRkdTeVVV1XRGm/h1MBpqhZV0X24J6PsujU30He8Pzhm9uwmkUik9v3S+noWV1eTSFzgwsULqeNhiltuvImunm7aW9vYvmtnRl5Lc5ykJqmqrOTUwBnODp5N5ZWVlbG2czWH+46yrH4p+w92ZZTt7FjJmcFB4k1NdHUfYnx8PJVXU1NDY8MyhkaGOXK09zKnjevWc6TvKJ3tHfx05w40eelCtnzZMsrLyhGBsfFxTp46lcorKi7m5g0b6erpprU5zq69ezLWu6K1jfMT51lSU8uxE8cZHh5O5VVWVrJyRTv9J45TW1PDwe7ujLI3dK6mq/sQy5c2sPvAPiYnJlN5dUvqqKutZfz8eQD6+o9llL1pw0a6e4+wsm0Fz+3ckZHX3NREkRRRVlbK4NAQA2cufS9jpTE2rQ2Omebljew9sD+j7Mr2doZHRljesIye3iOcO3cOgKGRYUbPnaOlqZmBtO95AVMCVIhIAqgE+oGXAPeG+Y8AHyIYWn13+B7g68BfSXBRuxv4iqpOAN0i0kUw7PpHrmX7jx+nadnyuRfMM+bpnlxcxxIJ7n/ycZ7u6wHg6b4e7n/ycR69616qSsuugaU/dTrlmWud5bNufavTQidXT1GdO0YRkRjwbeC7qvrJMG0f8GJV7Q+Hqf2bqt4gIn8Tvv9y+nJTL1X99TA9Y7ls3Hbbbbply5Y5/dLZun0bt9x4U07Lpj8zZ+r99P9ne3+1zMc1n5ine3xxNU/3+OI63VNEnlXV21ysO7yHczojqnrFD3cIJ8/5GDAOPAm8C9gc9t4gIq3Ad1R1o4jsBF6hqkfDvIME95N+KCzzpTD982GZr2fZXnrD3K3f+ud/Ts+cs2HuwKGD3Lh+Q04Nc+msWbWKUwNnaI3H2X+wi/NhcAxQW1vLsvqljJwbpaS4hN6+oxllr6RhbmhkmLolddy0fkNBN8z9cPN/UVO9OJU334Y5RbmoSrEUEW9p4fzYOZqXNy1Iw9z+gwdpjcdnbZjbsWc33zu8n6kmmn8Z6GNJrJQv/cqbOHHq1DVpmBsaGebnb/9ZenqPFHTD3OkzA9RUL0aBT+x6hoPjwywqLqGyqIRfqF3OS1esYap5/pYbb+JA9yFe8t1HeUd8bWqdRcCbf+bFqOqCN8yVxkozyhZiw9zFixepq6ujo7WtoBvmhkaGqaleTHV1NS1NzVRWVGS9Ts3ZsxO2dn0e2DMV6IR8C3gT8Efh339MS3+niHyF4OIxFAZE3wU+njYpwZ3A++favmEYhhE5tgKtBEPLBKgFjovICeB+VX12PisLryt3Ax3AIPA4C3BPaDqq+jngcxA0zGULYDvbOwCyBre9fUepWrSIqkWLoG3FZfkdbW0zlq1aVAXA+jU3ZHWrWrQIgIYsMxWlnDbdmLUswJKaGlqbg1HmW7dv46b1G+b8PHW1waV9dcfKrOtsi7fMWHZqvZvWrs8uFN6D3NiwbMayNdWLZ2xEWFxdTXtr62Xpq1a0A3Dzpk2cHh/j/icfZ3P/EW5vauOhO++hpKSYm+Kh7AAAIABJREFUtZ2rs66zvLyMsUSCmzfdyFhikspYjCIpynC6ecPGrGVLYzHijU3EG5tm/Dw3rF3LRw5tTfU+ANwRb4eSkpT3dKY+Y/ZjJjgmNtyw9rK8ID84ppYtXZpK27p9GyXFxbPu9ylqF9ek9nG2z5Ot7JKaGgBWr1yVdZ1Tx+CS2Y6ZdetTDTOjkxOMHdrKwTOXgu2S6kU8sHZtRo9NUzzOLY0tfOLIpVEId8TbeWDx4tRyVYuqUt/BdFaG39Vsn2fqZvl1q9dk/TyLKis5fuLErN+B523clLUsBN+xljCIz1Z2tv0ztcx0VrRkP2a2bt+WOs423rAua9mpz7u8oeGKnGqqF6e2n2vZ2sXBMXPDqs6UZy6Nh7ncs/NzwBuAl4jIc+HrLoIg52UicgB4afg/wBPAIaALeAh4B4CqngE+CvwkfH0kTDMMwzCuL54C7lLVpapaTzCL57cJrhefuYL1vRToVtVTYe/QNwmuXbUiMtWo1wJMNVv3EQRbhPk1wEB6epYyRkRJH9Z0IZlMDWsam+Hp7Nfino/KWIyH7ryHO+LtlBQVcUe8nYfuvIfKWMzZNqJGrnVmdXv9kdMwtnxxJcPY5kO+h7EZhmH4guNhbDtUddO0tO2qeqOIPKeqN89zfS8EHgaeTzCM7QvAFuAXgG+o6ldE5LPAdlX9jIg8AGxS1beLyOuAX1XV14rIBuAxgvt0mgkeq7BaVS9evtVLLPS1ysgkqUnGEgkqY6WX9apc2fqUps9+hAtpw4dKiorof/sHKUq7P3mK0ckJ7nvisct6XVzf8+H6c14P5FpnVrfRZKbrVOT2bFdP99wLFQi+uJqne3xxNU/3+OK6wJ79IvK7IrIifL0XOBHOfDbv5nFVfYZgooGtwA6Ca9vngN8FfjucaKCeYEg24d/6MP23CZ8Tp6q7gK8Bu4F/AR6YK9C5Uuw4uDJm6lU5MO0+lfkwlpjk9qbMIUu3N7UxlpjMunxlrJTN/Zn36mzuP0LltHsxZqLQ6nQmfPQskiKqSssoEgn/Zv+Zm+tyC+layETNM3LBTnvr5WMsCxVfXM3TPb64mqd7fHFdYM97CYaI/UP4agvTioHXXskKVfVBVV2rqhtV9Q2qOqGqh1T1Baraqar3hLOsoarnw/87w/xDaev5mKquUtUbVPU7V/1JZ8COgytjpiFnyxobr3id8x3WNN/gaDq51GkhTD1daPt+JnzxBH9co+YZuWBn+rS3hYwvrubpHl9czdM9vrgupKeqnlbV31DV54Wvd4b320yqatfca/AfOw4uJ5cHPc7Uq9J14MBly+ZKkRSxtKKSR++6l/63f5BH77qXpRWVM7b2X+09H7nU6XzvI1oI7Bh1jy+uUfPM+Tk7hmEYhuECEVkDvAdoJ+06pKovyZeTkV+mejKmz4g2PeiY6lVJv1/m9qY2Ll5lj8fUsCZgzvtu0oOjhbrn42qHyhmGcYnI9ewYhmEYBc/jwE+BDwC/k/YyrlNy7cmYqVelOMtEAgvJQt/zcbVD5QzDuIT17BiGYRjXmguq+tf5lrheKcSZqHLtyZipV+Uo1zbYWWimgrrpPV02PbJhzJ/IBTst4UOofMAXV/N0jy+u5ukeX1wX2POfROQdwN8Dqce5X0/PXsvXcZDrcLEprpXnTMPTxhKTlw0rK5IiKmOxMNAJAp548+UP6CxUcqnTazFUbi5m8yykgNmXcyr44xo1z8gNY7uWM5VcLb64mqd7fHE1T/f44rrAnm8iGLb2X8Cz4eu6elBNvo6DK3mA5rVgPjf9Z5upbOj8+YL+bqVPvjCWyD75wnTyNT3yFDM5FsJMcdN95rPsXJNgLCSFfIymEzXPyPXsVFVW5lshZ3xxNU/3+OJqnu7xxXUhPVW1Y8FW7gn5Og7me+N7uudCtubPpycjPWADeLqvh48X/ZC/WnaP04d6umJ6b9or46v4k5e+ZtYZ3wqBmY7RbPV//5OPO3+oaq7k+l2ab6/mQmDnf7fk6lm437Ir5NSAP6MgfHE1T/f44mqe7vHFdSE8ReQl4d9fzfZyvsECJl/HwXxvfJ/ynPqh+NltP2LfmVNUlMQYmZzkYtJdC3CuPRnZAjbGzxfsTGXTe9OSY+PXfBrpK2GmY7TQZorL9btUCNN5X8/n/4UgV8/IBTtnB8/mWyFnfHE1T/f44mqe7vHFdYE8XxT+fVWW1y8vxAYLlXwdB/N9RsyU51giwRd3beG/r7mR9/3HP9P8Nx/ljd/5MqfHz13zIS/ZArafqWucdaayfA5fmh4crFtU68U00jMdo4U2U1yu36VCCNKu8/O/c3L1jNwwNsMwDKMwUdUHw7dvVdWLeZW5TrnSG98rY6X80sr1vOsH/5gxfOltT339mg9fyjZT2Y1Lm2YM2PI9fGk+ky/4gK8zxUVtPxi5Y8GOYRiGca3pFpF/Ab4K/Kuqar6Frifm8wDNKcYSk6xZsjTvLeNTlBYV86kXv5oVi5dwePgsp3oOz7hsvu8xmR4c1JVX8NDPvbLgg4OZKISZ4q4EX4M04+qJXLBTVuZPdO6Lq3m6xxdX83SPL64L7LmWYNjaA8DnReTbwFdU9emF3GghUWjHwUyTD0x5VsaCe3QKoWV8LJHgTf/ylQyP3++8hQ2JDVk98j18aXpwsH3v7oKfnABmP0avJGBeKHL9LhVCkFZo3/uZiJpnYX/TroC1navzrZAzvriap3t8cTVP9/jiupCeqjqmql9T1V8FngcsBv59wTZYgBTScTDbVMJTnkVSRFWslM+97L/nfL/PQpEtePlk97YZg5dCuMckffKFTWvWZv2Bne9pkad7rOnszKtHrsznu5Tv6bwL6Xs/G1HzjFywc7jvaL4VcsYXV/N0jy+u5ukeX1wX2lNEXiQinyF4xk458NoF3WCBUUjHwWyzVKV7FhcV0VC5iEfvupf+t3+QR++6Ny89FNmCl7eu2DBj8DLfSRkWmmz7/lo9u2augCrd49e/8Ujen6GTC4X0XZoLX1yj5hm5YWzL6pfmWyFnfHE1T/f44mqe7vn/2Xvz+KbuM9///ZW1WfKCDTYYgzEEm4SwpIUASdmXFMxcKNRpgITAvW2AptzJJLe5Ja82yTTLQCfzC0NfaQIkMzeEJJCGgYYOS0NYEpiJoTgNmwGzmcUYsLGxtdg6knV+f0g6SJZkG5Cxj3Ler5dftnS2j46OJT16nufzqEVrW+oUQpQBfwP+CDwvy7KjzQ7WQelI10FzZV5NdXaE8qVIvRf/a9joqMFLRyhfCiZwToNLB+2SxP7L51k2qoD8tAxKayr54NhBFg1+KGbnuTVGDcGBb3djIpel+nadodMaOtL/UkuoRWu86Yy7zE7pmdPtLaHVqEWrpjP2qEWrpjP2qEVrG+scJMvyDFmW130XAx3oWNdBc2VeHUlngODgJZBhqiwvbzZ4udXypbYsKSs9czosk7P6cBEPZuWwZO9Wuq96lSV7t1KYP4hEfeyyT62ZMxMc+M7pdg/QvjN0WkNHvEajoRat8aYz7oIdDQ0NDY2OiRDi//r/fE0I8fumP+0q7jvMrZR5dZS+kqbBi0DEbN93o6SsaeAxtc99LNyxISQQeWb3Zzhi2FfUGqOGjtDfpKERa+KujE1DQ0NDo8Ny3P+7OMIyzX66nWhtmdedzquJ5vjW0bgbVtVNA4/8tIyIgUiS0YRdcsXkXLVmzkxwiaAO2r2/SUMjFnS8VxkNDQ0NjbhEluU/+3+vafoDDGpned9pWlPm1ZoyqGjcrQb8O8Ure5GBjdPmsW/W08zMGwDEvpSraQaltKYyYkblZHVlzM5VazJ4wYHvxF757WZCoaERS+Lu6u3bu097S2g1atGq6Yw9atGq6Yw9atHaDjq/U25sarwO7mRezZ0ESreq83YJBGRPbP1Y6Zv59fAJzMwbENNSrr69+4QFHlvOHg+z9V4xbjpvFn8Zs3MVqdcpUiATCHzzevdpF3vmW0Ut/0ugHq3xpjPuytiqb9wgJTm5vWW0CrVo1XTGHrVo1XTGHrVobQedsWu6UAFqvA5aUwYVjbYe7BmL8xmpfO2Z3Z+xfOw0koymmJVyBbQ2LR1M1BuU2yerK3l9/042njoKxO5c3Yqbnhqv0Y6OWrTGm86OHa7fBtlZWe0todWoRaumM/aoRaumM/aoRWtb6BRCpEf56cx3LNhR43VwJ/Nq2rrxPRbnM1pAlpuafselXMHGDp06p+OVvWGlgwk6322nW2LJ3i1KoAPtYxKgxmu0o6MWrfGmM+6CndPnzra3hFajFq2aztijFq2aztijFq1tpLMYOOj/HfxzEPhO2T2p8TqIVAbV2WzB6Xa36M52J4FSaxzgYnE+mwvI7jTQCe5XenfvjmZ7cDrKENRYXqNt6eKnlv8lUI/WeNMZd2Vs9fX17S2h1ahFq6Yz9qhFq6Yz9qhFa1volGW5d8x3qlLUeh0El0FZDIZWu7Pd7mDP1jrAxeJ8RhpWGosgo2l5nEdyN+vu1lGGoMbqGr1TF7+WUMv/EqhHa7zpjLvMjoaGhoaGhkbbE810wOF2t+rbe1mmxW/729rYIJjWNvDfKrfTr3SrQ1A7MnfzOdTQiIR6/3s0NDQ0NDQ02o2oH+L1hjC75KalXCsPfU1VvaNFK+rmAgW75KLR6yuPkqFdh5w2x3d9UGdbm1NoaLRE3AU7qamp7S2h1ahFq6Yz9qhFq6Yz9qhFq1p0qhW1nN/mdEb7EF9aUxn27X3Tb/en9rmPBTs2tPhtf7RjnKyuDAmY3jlRfMfzaNpqFlDTHhy3XqeKQZ2xukbbOthTy/8SqEdrvOmMu56dbhmZ7S2h1ahFq6Yz9qhFq6Yz9qhFa1vqFEKslWV5bkv3xTPxcB1E6nF5a8IMkOHaz1+mtKaKRL3Bv27ot/v5aRmt+rY/0jFWjJvO6/t38tyQ0UrA1NVo5qrU0GwvTEtEsp5ubn9e2YvT7W6xr6ZpD871ulo6q2BQZ6yuUYvBwJrJs6hucNIrJY3zdTWkmy0xC/bU8r8E6tEabzrjLtiptdVhtVjaW0arUItWTWfsUYtWTWfsUYvWNtZ5f/ANIUQCMKStDtYRiYfroOmH+ApHHXqhY8GODUpgsnpSIRkWK063O2RGT2lNZatn9iTqDWyaPh+HWwJk1h3/GxtPHWXlxB8rAdM9iSlclRruqDzqVsqtbrXpPtjYwd3QgC61021pvJvE8hqVvI08u2dzyLmKFWr5XwL1aI03nR37a4XbwKiiGlC1aNV0xh61aNV0xh61aG0LnUKIF4QQNmCQEKLO/2MDrgGfxfyAHZh4uQ5ufoiXSTKYwkrTFuzY4M9+hJZybTl7nNWTCpu1Vw4EFE/4y8qe2PoxNxoamN53ADPzBigBE0Cdx1f+dqvlUU0tkSOVW9klV9h2zZkztES8PPetpa0NCtRyPkE9WuNNZ9wFOxcuXWxvCa1GLVo1nbFHLVo1nbFHLVrbSOdpWZaTgY2yLKf4f5JlWe4sy/ILbXHAjoqar4NIM1N0Qkey0RQ1M9LU6WzR4Ifokmht1vks0ofkxbs24XBLPDdkTEjAVNA5+5bn0TTt0Vl9uIhVTQKwFeOmY43wgSpaFshqMLbY46Pm5/52aGuDArWcT1CP1njTGXdlbBoaGhoaHZYXgE+Bvu0tROP2aK58q2mpGoSWpgWXckX7HUy0D8m9UtIQQtAz+SES9QY+KpjDyRMn+MW9997SPJqmPTpLD+wG4P3Jj5FiNFNaU8mG0sMsGvxQmL5A033Tx1pWW02mJUlZP1JfTzRa2wOkNqKdq0glixoabYH6/4s0NDQ0NNTCdSHE50BvIcTmpj/tLU6jZZorSWpaqnarmZbwY0V28TpfV+Pv3wEhhH+JfMvzaCIFU28c/JIUo5nuq15lyd6tPHn/0Ij6Iz3WFeOm87sDu5WMRTR3Nxk5bH+R1q1zubC1MIdIDcT6utDQuFW0zI6GhoaGxt1iKvB9YC3w/7WzFo3bIFq2JVFvwOl20yXRytops7EajNR73HeUnYjo9jZ+BslGE41eL/O2r1fuX9JrkFJO11qiZRwcbomKRS+16LCWZDCyfOw0eqWkUVpTyev7d3LNaVcyFtHc3X7d+4EIWkLXzbQkYZNcLN61qVUGCB2ZpkYW8ZS10lAHcRfsDLivf3tLaDVq0arpjD1q0arpjD1q0doWOmVZloAiIcTDsixXxvwAKkKt10GkAOH5oWOoqneEubB1SbS2+IG2udKtwIfkDwvmYDUYcbglEvyZnNlbPgoJIpYLHf9+/4BbKouKFEy9+8ijJBmN6IRocV9mvZ4ko4mZm9eEbB/IWEQLDAcVPB5BS+i6zw0ZzeJdm1ptg90WxPIajVTCGCvU8r8E6tEabzrjLqy+UH6pvSW0GrVo1XTGHrVo1XTGHrVobWOd/x6pjO27VM7WEa+DSMYDTXUm6g1hLmpPDRoR0YXN7pYill75AhwJp1uKOsQzoAUEvvBGJtlowmIwYtYbwoKINMl7yw3vTU0TIpkk3Mn20crwzl48H7avpuu2dg5RW9IRr9FIqEUnqEdrvOls8T9aCPHvQohrQoijQff9oxCiXAjxrf+nIGjZC0KI00KIk0KIHwbdP9l/32khxJJbfDytpm9u77badcxRi1ZNZ+xRi1ZNZ+xRi9Y21nkWqAfe9f/YgTP4Sttuq7xNCNFJCLFBCHFCCHFcCPGQECJdCLFDCHHK/zvNv64QQvze/350WAjx/aD9zPOvf0oIMe+OH2kUOtp1EK2/pE+vXiHrXG9wsrakmGWjpnJ54Yt8MGV2VBe2JIMxzF640evFLkmY9Qakxsao9s3BWlYe+hqbJClBWIPHHRZEXDEQ1XI6UhAXIJBxCGRybrW0qrnto/Wq3NvnnrD9NF33fF1NxEDpVmy175SOdo1GQy06QT1a401na/6r3wcmR7h/uSzLD/h/tgIIIfoDs/ANjJsMvC2ESPAPjPsDMAXoD8z2rxtz/nb0SFvstk1Qi1ZNZ+xRi1ZNZ+xRi9Y21vkDWZYfk2X5z/6fOcAoWZa/lGX5y9vc5wpguyzL9wKDgePAEmCnLMt5wE7/bfC9F+X5fxYA7wAIIdKBl4HhwDDg5UCAFGs62nUQzXjgmyNHwtZZemA3I9f/gcx3fsuT29ZFnU/j9LiRQQkufAGVg7nb1pG18pWoQZLVYFS0TLunP4X5g3jSv83jWz/G7pZYM3lWSBDxQu8HIja8Rwvi7kazf7TMz6Fjx1pct5s1ud2b+jvaNRoNtegE9WiNN50tBjuyLH8FVLfyuNOB9bIsu2RZPgecxveGMQzffIWz/prt9f51Y47sVY9biVq0ajpjj1q0ajpjj1q0trFOqxCiT+CGEKI3YL3dnQkhUoHRwL+BrzdIluUb+N5n1vhXWwP8yP/3dOAD2UcR0EkIkQX8ENghy3K1LMs1wA4if9l3x3S06yBaf4losk43SzL7Zj3NtZ+/zL5ZT9PNkozVYAwrbXtr/Ax+uefPPBEUXDjdbn9WqIDLC1+kTmrg+aFjQo4ZMAgIaHluyGie2f1ZWBCWoNOFBBEGISJmZdp6oGVLRMr8RHvug9e1GIx3VGIXCzraNRoNtegE9WiNN513YlCwWAjxJHAQ+D/+N4ZsoChonUv++wAuNrl/+B0cW0NDQ0NDvTwL7BFCnAUE0AtfhuV26Q1UAv9PCDEYKAaeAbrKslzhX+cK0NX/dzbh70nZzdwfhhBiQUBzdo9svjl8KHgh3x84iNNl5+jVoydHSkK/ye+Z3YMGlwu7w8G161XcuHFDWWY2m8m/py8Xy8vJ6JxO6ZkzIdvm33MPlder6ZmdTemZ0zQ0NCjLOnXqRGbnLtgcdvQJei42qWcf2P9+zl+6SN/c3nxz5DDINy2Q0zp3prBnPsXXLtPDbGFIchfSzYnU2uo4VHKMwf3v5+TZM/zq+6PZ8c0BNjfU08mcyJNdcrl6/Toet8S742dyo6qKqzeq2XmwiFyXjMGj4+93/Aevfm8MnVM6MSIhia+++auy/aNDRpApyTx/+Cte6PM9hmdmc+J4CW8NGEWvzG5kGRJ5efAoNlvTMTX4Srh0QOmJE3xvwEDOni+jd88c6my2kOcgO6u7/3yaSG3w8MucAcqyOo+EWa/nzPkyumVkcuJ0KY2yTILQ0Sh7ye/dh9q6OrK6duPs+TKcTqeybUpKCt27dqOm9gZmk5nzF0MDxPvvvY+Ll8vpm9ubb48dxdvYqCzLzMjAkpiI5HZTfqWCq9eu3bxkdDq+N2Agp8vOkZPdg6PHS0L2m9OjJ3VuidTkFK5UXqO2tlZZlpiYSN/efSivqCC9UydOnzvb5Jrpy7XrVfTK7sGJ06dwuVw3n/dOaWR0TsfudKITOi5dLleW1drq8DQ2Unbxgu+aCb7Ggaxu3dAn6DEY9NTZbFRdv64sMxgMDLyvP6fLzpHdLYvjpSdDtu3dKxe7w05G5y5cLL+EzW5XliUnJdEzuweV16tIsiZx7nxZyLb35fej/EoFfXN7c+R4CbW2OkVbl86dSUlOxu324Gn0UHHlSsi23x80mNNl58jtmcPhY0dDlvXono1X9pJksVB5vZqaGzXKMpPJxL198zhffonMzl0oPXM6ZNu+vftQfeMG2VlZnD53lvr6emVZamoq3TIyqbXVIbmlsPM44L7+XCi/RN/c3vzt6JGQD/BdMzMxm8wIAc76eq5V3vR10SUk8MD9Azhddo6e3bM5duJ4yH579cyhwdVAWmonLl+9Ql1dnbLMYrHQp1cuFVev0Ck1lTPnzoVs29jYyBn//1bJqZNIrpvlk+lp6aR36kS9/7WnvOJyyLaD7x/AuYsX6JPTi2+bZF66Z2WhEzpMJiM3amu5Xn0zl2IwGhh4r++a6d61GydOlYZs2yc3lzqbja4ZmZRdvIDD4VCe++TkZHr4/+cjIWQ53O89bCUhcoH/lGV5gP92V6AKkIFXgSxZlv+XEOItoEiW5Q/96/0bsM2/m8myLP/Mf/9cYLgsy4sjHCv4DWTI5i1bghe2+AZy6uwZBvW/v1VvIH857zuRP+yVz/wD23kgKZ0vqi/zxcRZfHbysLLs98f/SnFdFZ9P/5/M2baOR9Kz+WGvfGXfzb2BdOvaFaPBSEJCAnaHncqqKmVZnd3G2IdHcrrsHD2yulNy8kTI48nNycFZX0/ntHQuVVzGZrMpy6xWK7k9c7haeY2U5GTOlpWFbHtvXj6Xr17xvRicKMEt3fwWq3N6Op1SU3G5fM2jlysqQrZ9YMBAzl44T++eORw6dpRaWx2pySnAzTeQRLOZ6hs3qK65eaEaTUb65/Xj3MULdMvI5OTpUyH7vad3b27U1rbZG0jpmTP0zM6+5TcQ6S6/gQDIskxqamqHfgNxu93Kc9/R30AuXLqIsUnjbkd8A+nXN49vjhziBw8O79BvIOD7oNOjezY9srpzvaaant2zi2VZHkqMEEKYgHv9N0/Isuxqbv0W9jUU3xdtP5Bleb8QYgVQB/xvWZY7Ba1XI8tymhDiP4Flsizv89+/E/gVMBYwy7L8mv/+F4F6WZb/pbnjDx06VD548OAtaf7m8CG+P2jwLW0TK6INuow0LPTi6dMMGeSzSrZJLp7Y+nGIE9vI7Fw+LJiDRW/A6XFjNRjJWvkKnqD/N71OR8Wil7BLLuZuWxe2/dops0kymqh3u/EiY9EbwtzdVoybzudlJxnVow/5aRk43JLfNc2X6Yh2Pu2Si90XTjOyR29STYnUuurZd+kc43L6kmQ0NTskta2yKO353N8Kms7YoxatatUphIj4PnVbwU60ZUKIFwBkWV7qX/YX4B/9q/6jLMs/9N8fsl40bucNpPxKBdndslq1buc/vAzA9V/8Vvm76e3m/r5TbkVre6LpjD1q0arpjD1q0dpUZ7Q3kVtBCPEgcFGW5Sv+208CPwbO43uPaG3JdNP9dsP3RVuu//YofP05fYGxsixX+MvU9siy3E8Iscr/9zr/+ifxBTpj/esv9N8fsl402vq9KlZ4ZS8NHg92txTxwz0QFgRVXL2q6PTKctRABqCsthqDTs/iXRvDAprAjJVo29e66nF7vTz1+acsGzWVLWdLmNrnPvLTMiitqeR0TRUPZuWwMCgACg5Kop1Pj9fL9Qi22J0TrTR4fI+1rLaapQd2sfHU0RC9d2KR3Jyldkd5DWhOI3QcnS2hFp2gHq1q1Rntfeq2vrbwv2kEmAEEvsrdDMwSQpj8Ndh5wAHgr0CeEKK3EMKIz8SgTexFzSZzW+y2TVCLVk1n7FGLVk1n7FGL1jbSuQqQAIQQo4FlwAdALbD6dnfqD54uCiH6+e+aAJTge58JOKrNAz7z/70ZeNLvyjYCqPWXu/0FeEQIkeY3JnjEf1/MudvXQSCDccVhi9rDEqm/JFinI4qVclltNVkrX+HZPZtJ0BHWv7N6UiGJekNUIwOb1IBXlhVdeWldKMwfxJK9W+m+6lWW7N3Kg1k5fFhSHLX3Jtr5dLqlMFvstSXFXK93KKYFz+7ZzK+HT2Bmnu/73Du1eG70erFJEol6AyerK1l56OsQU4SO8BrQGuOGjqCzNahFJ6hHa7zpbI319Drga6CfEOKSEOKnwD8LIY4IIQ4D4/DVXyPL8jHgj/jeZLYDv5BluVGWZQ+wGN+bxnHgj/51Y44QLa8TCxo87oh/3wp3S+udoumMPWrRqumMPWrR2kY6E4KyN48Bq2VZ/g9Zll/El4W5E/438JH/fekB4J/wBVOThBCngIn+2wBb8dlfn8Znff00gF/bq/i+oPsr8MrtZpta4m5fB4FG/V4paVHnt0SyaA7WaTWEz9h5a/wMlh7YpQQSP/9iI15ZZtmoqVQseokPpszGqEvAJrmwGoysarL9inHTeffwfowJekWXTWoIMyVYuGMDU/v0j6gbop/PpAiOb1P79A8LgJ7Z/RnPDRkN3JnFc8Bx7slt6/yB2hYK8we7cyotAAAgAElEQVTxwbGDSmDWEV4DWmPc0BF0tga16AT1aI03na1xY5sty3KWLMsGWZZ7yLL8b7Isz5VleaAsy4NkWZ4W1ACKLMuvy7J8jyzL/WRZ3hZ0/1ZZlvP9y16/nQfVGpxBNf1tiVlvoPMfXqbzH17GrL89K8i7pfVO0XTGHrVo1XTGHrVobSOdCUKIgDHOBGBX0LI7McxBluVvZVke6n9f+pEsyzWyLF+XZXmCLMt5sixPDAQufhe2X/jfjwbKsnwwaD//LstyX//P/7sTTc1xt6+DgNtaaU1lxOyKwz/gc/eF09S66rEYjNglCXtQb6XPHtnKB1Nmc3nhi3xYMIfXir5Qyr/AF4B0tSazZO8WTlZf48lt66hx1WOTXDg8El0SrcqMnmWjCnh9/07eOPglVoNR0ZViNEcMyPLTuoTpbvC4sUsuHM76sBk6QMRsUn5alyj7z7hji2en2x0xkJrap78SmHWE14Bo7nvBGa2OoLM1qEUnqEdrvOm8ex6Gd4ngRuOOjlq0ajpjj1q0ajpjj1q0tpHOdcCXQojP8A0V3QsghOiLr5TtO8Pdvg6c/hK0N4u/YsW46WHzWxKEYP/l8zyYlcP87Z+QtfIV5m5bR/nVKzjdkn8YqAshBDohcDV6ALjitIUcJxA4rRg3nTeLv6Ko4gK9UtJYvGsTeqHD4ZZYsncLme/8ljeLv+K5IaO5vPBF7JKL9/xzZUprqiIGZHa3FKJ7zeRZ2N0Sj2/9mI+L/ytiKVYkW2y7O/pcoI8K5tDZbMHpdkccQtoS0YKI/LQuSraoI7wGOKOUJAZntDqCztagFp2gHq3xpvOOvknT0NDQ0NBoLbIsv+53PssCPpdvOuTo8JWhabQRFoOBdx951DcUdP8ulo+dRm5qOg63hNVgAASjevRh3vb1irnAvvIyDlsz6OrIxWowsrakmDcOfqmYA6QYTbw1fgaLd21Smv/fGj8DkHl9/042njrqD14qKaq4gClBD3h495FH+eDYQQrzB/HM7s9CDAc+KpiDWW9g1aTCMDMCg07HpunzcbglEoSgUZZ5yq93RM4ApRQr2FwgQefLRgUc3+ySC4vBqJyLYNMCi96AEOGudGsmzyJBp8OiN/hd4EwRG/rhZhARbNAQCNSSjbffB3QnRDIiCL4egs/x3RxaqqFxt9CCHQ0NDQ2Nu4Z/iGfT+0ojrasRO3wlaBbFFc33Db5Msj8ocLolUkzh5WM3GurplZLGzM1rWDaqgKUHditBxYcFc0g2mlg18cdYjSasBiN2ycXJ65U8N2QMKyf+GJvk4o8nv+X5oWMUO+lulmR+N3pqWGD11OefsnzsNJKMJky6BCVAcfgzS7O3fNTEic3aYikW+AKeFH8jc+B3Z7OFD6bMJslgpLSmirUlxTx5/1CSDEallwUg05KETXLxyclvIwZnTS2qIwURqycVkmQw3tWBoAGaWms/P3QMTw0aQbLRRKLewLqpj2PWG6IGbxoa8UDcBTu6hIT2ltBq1KJV0xl71KJV0xl71KJVLTrVSluc35ashANuazfLsoQ/02HAI3spq60Oy0okmcxcstcqPS0BiiouYDUYkWUZV6OHJ7Z+HPLhfm1JMWP9WaC3xs8g1WQOmbHz9sSZEQOV4MBqy9njLBg0AiFExMBo7ZTZil7J65u9FijFask2ut7j5skmM3/2lp9j7ZTZdLMkMzNvAM8NGU2/9EzKaqt5tN9g/n7Xn8I0NLWojhRUhj0Pbfy/5ZW9ONxuf/Ap8cGxg+wrL2Nm3gAK8wfx5LZ1IQGbWa+PeL7U8hqgFp2gHq3xpjPuQvgH7g8bBdRhUYtWTWfsUYtWTWfsUYtWtehUK7E+v9GshJ1uKaTnJNJ6lU4HSQYTvzuwm7cmzAjpb5k2YhTmBD3PDx1Dac3N+vhAb06Dx4PDLbFx2jz2/GQRmZYkFuzYwE8HDlNMCD45+W2YK1o0o4RAyVu/9Aye6D+EudvWYdEbIgZGVn852sjsXN6+fPKWzAWi9dZYDUZeGzmZlx6axJK9WxVr6p7JnVqVRQIiWngHcyfPfSTHvKbLq+qdPOF/fuduW0dh/iAleGvqctfUgS1WOu8matEJ6tEabzrjLtg5XXau5ZU6CGrRqumMPWrRqumMPWrRqhadaiXW5zealfAVhy2kad/pdvPBsYMsG1WgBCNrS4qpddX7zAZkWD52mrLs0wP7WLBjA08NGs6Ws8fDTA0cHoln92xW5uH8evgEulmSSTGalfseu/cBGjzukODmzeKvItpQv1n8la/HRZJY6Hc1ixYY1XvcShZl3+S5fFQwRxmO2lxA4DsPkRv0S2sqsUkuFu/cFHIuA1mvpuvfikV1IFA5de7sLRsfBLZvaTZOpOsgYKudn5bR6oAN1PMaoBadoB6t8aYz7oKdnt2z21tCq1GLVk1n7FGLVk1n7FGLVrXoVCuxPr/RMhW9UtL44NhB7JKEV5ZJ1BvCBnYW5g8i2WhmxbjpZCWl4GpsVPZx1ttAUcUFko1mFg5+iIpFL/FhwRysBiNurzfiB+tfDRtHaU2lct/inZtolL1KFkav03HNaScj0crvx/+IikUvsXzsNJbu38U1p92fnbn5eKI5yAXKw5KMJnKye5BkNCHLNDvQ8+b5MoToCQ62Is0i+t2B3WHrN5dFapqBafTeDFSm7FgfMVCJRmBfgbLDTEtS1MxMdDe4jKgud9ECNrW8BqhFJ6hHa7zpjLtg59iJ4+0todWoRaumM/aoRaumM/aoRatadKqVWJ/faJmKS/ZaCvMHMXfbOrJWvoJNcoWVMj2z+zNsUgOfl53ker2DJXu3KIHQrC65PD90DA63hMMt8ebBL7lkq8WcoCc5wsDOoooL5Kam82bxVyH3WQ0mJQtTsegl1kyehdPj5u93/YmFOzbgavTw9sSZPoMCgxGb1MDzQ8co5Vc9kjuxZvIsKha9pGRwgsvDjp043qqBnnCzt6mz2cLaKbO5vOjmzJ+Np45yxWkLO5dXnDaseiNrC2ZTsegl1k19nES9z8WuaZYmcgbGofTO/DQrr8USsmj7enbPZn49fAIz8wYo5zZkNk6U68AmuchP6xJmw91cwKaW1wC16AT1aI03nXEX7GhoaGhoaHzXiJap0CFCgptoAUqqycyj/QaHDcQ8dv0KPxs4nBsNDXxYUuzPCvmCoWilXRWOupBBo4H+Hp3QYTEYqKp3MG/7en6558+8NX4G15x2xv5xJTM3r8GgS+DZ3ZuZv/0TfjpwGP/40CMs2buVX3yxkeoG34BTOeh4gayHDNglibUlxc0O9AwOHrqvepW529ZR5XSw5exxNp8p4YVh47DoDbw1PrR36Z2JM7FJDSQZTL5Bpm5J6YtpmqWJVEq2YMcGpvbpH3beo5WQBYhWlvbPo6cyM29AWGYm0nXgswP3TZvPsFiVgDNS0KihEY/EnRubhoaGhobGd42AC1igxKystpql+3eFuZ5dstdGnANjk1xRraeTjSaSjSam9umvBE4ASw/sCpuzs3pSITohGJmdGzJ7R6/TKTNuAqVYG04dwYuszPwpq61m27nj/MOQUeSnZWCTXKw78TcyLUm8MHx8mO1zZ7OF6w0+W+WHE5J45ew3rBg3nZM1lUqw1c2STI/kVMDXx6MTIsRaOhCIrJ0ym+eGjsEuuZi7bR2ZliSWjSogPy2D83U1pBjNPO53nPt69mKe3bM5qjObxWCkmyWZfbOe9pePVfKvxXvJT+sScm5b4xwXrSwtxWjmpRGTSDaaQjIzTa+DwEwis16vBDWB47XkWKehES9owY6GhoaGhkYcoBM6kv3W0pmWJN6Z9GPskiskuNEhIg4C9XplbC5XWCDUyZzIJXstKUYT+WldQj54bzx1FB2CDwvmYNEbsLtdmBMM1EkNYbN3Gjxu5m//RDnminHTlX1sPlNCxaKX2Hn+FNP6DggZJrpi3HSEEBFtn9dOma0ELoGhos/s/oxlowoAeGnEJEx6vWKLHZgxs3HaPEprKnmz+Cs2njpKUcWFEPe0oooLeLxeJWDS63RcXviicvxIPT3BWZoGj5vfjJgYdo7rPR5GZueig1Y7x0UbUlpaU8mSvVv5sGBOWGYmcB0Aym8Nje8ycZe77NUzJ+qyBk/ztbF3m+a0diQ0nbFHLVo1nbFHLVrVolOttOX5DTTtg4xepwspa+qelMJrRV+EuLG9VvQF+oQE3juyP8wIYMKA72FO0PNvRw5g8wdOweSldUGWZYQQ2N0Sep2OP506CkKE2B/bJSmksf6Z3Z/x5thp7Jv1NM8PHYPTLfFov8GKA1vwetFsn4PtrLdWXVLuz0/P4LUfTCbZaKKz2cKyUQW8+vAjyoyZYOe4pqVg0V3aqpTb0dzhAvtolGUW7wp1c1u8axMyMh8VzOGnP5jQ6hKy5owUAlbZbYVaXgPUohPUozXedMZdZqfB1RB1mVlvoPMfXgbg+i9+e7ckRaU5rR0JTWfsUYtWTWfsUYtWtehUK7E4v80NEQ30pjz1+ad0syQrpWJ2ycUVp42R699W9jMyOxerwcgbB7/kZE2lUr5VWlOFSfLQJdHKo/0GI4DVkwpZ4M+8PD90DHP9s3CCy9h+cu9gntwWOgR08a5NLBtVoGRLiiouYNEbWLJ3K6smFSI1NpIaoYyuqOICjqDsRsCwID8tIyRr1dngy2CMyMrBIUlIjY2sLSlmap/+5KdlkJ2UyurDRYqmTEsSXllm1aRCbJLLbzZwM7h46vNPlce0alIhH5YUK5reLP4qLDsWnKWxRik9s+gN1HvcNLhcmJOsyDLY3a6ow0fhZlna2imzsRqMlNZUKkYKI7NzWzVA9XZRy2uAWnSCerTGm864C3bSUju1t4RWoxatms7Yoxatms7YoxatatGpVu70/AYHM8EfuAPZguDGdoANp44wMjuXdVMfD/sw/+4jjyqBw8ZTR5WAZGR2LqvG/ojuq15VSrG2nzsREjjN3bYurP9l0/T5Ue2PAwRKsfaVl7FwxwY+LJijWCM3LdkSwLuPPMoHxw5SmD9I6d15fugYVk0qZOGODZyquqqUhul1Oj45+W3IuoGSuJP+wai/Hj6BDaWH/cFQF+xuCavBSIPnpktbktGE3e3CqEvgyfuHsrf8HEUVF7jmtJNsNCl9McGBis8wIbz07PmhY6iqd7BgxwZOV12lb5eurJ5UyNqSYt44+GXY8xeML1NnpKreyZK9WymquBC1DC4QACfqDTj8gVC0QKol1PIaoBadoB6t8aYz7srYLl+90t4SWo1atGo6Y49atGo6Y49atKpFp1q50/MbbYhowMo4WmO7WW8IsYAOlFNZDcawErYV46ZTV1MTUoo1skdvHvzo98z47P2QMrLgYziilIKdr6sJK8UKbGM1GNlytiRMw+pJheh1OjqbLSwYNCLEWW7pgd18WFLM2imz+dOEnyiPxaw3hJgpNB2u+dyQ0WwoPRziLPfktnVcr3ew8tDXiktbpdPO6kNF9Hr3n/jg2EE+mDJbOWcpJp9pQ6DPJxDoVNU7WX24KOxx/GzgcMXp7qGUzCCHtvuizswJJpDhac5JLXD8lYe+5qLthmI3fiszfYJRy2uAWnSCerTGm864y+zU1dW1t4RWoxatms7Yoxatms7YoxatatGpVu70/EYLZiwGo5JduLzwxZBG/EBfSXA2QCBwuN1YDUYMCQm8NX4G3ZNSKK2pZEPpYR5KSApyFasiz+8o1s2SrPTwRMrERHJpM+oSqFj0EmW11UopVmCb6/UO5vYfwtqSYpaNmkp+Whdskou9l84yvHsvuiRaIgZXbxz8kueGjuG0zUaev5wrMFOm2exSHxHiLBcIPpaNKmDpgd1ht5ce2M3e8nOK41okggPQ4HLAOqkhxPL7nsTkcD20bEV9sxcrspNa4PjLRhWEPbZgt7jWopbXALXoBPVojTedcRfsaGhoaGhoxDvRXLocbgmnW1L6agLlW/3SMpjbfwhmvYFKp6+cqpslmddGTsYmubCkpOHyeEg2mnj6i41ccdp475FHKT15QimdCgQtS0dOZmKvfL66eIY1k2eRbDRRWlPFlrMlPNF/CCsPfU2K0RSybK1/Rs++8nM8mJXDNacdvU6n6NMJwZpjB/npwGGkGM0hQdrI7FzWTpmNECLiYw6eMwNgNRiwRSglC5wfIUTLwVCU280FI8EBaKAcMODkdr6uJqqrWtPHcrs9OIHj56dlNOsWp6HxXSPuytg0NDQ0NDTinWjDIxs8HhxuiY3T5rHnJ4vItCTxzO7PWDBoBGtLijlVU6WUU/1mxEQaPB6e3bOZ7qte5dk9m2nweFg+bhprJs/CrDdwtOpK2HDMn/R7gJyUNAZmZDFv+3q/u9kWnug/hC6JVt44+CWjevRh3vb1ZL7zW0au/wNLD+zmmd2f0Tctgy6JVpaPnUbFopdYO2U2PZI7kWa2kGo0kyB0CCHoZk3mwa49AH+Zm9HXG/P2hJlhZW4Bc4EAOqEjyWBk9aTCkHUD/TwBm+zIrmuVzd5uGlgFE83JzSa56JWSpugJWE+vnlTIlrPHQ/S1ZEXdHA0eN1/PXowQgq9nL2Zm3oBWa9fQiGfiLrNjsVjaW0KrUYtWTWfsUYtWTWfsUYtWtehUK3d6foN7OCz+IaLbzh3nR3kDeXbP5pCsztL9u7AajCw9sJtnh4ymqOICM/MG0MmcyBNbPw5zTfuwYA7ztq9n47R5HK+rDjluUcUFkk0mHJIUViq1cMcG3p/8GCOycqJmF/LTunC+rgadEMiyzP/58s+8+vAP6WROZFrf+0Oc3VZNKgRgy7kT1LkaWLBjA+898mhYxujJ+4diNPvmCwV6WBJ0OjIsVuX8ONwSjV4vs7d8RFHFBd6bVBjiLBfsuhbIOAUMBArzBvKrYePITU3H4ZZCjhPynEZwcls9qZD3juznVE0Vr42czPKx03BVXufRjM5YDUYWDn6I5/zW24l6Q1R3vZbwyl7sbinkuX9r/Ax0CK44bRGDwpZQy2uAWnSCerTGm864C3b69MptbwmtRi1aNZ2xRy1aNZ2xRy1a1aJTrcTi/OqETskEPLTuLfb8ZFGIA1ugKX/52GnKnJjL9jq+mfsPZFlTcLglulmSQ/YZMAvYV15GaU0ll/VyyPIRWTnYXC5SothEp5oSWT2pMGrZlt0tYdAl8MnJb3m032DANxZCamxUMk4B7Qt3bGDN5Fk8ktuPZKNZMVgIDtAA9paf419GFlBV7wxp2g+cH6ffaa2stlqZ9TP/L3/khWHjlIGopTWVbD59lKl97uPZIaNxetxY9AaefuBh7G4pqutd0+cjOAC1Sy5WHy7y9QDNepqf+Z+bpAQ99kbfgNGPCuagEwKLwdCsu15LNHXfCw5cL9lusLakmEWDH7qlEjm1vAaoRSeoR2u86Yy7MrYKlThIgHq0ajpjj1q0ajpjj1q0qkWnWonF+Q24b5XVVjebTclNTWfL2RKWjpyMISGBp7/YSNbKV3hi68f8ZsTEsHKni7YbgG+ezEsDfxBWNubwSFEd167XO/jNf23Hog8vs1s9qZA/nviWxbs2Mbf/ECx6A78ZMZHqBmdUZ7cUk5kNpYc55R/kGW2Gjb7eFeZm5pW91LlcXHPa8coy6WYLqyYV8s3cf2Bm3gDeOPglADM3r2Hk+rf569VLyra+QangleWIrncOtxu75ApzOAuYCARc2gLHCH5uHk7NVHQH+mhactcLPB7fMeWwY0c1rNAbGLn+bd44+OUt9+yo5TVALTpBPVrjTWfcZXY6paa2t4RWoxatms7Yoxatms7YoxatatGpVmJxfgMfkDMtSawYN13JpmRakpTBm+framjwuPnZwOEAzNsePuxz+dhpbD5TopRddU60cuJ/Po9BlwCSm/cnP0ay0UyFvQ7J28jinZvoZknm7QkzeXrnxpCyKYMugX8aOQWAdLNFKTk7X1eDMSGB/zlwGCN79GZtSTEz8waSm5qufHgPzgTNzBvAC8PGA/CzgcPZe+ksK8ZNp8JRF/ExuhtcSvBg9w8JbfB4kLyNbDx1JOLMnX5pGegQygyfuf2H4PD3tVTVO/DKckgGK3igqdPjZuWhr3ny/qFRsy/BJhKl/mBtX3kZp5w+B6lgQ4Lm3PWgNXOVIhsyBHqObsf8QC2vAWrRCerRGm864y6zc+bcufaW0GrUolXTGXvUolXTGXvUolUtOtVKLM5v4APyxlNHeX3/Tgy6BD4qmMOrP5jMkr1bFdMBm+QiQacLsT8OEMj8lC98kWWjClhbUkx1gxOvLDNv+3pWffkX5m//hEu2G3iRWbxzE/vKy9hw6gheWWb52Glc9m/7StEO3juyH68s87PPP6X7qleZt309VfUOEnQ6UoxmnG6JN4u/4o2DX5KTkoZNakAnBDa3S2ngL8wbyEsjJvHsns1krXyFedvXMywrx2dkYEpkzeRZvDRiUshjrKy4zPNDx3CyupKVh76mqt7BVaeNpz7/lKl97os4c+dnA4dj1uvpkmjh54MfRmpsDDFrsEku6t1uRmTlMDNvAL8ePkE55hNbP6YwfxAfHDsYdTZOsInEvxbv5a3xMxiZncujmblhhgTRzA0CpgItz1WKbFjxr8V7b9v8QC2vAWrRCerRGm864y6z0x40eNyYgxr/mt7W0NDQ0NCINcHf5gdm1rwx+u9Y2KT3ZcGODWyaPp+T1ZURv/2vczUgI5OXlsHUPoLEoL6YETkDlOBg0/T5IcFS96QUuq96FY/3ZjnVvlm/COm9ybQkITU28ve7/hSSkfjnUVO5Xu8IMQhYPamQP/7dXFyNHqwGI8tGFSj20wt2bFDmxDTKLhbv2hTyGAusGfxs1ASe/+o/eW7IaBbs2MDGafOatWJONpmod7uxGAw0yt6wfQZ6Xt595FHskivMkOGZ3Z+xbNRUZbZRsLlAot6Aq9FDot7ApunzcbgljLoE1k6ZzamTJ/l5v35YDUYlIxTJ3CA4QGkp89O0X8jhlkgQgncm/fiWzQ40NOIN7cqPAWa9gc5/eFn50QIdDQ0NDY22pum3+a8+/EMSdDo2TpvHvllPK704RRUXsEsutpwtYcW46SHf/q8YN533juzHLklU2G0s2bslal9M0z6dQGlWME3n1zw3ZLQSRARnJIQQSlAUuH9tSTE3XPXM3bbOb2e9lV8Pn8DMvAEhH+wj6bvRUE+y0cTGU0eV4CagL5LOEVk52CVfUFJV74z6mK0GI10SLeSmpkd1l2vwuKmqd/L41o/JWvkKj2/9mDqpgZqGep7w3/fE1o+54apn9eEivjhfytxt66iqd9Do9fXhyDIYdQlKpmz52GkYdQnKsVrK/EBov1CyvzQu0DukBToa32W0q19DQ0NDQ0OF6ISOzmYLa6fM5vLCF9HpdDyx9eOwQOE9v4Xzs0NGk262sGn6fE7/dAlvjZ/B52UnebTfYLKTU+lkNvPW+Blcc9gifrB2SC5WBc2u2XL2eNgsG5sUOr8mUlalmyU5YlA2tU//sADomd2f8cbov6Ni0Us43BIOSaLB4w7T18mciE1yMTNvgBLcvFn8FSvGTWfL2eNhQd67jzyK1Z8B6Wy2UCc1RHzMDR43Trc7qiGD3S3RGMHEoKahnqd3biTTksSenyxi47R5ONwSj/V7AC83M26X7LVcc9pxetzM276eBz/6PZnv/JYHP/o987avb7ZM7U7n8mhofFeIuzK2fn3z2ltCq1GLVk1n7FGLVk1n7FGLVrXoVCuxOL9e2YtNciluZnbJpVgrBwKF93/4GJK3kdWHi8Ka9N+ZOJNH+w1m/vZPQkwGEnQ61kyexbzt6/n46lklA/Tyf3/OHybOZNmoAqVJP0EIlo+dRq+UNEprKvnjyW95a/wMFu/aRFHFhTAL6pl5A/jNiIk8sfXjELMACM8Kgb/czGgia+Urij6zXs97jzzKz4JKvsYNHsJ7R/bz0kOT+PZquTJDZ+n+Xfxq2Dh6JnfigymzSDaacbgl9EJHdcPNhv/nh45h1aRCFgaV1a2ZPEuxne5mSQ55XIGyO6vBSL3HzcZp8yitqVTK7nqlpNHNkswLw8eHnPN3H3mUqyad8th6JndixmfvKyV3TR97tDK1u1GappbXALXoBPVojTedcRfsXKm8xj0q8QdXi1ZNZ+xRi1ZNZ+xRi1a16FQrsTi/DR4PNskVNkQUYOOpoxRVXCBBp2PBXz5h2aiCsJ6Tn3+xkeVjp0V0Z+uSaGX52Gl4b9ThTrbw+v6dXHPasUsu3iz+imtOO8vHTsMuuehqTWbm5jVK0JBqMit9KglChPSivDBsfFhvTGAWkN3tithTVFpTicfrJdOShNvbSHZiKnbJxbqpj2NK0FNaU8mf//ZXlp78K3vLz7F2ymysBqMSGNgkF1vPHmdgRhZPbltPUcUFDs19FmeTIOXDkmLen/wYKUYzNsmFToiQ2TVefIYMgeGiCUKgE4JKp53fHdjNFadNOf/n62r41bBxYef8qc8/ZdmgUSwcMpItZ49TVlsd5tYW/NiDHdQCZWrALbmq3S5qeQ1Qi05Qj9Z40xl3wU7vnjktr9RBUItWTWfsUYtWTWfsUYtWtehUK3dyfgPN8DJEDByWjSpg46mjjMjKUebXRGvS75WSFvE+IQSuRg91qcnkp6bzwrDxWA1G3juynxeGjceUoCfNnIjNYORAxQXFYrqq3sHcbevoZknmV8PGkZuaTqMs8/HUx0n097NGc4T75sqlsOzKinHTeX3/TsUNLThLsnpSIQ7hJj8tA/mBoSRndGZUj3tIMpoUkwCnWyLZaGJMz3tYfbiIfeVlLB05mUSDkQxLEqU1VWw5e5xfD5/A0v27SPYHOslGE06PO2To6sZTR9l8poTLC1+k3uMOMRMI6AwEbmnmRFJNiREfa9//kUvhn9eyelIhm08fA1BK7ppmgdqzTE0trwFq0Qnq0RpvOuOuZ6fk1Mn2liTj1uEAACAASURBVNBq1KJV0xl71KJV0xl71KJVLTrVyu2e38C8lZWHvo7aVJ+flqGUngXm10Rr0j9fVxPxPqdbwivLbD7wX4oVs+Rt5FRNFb1S0tAJgVlvwCN7GdG9F/O2r6e0pooFOzaQaUniheHjFevoJ7Z+rAzDrHNF7o1xuCWsRiNdEq2snTKbikUvsXbKbDaUHmbjqaM8N2R0mH30gh0bMOh0dF/1KodOlDCt7wCW7N2imARU1TtYeehrsla+wtxt6yjMH8TSkZOZ1ncATyomCFsozB/EhtLD/PbhR7he72De9vWKvfRvf/BDlo6cHKK1wePBLrnYOG0ee36yiExLEs/s/oznhoxWArcUoxmH2xXxse4/eljRP6HXzTIcIYTST7Vu6uNR5/fcLdTyGqAWnaAerfGmM+6CHckltbxSB0EtWjWdsUctWjWdsUctWtWiU63c7vl1ut18cOwghfmDKKutjuzQ5XHz+/E/IkHosBiMrJpUGLFJ/+0JM7EajCH3rfL3ocjAgh0baJAkJbhYvHMT/zRqClX1DhZ98R9krXyFp7/YSIPHQ6YlSem5iRaYnKqp4r0j+8NMDd4aPwMdgry0DCrsNhoaPZTVVrP6cBGP3fsAI7Nzo2amUkxmPF4v3ROTFMvt4GM+2m+wYhDglWV+0u+BsPWe2f0ZU/v0x2o0hRkkLNyxgZ/0e4DCvIGMzM7l/cmP4fBIyjyegBFEN0sy+WkZSulZgk5HgtAps3WC3e9OX7+m6M9NTeeFYeP49fAJ/P2uP90MDt3h14dX9ipBo++3N2ydWKKW1wC16AT1aI03nXFXxqahoaGhoRGvWAxGpvbpz4bSwzzW74Gw2SyrJhWy58JphvmzJTohMOh0PDVoBEkGo1Judslei16n40DFBd6f/BippkS/BbLMu0f289zQMb599uivHLuo4gLGBD3/6y8fh/X5LBtVoGSPogUm+WldGHlgN4DSG1NaU8lrRV/w9sSZXG9wkmY2s/LQ1zw7ZDRvHPySkzWV/H78j3D6Hdia9rTYJRf7Zj2Ns7wi4jF7JneirLYagHSzhRSTOao2IURUg4R/Gfs/uGSrRa9LYP72TyL2HJ2vqwkpPTPrDbxW9AXLRhXQLz2TstpqXt+/kz6um/odbokFg0Ywd9u6sN6edVMfxyvLytycRq+XedvXh5S5tXf2R0NDDWj/IRoaGhoaGirB6ZbIS+vC3P5DcHsbSTdbWDN5FhWLXuLDgjmkGE0U9LkPY4JvRossy9S6GvDKXraePY7kbWTm5jUM++j3HKi4wLCsHOZv/0Qp9aqTXMztPyTManlm3gC+nr3YP+xzqmIXDTdL5wJ9JwEHtmACpXQAbxz8khSjmcx3fsvI9W9zxWmjtKaSpz7/lGtOO4X5g7hkr2VEVg4bTx3l+2v/lXXHvwnLCK2eVMgnJ75lyd6teLzeiMesczUoWZh529dHLaOzSS6q6h1RdFdhNRjZcrYkaulgbmo63azJIcGH0y1xxWlj5Pq3WbhjAzohuOa0o4Mg+2uD0lcVTDdLMna3pMzueWLrx9iauO099fmnijW1hoZGdOIu2ElPS29vCa1GLVo1nbFHLVo1nbFHLVrVolOt3O75tRgMuDwepMZGnt2zmWz/h/gKex0ebyOzt3zEZbsNu3Sz1OrZPZtxut1M6JVHitGk9MSM6XlPWNnW4l2bsBiMyLLMpunzeaTfQJaNnMJLIyYpPThL9m5RZvjAzT6fzWdK2FB6mExLUthMmBXjpvNm8Vch6zddFjBHeGb3Z+gQStldYd5AJvbKD+nnWTN5Fp+dPsqSfdvYV15GibM2YjD03pH9IY/vvSP7Q2YFBdaTZZnOZkvYPnxzekooq62OGAQGHo/D7Sv3a/pcBc5D4Nx8MGU2P3lgOGunzKaz2YLT7Y44N+hXw8aFze5ZvGsTzw0ZrawTbE3dFqjlNUAtOkE9WuNNZ9yVsaV36tTeElqNWrRqOmOPWrRqOmOPWrSqRadaud3zqxM6vMghLmyZliRcjR6yklJYNqqATmYzT2wNLTV7eudGPpgymzpXgzIr5vLCFyNmKawGIzM+e5+iigtM7XEPyx8p5MkmZVY+17epXHPaWT2pEL1Opwz+BNh/+TzLRk0lP70LdsnFu4f3s/lMia9HZ8IMOpnMXF74IqU1lby+fycbTx1lZHYudVIDG6fNw+lxs/7431g18cfodLqQUr13H3mUF/dtZ8OpI4rut08U8+SIMayZPIsUk5my2mq6JFp54+CXIY/vjYNf8g9DRrNs1FT6pWdQVluNMSGBfztygDcOfsl7kwr5YMoskgwmSmuq2FB6mML8QYr19rqpj4eVDr41fga/3PNnrjhtIaVlkWbjJOoN1JhNzN22LuTxBOYaBe7LTU2Paj4RoKk1daxRy2uAWnSCerTGm864y+zUNzS0t4RWoxatms7Yoxatms7YoxatatGpVu7k/AaXUgUsmW9mXbZGLbVKNpqUIMnj9UZ1aKtzNSjlUuU11SRF2V+/9Awlw5L/7//MjM/eR5ZlHt/6MfP/8kdGrv8DmW//lncP7+dnA4dzeeGLrJk8i09OfMsv9/wnl+11LNm7VQmCVk0q5N+OHFCc0KbnDcBiMIZlOJ76/FN+NWxciJ4fdO3BJXutkuWyGoxcb3BGKVlrID+tC7WuepYe2MX87Z8wtc99eLxe5v/lj7x7eD9Oj5v8tC5M7XOfEowVVVzArDeQZDCyfOw0Kha9xPKx03ilaAcbTh2JWFoWmI2jE4Iko4l6j5t/PbA77PEk6HR8VDCHikUv8VHBnKgZpOCMWFtbU6vlNUAtOkE9WuNNZ9wFOxoaGhoa302EEAlCiL8JIf7Tf7u3EGK/EOK0EOITIYTRf7/Jf/u0f3lu0D5e8N9/Ugjxw/Z5JNEJOHIFPghHcj6L5tLmcEshQVInUyKbps/nr4//veI2tmLcdN47sj+kXKq0piri/uySiz+e/JYX9m0HfAFQoP9kZt4A9s16mms/f5mpffqTbDSR+c5vef6r/2Rm3kDemfRj0hMtfFhwsyztw5Jilh7YrQwQtUsukoymiD1CuanpIeVmTw0cgQ6hZLE8shejLiGsnC44oJq//ZMQJ7UAgWzQzM1rGLn+bTaeOqo8ZqdbwqzXk2Q04ZVlXI2NrJz4Y/bNepqZeQNaLC2zGIycrK4KuS+QTQsOiqxBJXDBwU03a7ISEGnmBBoarSPuytjKKy7TNSOj5RU7AGrRqumMPWrRqumMPWrRqhadTXgGOA6k+G//Dlguy/J6IcRK4KfAO/7fNbIs9xVCzPKv95gQoj8wC7gf6A58IYTIl2W5MdZCb/f8Ot1uVh8uYvWkQtaWFNMvPTMs6/K7A7vDSq1WTypEavQwIiuHTEsSvx4+gUVf/EdIKVWiXk+i3uf2lpfWBYCxaVlsOVvC6kmFLGgy7HP14SIK8wfx16uXlCGmta56nh86hsL8QWEDQAvzBnLFacOgS6DB41bc31YdKuK5oWNINZo5/dNfkWI0U1XvCDseoBznou0Gy8dOIzc1nYu2G+w+uJ+5Yx8BfMFDljUFkAGUMjK75GL14SKW+h3hAuV4b0+cqZgnADw/dAw6RNg5DGRSdEJHZ7OFqnoHS/ZuCdHYLy2j2dIyp1viR91yOVB383iRytEilcAFjg20WelaMGp5DVCLTlCP1njTGXfBjoaGhobGdw8hRA9gKvA68JwQQgDjgTn+VdYA/4gv2Jnu/xtgA/CWf/3pwHpZll3AOSHEaWAY8PVdehgtYjEYOVVTRedEK3P7D1GyOMGWzFecNhL1en4//kf0SErlfF0NSQbfh+O3xs9ApxM8/cXGMKvjNZNn8YsvNnLFaWOVPzhJF2Z+1H8IyX5jA6vBGNJns7f8HO9Pfox+aRkU5g/i05OH+NnA4czbvj5k/2tLinljzN+RZDRhk1y4GxuRZRmr0cRzQ8fg8niY1vd+5m//hGWjprJk75aoPUIrxk3ntaIvlD6fZaMKcDV6lIClafAQyMIEbLufHTKa0ppK3iz+is1nSsiypvDNlUvodTqeHzqGuf2HMGfrR3SzJCsBlcMtYQ0KNuo9bsXcIVjjB1Nmh5SWeWUvTrc7pGdnUJcsRjpyQ4IonRB4ZW9IpiZQAhd4DBoaGrdHi8GOEOLfgb8DrsmyPMB/XzrwCZALlAE/kWW5xv9msQIoAJzAfFmWv/FvMw/4jX+3r8myvCa2D0VDQ0ND4zvMvwL/F0j23+4M3JBl2eO/fQnI9v+dDVwEkGXZI4So9a+fDRQF7TN4mxCEEAuABQDZPbL55vCh4IV8f+AgTpedo1ePnhwpORaybc/sHjS4XNgdDq5dr+LGjRvKMrPZTP49fblYXk5G53RKz5wJ3bZXDv/7nsHUNThZu28nXQxmns8ZSIE1g/01V2k0G/k/gx7my9ISMjxQpdNhd7upMRgYcG9//rvkCI8OH8mIhCRG5NwsDdtfV4nb4WTpsIlcrK5k7zd/5add78Fms1F+rowH7h/AmbJz/N0XnzC/W1/6AL/MGYAOaHQ2UJCRw9XaGww0JHGmtJTneg5gfkZvuqam8b2+/bhwpYK1f/sae2UVncyJDOqSBcAfLpZgcXn452mzWLdvNyMSknCWX+bhhCQs6d055ayjk96Ix+7AWV7Bcz0HsKf4AH1cvjr8rm64JzWd8sQU9h/6G/+U/yCDumRhu3GD642NdEpK4ur169TW1uL2NnKkqoIP7LUcFxIv3jecgald+K+/HSRTl8DWH0zH4/Xy612bsdRLbL9yHFOdg/tTO/O9zGz0QkdKSgrdu3bjxo0b1NbW8sugc6gDTAjOnj9P39zefHvsCPWSxOGqCm401FOdILPw+yPxNjby+gNjcPWx4fRIbC3ah9PjpnDkOGqvXqNXj54cPV4S8rzn9OiJ5JZITU7hSuU1amtrlWWJiYn07d2H8ooK0jt14vS5syHb5t/Tl2vXq+iV3YMTp0/hcrmUZWmd0sjonI7d6UQndFy6XK4sq7XV4WlspOziBfrm9g69xoGsbt3QJ+gxGPTU2WxUXb+uLDMYDAy8rz+ny86R3S2L46Whk+5798rF7rCT0bkLF8svYbPblWXJSUn0zO5B5fUqkqxJnDtfFrLtffn9KL9SQd/c3hw5XkKtrU7R1qVzZ1KSk3G7PXgaPVRcuRKy7fcHDeZ02Tlye+Zw+NjRkGU9umfjlb0kWSxUXq+m5kaNssxkMnFv3zzOl18is3MXSs+cDtm2b+8+VN+4QXZWFqfPnaW+vl5ZlpqaSreMTGptdUhuKew8DrivPxfKL9E3tzd/O3oEOcjVr2tmJmaTGSHAWV/PtcqbGUFdQgIP3D+A02Xn6Nk9m2Mnjofst1fPHBpcDaSlduLy1SvU1dUpyywWC3165VJx9QqdUlM5c+5cyLaNjY2cOV9G7545lJw6GTK8Mz0tnfROnZR+mfKKyyHbDr5/AOcuXqBPTi++PXokZFn3rCx0QofJZORGbS3Xq6uVZQajgYH3+q6Z7l27ceJUaci2fXJzqbPZ6JqRSdnFCzgcDuW5T05OpkdWd6LRmszO+8BbwAdB9y0BdsqyvEwIscR/+1fAFCDP/zMc3zdow/3B0cvAUHx55WIhxGZZlmvQ0NDQ0NC4A4QQgS/kioUQY+/GMWVZXg2sBhg6dKj8/UGDw9bpm9sb8H3AasrF8kskWa0kWa2Q0ytsee+cnIjbOt0SPXqYSTaYeOPsIabd059xfYfz8PeGMMctcaOhnm3njvNIbr+QMrJVkwq51uDkwXv7U1ZbTVGjPSQbNDI7F1sCPLRuhVKS1TO5E8VHDrGr/jpTVr7CN3P/gXszuvEvF46GbDdNdrO18gILBo3A2sNIaU0Vl2sqeejee1m4YwNFe/+k7PP1up1sLP0rIx25LB87jW3/vRmAlUYzx4XEPzw8mn7pmaTl9OT0gV2cq76sHOee/PyQjBHAVQM0NDaSmpzM+EGjqHDU0cmYiMMj8emxg0zt05/8nF443FLI4E6AV4A1k2fx6clDzLpvEFb/8M6Ea2f5/ILvg9ZHV8+iryyjYuJUdEIo2zYm6EhNTQ07F78QN5/3vnn5PB7kigdwQnLwy5wBJCRbeeXbL0OW/cVZxUcFczAaDBGvmQD39MqNeH9uz55A5OstyWoF4P5+90bcNsmaBEBmly7Kfd8c/v/Ze/f4qOo78ft9JjNnJjOThIQECAkhIgmCJGihELsgBAgLYQslhnJRpNuKoOWpK0/9iVvRVnSl629lcSkKaB8QLFgpLFQu5SqXaoJQIUGEBCHcEkhCbpOZzC1znj9m5pBhJhh1LDnT8369eJGc27znZCbJJ9/P5STaqKjbvo79dImNIy0lNWj77c6Nj4sDIKPP3SGv2aun9+8M8bd5b2X1H8DfSk6265bcrfvXcvJjNpnl92Bb+vjeq6HOjY3x/p2lf0ZmyGuajEauXb8e8ly/0/0Ds9p1SugSH/IX+o48H/8xt9I7NfRr5m8lJ+XX2cB+/UOe63++odLIOuIUFxMrP35Hz+0S633N9Lu7r+x5u8fw85WVbZIkHQLqbtk8GW9KAL7/f9Rm+7uSlyKgiyAIycA/A3skSarzBTh7gPFfafcNGHTvwK8+qJOgFFfVM/woxVX1DD9KcVWKp49/AiYJglABbMSbvrYM788g/x/1UgH/n62vAr0AfPvjgBttt4c4J6x8k/vb6vHg8niIFQ00Oe28nVfIr4aN4Rf7/1cePNkqeZhwV/+gpgVz92zCg0R6XAK/PXpAnmHjL35fPnoKrx7dLx//1IGtWJwOEnqlsmDISE7MehqjVsfy0VOCCv6vWhopzMxm1s4N9Fy5mIWHtzMitQ9zb5nh89SBrXLjA/9MHT9Wl4Pnc8ay8PAOkt96iac/2sYLOXly44S3x01FEISghgor8wpZVVLElE/+zJSta3C1tmLQann382MUZmaz8PB2nty7WR6GWv3EiwHNBGL1esbfdQ+PtBne+XzO2ICGCP60OD8eyUOrxxN0L27tjmZsp4vdsOz7yIxPCrnvu5yb83VRyvcApXiCclwjzfOb1ux0lySpyvfxNcAfOsupAT78KQDtbQ/i26YGXL1WSd/0Pu2mBkzsmsqJ5jr+VnJSXn7+W8lJUvRG7jMnsLeuktNlZwP2/TCxF8ebaqmqvk62OZ5xCSkB57vcbqYk9WZLzUUW9Lo3wLlH9+6IOpGoqCiarc3U1N7swtLisPNP3x/GuYoLpCb35PTZMwHPJz0tDVtLC13jE7hSVYnFYpH3mUwm0nulcb2mmtiYGM5XVASce09GJpXXr3mXec+cxuW82Qqza0ICXeLicDiceCQPlVVVAefeNzCL85cuclevNE5+fgpriw1TtBGAFN9fFaINBuoaGqirvxkHi3qRARn9uHD5Ej2SunH2XHnAde++6y4aGhtJ7t6D8xcrsNls8j5/akB9YwMGvYGLlwN/CNx7T38uV171pQacwtN6s164W1ISxuho7xJ+fDzXq6vlfYJGw/0DszhXcYG0lNROkRoAYDab0Wg0nTo1wOVyyV/7zp4aUFtXh/uWSeKdMTWgX98MTpedZXD2oE6dGgBgbbHRo1t3UpN7cqP+1r93dS4kSXoOeA7At7LzS0mSHhYE4QOgEG8ANBvY6jtlm+/zT3z790uSJAmCsA34gyAIr+NtUJABHP0unC/4UoM6ikfyBBXtvzthOo/u3BhUN7I+f2bIX6RTzXFct1q4ZrPwSvE+1oyfRqxowOZ28cuP/ix3HfMfH6s38D8HdrK8olSuLYk3RLNuwgzMop5GRwtHrlxgRGqfoBqdGF9Xtlsd/F3P/G2U/TQ7nQGzg/xDNNfnz6TB3oK91c1jtzQLMGlF3jz5Ma8ePeD9Gex7/usmzGBinwE8dWAr3YxmXvqnf6amnWYCTQ5HyMddOmoSGgSeHZpLelwCzS4HrR4PURoNNpeL2bs20s1oZsmIfDLjk3x1UaJcc+PtmucMqqfKSU7j5LkyzEmJIfd9l3Nzvi5f9zV6p1CKJyjHNdI8BUmSvvogb1vOD9vU7DRIktSlzf56SZLife0+l0iSdMS3fR/e9LZRgEGSpJd92xcBLZIk/d/bPe6QIUOkY8eOfaVfWzweDxpN+wtWXX/3IgA3fv6bkB/fbl9HPvZ/Hg7XzoLqGX6U4qp6hh+luN7qKQjCcUmShtxBpQ7RJtj5F0EQ+uANdBKAz4BHJElyCIJgANYB9+PNXJguSdJ53/m/An4KuIF/kyRp51c95nfxs+pWLE5HwKBQgOonXqTnysW42wTy/uGeU7auCUpTWzt+OjGiXg6aehhjeD5nLC5PK09/tC3o+KWjJjHsvTfwtNnmD7DaBg2pMV1IucXjyPSfBzQZ8J/vbz6wKq8QQ5SWR32DNCvnLgr5XPyDRxce3hF0rfX5M+n7zhLcHg8awNPm+UuSRM+Vi/nox/PQR2lDPr93J8zArBPbvYe1LdagjnaJ0SYEQSD5rZdCnuNvNFDbYpNXl9qmE64eNxWdoOGdU0dD7utM7aSV+r2qM6MUV6V6tvdz6ps+k+u+9DR8//v/lN5eCsDfLTXg1r94dmaU4qp6hh+luKqe4UcprkrxvBVJkj6SJOlffB+flyRpqCRJfSVJmurrsoYkSXbf5319+8+3Of8VSZLuliSpX0cCnW/K172/oQaFtjf/xupysmb8ND59+BdUP/Einz78C9aMn8bbpcU8uXczhigta8dPZ0VeAV2jjSQaTUHpYavHTeW3Rw+woE0BvncwqYElI/KZdPeANulu9iAPf7vqW1PeMuITvasmgoAHic2TZlP+02fbHaJZVl/bbsqXSSfenDfk8/S3pbb4ZhFlxifROza+3SGrNrer3RlCtw4zfXzPJqwuJ7Z2XP2pbjaXizm7P+DVowd4pXgfS0bkUzl3EesmzCAx2sj58jLmDXqAXjFdWDdhRqedm6OU7wFK8QTluEaa5zd9V/lTACA4NeBRwUsO0OhLd/sLME4QhHhBEOKBcb5tKioqKioqKrfh1kGifkIFFKvHTWXjF5/R7HLy9EfbeHLvZgDi9NFM7TeI53PG8uiujfzx7AmaHHZutNiYtWODXCezePh43sufiU7Q8OzQXMb1zpBrXHKS0zhb511l+dWwMW3qXgxBHrMGDOa61SL/Mr901CQW/XUX3d/8Dd9/7w0e2/0B9fYWyuprmbVzA62Sh7fGPhQyOLKEeO7+oG6l73GTTTF8+vAv2DL5J2g1GsSoKFblFXKxqZ6LTfXtnh+q9mZVXmHI4NI/NNXYzsBPf71O21qdzeWnGL5xBT1XLvYNDdUg4B0cGqXREKs3yINEO1Ogo6ISSXSk9fQGvGloiYIgXMHbVW0J8EdBEH4GXAR+7Dt8B9620+fwtp7+VwBJkuoEQVgMfOo77iVJkjp3EriKioqKisodpm1K1K2DPWcNGIyoiWLpqEn0jo2X58yM6Z3B/H1b6GY089yw0UGpUv9yV38K+2Wj1UQF1avM2f0B6/NnotEIbC4vpfVaDR+f/4zlo6cgRkXx/F93tZl7k0+1rRmLw0GMTh9UyzOsZ290migAHtiw3LsaNP1JMuOTaHLaiRUNWF1O/jRpNpea6kk2xbI+fyZGrY4rzY24WlsRAIvTGTTgc/noKfzyoz+TEZ/Ie/kPU/p5KU9/tC3gecbpDcSIepytrSwfPYX5+7fI+1eMKUCDwMrSYqb1u4/1+TMx6USqrE1IkiSvNN1aU9PsdBCrN9x24KetnXM7Uz2Oiso/El8Z7EiSNKOdXWNCHCsBP2/nOr8Hfv+17FRUVFRUVP6BsblcvPv5Mab2G0TXaJN3sKcoYnE4MIt6BOCeNa/J9SMnZj1NelwCRVWX+OjH8+SubHAzmPHX3WyeNDt0RzCtjqXHD/FY1jC+1J9lUkoy7589wdR+g+QmBv5mAyvzConSCDS5HN42021qed79/BgFGVkkRpt4ZsjIoBqVZbmT2VRWQmFmNpvLS3n03iF0NRipsTWjEQTm77sZnKwZP42146cTqzdQ0VjHS0V7ZJeCjCxKaquCnufSUZNIMBiJEfVYnE45oLlsaeClT/awYmwBhZnZAUHQstzJdI/xliTfGlz6V3zg9gM//Ss/c25pqNC2U5uKisrfj4hbM+2ZnHynFTqMUlxVz/CjFFfVM/woxVUpnkqlo/c3Wqtj2j338Yv9/0vKysUcvPwlzU4H5+prsbmcQTUnPc2xAfUqIVOxdHqOXK2grL4mZHpXZXMThZnZzN61kRdKjrDw8A4KM7NJMccFHGd1OYkSBFo9Usg20xP7DKB3bDyrSoqYkz0sqB2295j+8v9zdn9AWX0tNreL+fu2BBz7k13voxEEJEnigQ3LAzrH9Y6NZ9v1iqDn2Ts2njq7jYtN9bx/9jOuWBqQJAmby0lGfCJWlzOkk81Xl5NkNLNuwow29TYmojpQtK0RNPLKT6h6HKW8t1TP8KMU10jzjLhgR0k5r0pxVT3Dj1JcVc/woxRXpXgqlY7eX6vLGfCL/0/+8kdMOj29Yrswa+cGfvnRn3l73FRKZi+g5slf42h1I0kSy0dP4ZrN0m7RP8Drxw8FzdtZljsZD5IcBDhaW+UgoNnpCJjL0+rxYHU5228zneCttymvryVGNAQcU5AxkCUjJtIvoRtLRkwkwxeYZcYntttQwKQTqWmx8syQkQH7LjbVc3eXxIBt3udZQ+/YeM433GBO9jAyfOlzmQlJzMkedtu6nFk7vXVMs3ZuoLbFilEn0uJ24ZEkmp0OPJKH2+Ff+QlVj6OU95bqGX6U4hppnsp4Nl8Dvb7zDOT6KpTiqnqGH6W4qp7hRymuSvFUKh29v6GGYTrcbqwuJ5snzea/Rv0QfZSW7sYYLE4HBq2OKI2GeEM08frooEL6lXmFbD/vnTe2ufwUrxTvY+moSXITgVeK95FqjpODqlZAVgAAIABJREFUgAa3t8OYvxFB1bwXeHfCDLoYDERpNDx1YGu7K0RWp5O3S4t5dmhuQLe1goyB/GrYGBYe3k7yWy+x8PB2an1BTFl9TbvXK6uvYe6eTczJHhbwnEw6kV8MeTAoaHv9+CFqbM0MTEpmdUkxVywN/GTX+/R8azGP7twoP+atj1PRWBeiC5uDh32DRx/e8QdqW2xfGfC0h1LeW6pn+FGKa6R5Rlyw09BmKGRnRymuqmf4UYqr6hl+lOKqFE+l0pH76x8kuvDwdnquXMzCwztYMiIfq/tmp7V6ewurSoqosjbxqG814pEdf6De3oKj1Y1Rq2Pt+OlUzXuBNeOncaqmimn97pMDg2pffcz+i+UYdSLVtuaAltYZxlgAuRPblK1raHLY5ZWLoqpL7a4QmXQi5fW1pMclECUIctezBYNHBqWPzd2ziceyhmHW6cnw1QKFCl5utr+eSOXcRSwZkc/zf93Fn04dZ33+TCrnLmLt+Om8WryfalszRp3I3D2b5HS5W4OYWwMnf8vttrRN/fOfO2f3B9huGWQczq99Z0D1DD9KcY00z69sUKA0btTV0Tu111cf2AlQiqvqGX6U4qp6hh+luCrFU6l05P5aXS4e99XCgLfwvtFhlwdk/m3WvzF//xaWjMiXU938x83fv4UVYwvQCq0BRfbLR08hVtTz7gTvgNEmh4O3S4t57dhB3s4r9DZA0Ilycf59UWZs0SKrx03F4XbTzWhm/v4trM+fKc/X8dfPLBmRL3dae6f0KAUZWTw7NJeKxjrMop4uBgNLR02SGyi0xb9yNNs3ZPSZISN5d8J0zKKesroaXine523jnJJOk8MeNLD0PzK/T42tGbOox6wTeTPvIZocdkyiKDdTCD1rx8CKsQUkm2I5W+cdGXjNZgk4rm3qX9tzjbpv9tdvpby3VM/woxTXSPOMuJUdFRUVFRWVSCBUTYm/nqUgYyC9Yrrc9hf5ZFOsHCz5VyTm79+CIAj88uCHJK34Dc8c+pCCjCyq5r3AvYk9aJU8vFNazPN/3cWKsQWM7Z3pnZFzZBfz92/hhQfy6GGMwaQTOXT5vLyis+3L0yw8vIPLlgbeKT1KYWY2vz16gPS4BF49up85uz/A1eohyWhud4Bo2/SxV48e4NGdG7E4HCw8vINtX56Wa4XEqCjenTCd54bmyisy2YnJdDfGkBhtxKDVYnU5idUbaHLY5RS49h7T1dqKt5ksCBBy7o4/9a/tuf4hoioqKp2biFvZUVFRUVFRiQRCzXrxD8h8ZfgE+Rf5K82NIee6WF3Odgvw+8UnyWls8YZorC4nvWPjaXY5+VnWMMb2ziRGp+fD65f598Ob5fPn7/OuGNW2WOkbn8QrxfsCVnRiRAM/yxpKrGjg2aG51LZY2Vx+Cq1GQxdDNDaXE7dvkGfbls+rx01l0ZFdQa4xol6+fll9DS8X7WXF2AJmbH+PlXmF/NvgBymvr6H45GcM0+kAidoWm9z2+ZkhI1mZV8j608dZljs5qPX1K8X7qLY1s2b8NBYe3sHqcVPZdeFMwHM6cuUChZnZHL56IaANtdpKWkVFGURcsKMT7/w3H7vbhUGrC/r4VjqDa0dQPcOPUlxVz/CjFFeleCqVjtxfDUJQUBAj6nlzbAHRWh0rT37CstzJ6DVRQcctHz0FkEIGQRWNdfwsayhPD36QOnsLOo0WCQlBEGh2Ofg/Bz/kms3Clsk/4ZK1KcDJv2J01dJEZnwim8tPyWlszw3NZdaAwQFpc2+OLaAgY6B3+KjTzo0WG09/tI1uRrMcUFxsqseo1YVMH7vYVM/wjSvkbcNT0imrr5HrfJaMyGf4xhUs6jsYm8uJBMzZ/YH8nF/11d/8LGsoMaJBTtMrq7+ZGqfVaIgVDQHzeb7/3hsAVD/xIo/t2cSkuwe0CbpqSYw2feOOVUp5b6me4UcprpHmGXHBTtY9A+60Agatjq6/exGAGz//TbvHdQbXjqB6hh+luKqe4UcprkrxVCodub/ROh1nK6t99TXeYZonqysZmpyGSSfys6yhHL92hdG9M3inpJg146cRp4+m0dHCprMl/GvW0KDVjOVjpvDyJ97VkXl7/8R/PjiRerstMFAaM4WXPtmD1eXkFI4AJ/+KUU+zt/vb8JR0+bzHsoYxe9fGgNqhJ/Zu9tUOaTh0+Tz5ffpTVHUJt8cjB0lajYarcxcFua7KK0QjCAGP4V+NgZuDTYenpDNzxGjfSosQtJr12rGDLBgyEqvLCQIUbFsbFACW1dfI10yPS5Af07+S1jaoG56Sznv5M4OGiXYUpby3VM/woxTXSPOMuJqdcxUX7rRCh1GKq+oZfpTiqnqGH6W4KsVTqXTk/jY7HQzp0YtHd24EYN/FcgYmJcszYH6y630GJiVjd7sYf9c9/GTX+/L2yRkDaXE7Ka2pkruxLR01iRidnv8a9UNsbhcrfcHE+2dPBNb17NvCgsEP0mC382L/nID6leWjp2B1Omh2OTh85TxLRuTLXdFi9KHn7SSbYll3+jhZSclyyl1bcpLTKPettPivtz5/JutOH8fudrNkRL7sv7viLM/njKXmyV9z7mcLsbvdrBz7ENWVlThbW4NaXB+Z/iSVcxfR7HTQ6vGw6mRRyM5xrx8/JLtYXU55IGgPU0xQ++7V46Z+qxQ2pby3VM/woxTXSPOMuJWdnt173GmFDqMUV9Uz/CjFVfUMP0pxVYqnUunI/TXpRATBu1JhdTmZ2m9Q0MrJ3D2bWDdhBvP3B3Zj86djDU1OQ0Ji7p5NaBD47YMTqbe3BK3knK2vkVcu/CsmS48f4pGMbJZ2n0Tv2HguNtVj0GrRajRECRqGJqcFpKy9O2FGu7VDrx49wOGrF1g+ekrwatPoKbxUtEdePRmeks6SEfm8duwgTw9+kJ4rF/PMkJH8LGsoE/sM4Ml9mwPOFaOi2HfjKqmpvdD6UvreP3uCwszsoMc52yaoykxIotnpYHVJsdwAYVVeIUbfrCIAo07EoNXyXv5MjDoRm8uJUaf7VkMXlfLeUj3Dj1JcI80z4lZ2zpSX3WmFDqMUV9Uz/CjFVfUMP0pxVYqnUvmq+9vq8WB1OWl2OshJTkMAYvWGdhsOhNreOzaex/dsosnpYHP5KZ4dmkudL2Ut1EqOH3+tTGFmNp98XoKj1Q1AYrQJk07P8WtX8EgS604fZ834afKqi7PVHdTJbPnoKTTYW2SnnuZYXi3eLw8yXZ8/kxhRT7WtOWilJSc5DYvTQdW8FyjIyEKr0fDkvs1B3eWsLif3ac3E6g28dfJjzKLIY1nDgubqzN/vfZ6by08xfOMKer61GJNOz5zsHN/q1ETWnT7ODXvgwFD/TCGNIPj+/3a/OinlvaV6hh+luEaaZ8St7KioqKioqCgdu9stz3FZlVdItFYnBz63rpw0+ebdhKpDKaq6RKo5juEp6aTHJSBJUsjAKNPXnc1fKxMj6llx4mNa62r48jwkZ8USo9fT7HQwrGcaRq0or7xIksQDG5Yz6e4BvPxP41k6KnAlyO52U/3Ei1xsqqfF7WLF2ALK6mt4/dhBCjKy+O3RA/Ksm4rGOnkg6LLcyTx7aDtv5j1EWmw8GiG4Hscf1NXrdDQ6WmQnoZ1jM+OTAu6RzeXk0Z0bAu7d4asXvlVNjoqKSuci4lZ2VFRUVFRUlIxH8uDytFLbYsXmcmLW6bG6nBy8/CWr8gqDVk4kSWL5mCkh61ByktOwuV0sHTWJZqdDLrhviz9gqpr3AktG5LPu9HHcHg9PDPoBY3pn8MiAwczetZGeby3m0Z0bsblc1NltckDVtoj/+b/uClgJAnj/7AnK6mvpHRuP2+Ph96VHWXh4B9P63cerR/ezqbyU7LWv8/qxgyRGm3gz7yHeGP0jXinexzWbBZvLSZ3dRkVjXUj3i031tEoSGkHg7bxCWtwubO3M8rnYVB9Qe9Peqtg3HRiqoqLS+VCDHRUVFRUVlU6EP5j4tOoSLW4XBp0Ws6jnmtWCWadn6ahJclOAl4r28JNd74MEb4z+EVXzXmDF2AKihCjeGvsQ706YTrRWR4LBiFEnEm+IDk41GzOFd0qPcraumuEbV/DasYOYdCJWtxO7283cWwaTPr5nEzpNFKvyCjlXXxtwTf9w0Rt2G1GChrWfH6MwM5uFh7fTc+ViZu/ayKS+97Jh4sO8XLRXrhMCb9c0s6jn9WMHGfreG1TbmlmVV0ij086c3R/w6tH9vDm2IMB9xZgCYkQ9p29c59GdGxmanIZBq8Oo04VsLNDDFEPVvBd4L38midHGdoMidWCoikrkEHFpbH3S0++0QodRiqvqGX6U4qp6hh+luCrFU6nc7v4adSK9Y+PpZjTT6LSDBM0uBz/LHgbAAxuW4/bcrCnRajT0NMfSc+Vitv/op2QmJGEWvSln/lWhw1fO8+H5L/iPERPoGm1kff5MTDqRy5YG3j/jLeb3t3T2r9gsPLyDNblTKCreHuDnH/bZYnMxPPUu3ik9yrR+98nXrLI24XC30tWgZWKfATx1YCvdjGY++vG8m3N1dMFzdZ4ZMpImp50FQ0Yyd9ADaBB48+THPD34QYqqLjHp7gFECZqANDmjTscHZ0/ydsUpvmyx8LivYUOs3kBitLHdxgL+FDV/UOQfQuofcPpdDgxVyntL9Qw/SnGNNM+IC3aaLBa6xMbdaY0OoRRX1TP8KMVV9Qw/SnFViqdSud39bXY5sDmdaDQaqixNaAWN3PXskxnzQ9bnWJwOrs5dxI0WK7N2bgiYS7OprIRZAwYzNDmNn/7ljxRVXeKZISN5LGsYvWLieCxrGG+X3uxI5p9lU1R1idrGhnbqgWrpl5CEJEm8duygPLwTvMFX5dxFlNXXkBmfRA9jDM8NGx3QGW31uKmsyiuUn9czQ0Yya8BgZu/cGHBMeX0tZfU15CSnsWDwgzzuW2Xy4+/ctu/zk3zZYpEbNsDNxgJAu/U3GkFz26Dou0Ap7y3VM/woxTXSPCMuja17Urc7rdBhlOKqeoYfpbiqnuFHKa5K8VQqt7u/oiYKo06kyWEnMyFJ/gXf7fHw6tH9ITuePXPoQ8rrawKOPXK1gqcObGVin/48vmcTVpdT3ne2voY6uw0QEASBJwb9QE6Ne6V4H5vLT5GTnEbXhMSgOqFluZPZfv40FY11cnOEtvhrgA5fOY/F6eDZoblBndHm7P4Ae6tbnqvzeHZOkPuc3R/w7NBcXj9+iGW5k8mMT2q36cDRplr5sZudgYNQv4pwd1v7KpTy3lI9w49SXCPNM+KCnYrLl776oE6CUlxVz/CjFFfVM/woxVUpnkrldvfX5fFg1InEiHpMOjHgF/zN5afYdeEM706YQeU87/BNf+3L7YIBf9cy8A7b/NWwMTz90TaS33qJR3duoNHZQpPTzsLDOwJWePaV/o04vcH7eL72zJvKSijMzOa3Rw8QKxpCDuk8cuUCP+53HzGinvS4hJBeqeY4Fh7eQWVzU9Dz9B+THpdAta2ZV4v3Y/F1o2uLv+nAD5N6yXNyTJ28uYBS3luqZ/hRimukeUZcsGO1Wu+0QodRiqvqGX6U4qp6hh+luCrFU6nc7v6afOlUj+/ZJKdwtWVM7wxWlxRRa7PS6vFwzWahIGNgu6ss/mtYXA5qnvw1/zXyh2wqKwlYRXli72aanA7W588MWOGpa27C7nbT4nJysamezPhEJvbpL3dKK5OHdE6Uu7ntrjhLVlKyt4PbysVUWZtCNwFwu+RgLdTz9KfnvTH6R6wYW4DF6WDFmMAGBavyCulujGF2xn2smzCDrtEmeSBoZ0Up7y3VM/woxTXSPDv3dwQVFRUVFZV/MKwuJyadyJIRE8mMT2LthOk8NzRX/gU/PS6BiX0GYHU5ebu0mLfHTeWFnDzeKT0asgX19vNfsCqvkLdLikl+6yVm7dxAYWY2BRkD5ccsqrpEr5guSJJEja2Z/z5+mGpbMwO7eieUxxuMdI02+o4W6BefJLe33lx+ioWHt3O2robhG1cwIrVPQNrarz/eHZR6tyqvkAUfbaPObuOazSKnqrU9ZqVvlSZONPDk3s3M378FMSqKN0b/iMq5i3h3wgy0goaZO95j78UyZu3cQN0tA0FVVFRUIq5BgYqKioqKipIRNVHUtlhZeHi7XKy/dvx05g7KweTrrtY7tgtilJbXjh1kar9B/GL//3LkagVn62tYPW4q0Vqdt320y8kTg37Amyc/lpsI+Gt5lozIl1s/5ySncbauhoWHt7N89BR+++BELE4H+44V8UlpMbMGDJabCfgHj15uapBT3lblFbLu9HGAoHS6zeWn0CCwPn8mRq2OsvpaeQbPy0V7WT5mCvP3beHV4v0sHTWJ9LgELE47q0uKee3YQblZgcfj4d+P7GRz+SkKMgby2oP/QhdDNEtG5FN88jOOfHGKObs/UAeCqqioBBBxwU5MTMydVugwSnFVPcOPUlxVz/CjFFeleCqV9u6vR/LgaHUHdB3rZjTjkSTqbDaMsSI1tmbiRAOOVjfPDBlJqjkuILiwuZxBrZTL62sDHsdfy6PVaMhJTmP5mCkgweZJs7nYVI/d7aLZ5eCT2ir+bdDEAB//rJ11vjqei031aASBwsxsDl+9QFl9bVAHt2s2C1csDQzfuILhKemsnTCdBYMfZPjGFfSLT2Lt+OnE6g1UNNZx1dLI/P1bAh5vzu4PWDJiohzo/GrYGGbvutm5bdE9QylwW9j25elOPxBUKe8t1TP8KMU10jwjLo0tNbnnnVboMEpxVT3Dj1JcVc/woxRXpXgqlfbur93txizqA4KXV4ZPwOpy8vRH2+i5cjFPf7QNm9uFR5KYO+iBgML9BYMfDNn57NmhuQGP46+HqZr3Au9OmIGoiWL+/i3y9TUaDRnxSeyvryIzPjFk8wCzqKdg21p0miieP7KLTWUlvDthBpnxwR3cVuUVcuTKBfnjD86cJDM+ieEp6Uy75z7eLi2m2mpBIwj0NMe202ghsd3nuPTscRYMflARA0GV8t5SPcOPUlwjzTPigp0b9XV3WqHDKMVV9Qw/SnFVPcOPUlyV4qlU2ru/rZJERWNdQLF+tFYnr3T4f7n31q9oMelEzKJeDi7a68iWHpcQ1K7a0erm9WMHkZBCtn22OO0szHoAq8tJ5dxFHJn+pFzn42/xvHb8dAxaLSvGFjCj//1oBLC5XRytusSa8dOomvcCa8ZP42jVJab3v58146ex9dwpPrzwBc0uJ8tHF/D+mRPMGjAYvVbLprKSdhstNLuc7T5Hp9VGZnzSdz4QNBwo5b2leoYfpbhGmmfEBTvG6Og7rdBhlOKqeoYfpbiqnuFHKa5K8VQq7d1fk07kt0cPBDQaaK8ts0knUtFYR8rKxaw7fZz1+TOxuV0hA4XLlgZWjC2gat4LrM+fSYLByNZzp3jt2EFiRUPI68eKBkb26cesnRvouXIxCw/v4FfDxvDc0FxW5RXiaG3l7dJirC4n75QWoxU01NismHQiI1L78E7pUZLfeomf7HqfrKRkorU67vn/XmP7hTMsy53M6pIiukZH81jWMJKMZrSaKKb1u58Y0RC0MrR63FTMOpH32nmOXWNisbqcJEYbv/M5Od8Wpby3VM/woxTXSPPs3N8RvgEVl5TRGxyU46p6hh+luKqe4UcprkrxVCqh7q9H8tDscnDNZgEJOThpbpOmVpAxkCPTn6Ry7iKanQ72XizzDRs9QI2tmZUnPwnZ1ewvF87gam1lytY19H1nCTN3vMe49H48M2RkwPX95CSnYXU52Vf6t6AhpY9n57Dt3Oc8tvuPFGZms6mshBn978fZ2irP7pm9ayOFmdlMunuAfJ7Nt0Lkb2tdXl9Ls8vJ7F0bSX7rJR7Z8QdA4velxQAsHTWJyrmLWDpqEqImCkEAs6jHpNOxetzUgOc4O7UfZlHs9IEOKOe9pXqGH6W4RppnxDUoUFFRUVFRUSJ2txsNGlblFZIYbeL3pUeZ2f97GHWi3O2sMDObpw5slQvzl+VOBmBEah96x8YzJzuHQ5e/9LWtTsTidPDHsycC2kHDzY5s706YjiDAm2MLeGLv5oDrmkU9DfaWAEd/rc7CIzsBeOrAVtaMnwYIQU0F2nZ8K6q6hEkU6bbiN/K1Pn34F8zZ/UHAOfP3b2Ht+OnM3rUxoMHB8JR0ucuaRtCQGG3kvfyZGH0zic6e+UIRgY6KisrfHzXYUVFRUVFR6QS0ShLRWh0aAeytbn7Y914anC1EOQV0migeyxoWEAT4Awp/cOAPVFb6gqVm3xye144dpHLuopCpajGigbdLikg2xbJuwgzMop5mpwOjTqTZ6aCLITBNxN/YoCBjoBzExIgGNILQTlOBJPk8q9PJc0Nz5XbS6XEJodPn9KHT6tp2WdMIGrm9tFnUIyB88xuvoqIS0ah/BlFRUVFRUekEGLU6brRYWXHiY1olD3P3bCLZFIvD08pPd/+RmFu6tIE/YNEHpJrN3bOJS031aAUNBRlZVM5d1H7Rv9NBk9PB0OQ0VpUUcampnlk7N5CycjGrSorITkwOGlK6uqSIBYMfBOCZISOpbbEGNVXwX7/Jaaf6iRdZO346By9/ySMDBlM5dxHrJsygytrUrlOo7Z29y5qKikrnJOJWdgb0u+dOKwRgd7swaHVBH0Pnc20P1TP8KMVV9Qw/SnFViqdSCXV/rW4nR6su8VjWMMw6b2BjdTnlOTpl9TVB82tyktMoCzFDJz0uAYAEg5Glxw9RXl/L2+OmYnE66B0bz8WmeuIN0RiitPKK0ZIR+XKqW0HGQCb26U+aMZZ1A+7FpBMpq6/hleJ9bPvyNE8PfpDhKenyud2MZpblTg5IsVuVV8g7pUfllZxluZNZf/o4BRlZALx6dH/QOavHTcWkE1k9bmrQrKDbdVlT0utVKa6qZ/hRimukeUZcsHOlqpK+6XfdaQ0Zg1ZH19+9CMCNn/8mYF9nc20P1TP8KMVV9Qw/SnFViqdSCXV/TVqR7yen8XZpMY9n55CTnEaDvQWPXiInOY3Xjx8KCg5W5hWy/vTxgOvkJKdR0VjHAxuWB6S13Wix8vRH2wICCJfHI68Y+Vs6+4d2PnVgK91dcF0Hy3In8/rxQ2wuP8XwlHRsbhdLRuTL57o9HgCWjMgnMz4Jm9vFypOf8OrRA0DbGp6J9I6NB2Bz+amgc0w6XciaHKNv+9e5n50VpbiqnuFHKa6R5hlxaWxK+OL4UYqr6hl+lOKqeoYfpbgqxRNAEIRegiAcEAThtCAInwuC8JRve4IgCHsEQSj3/R/v2y4IgvCGIAjnBEEoEQThe22uNdt3fLkgCLO/K+dQ99fqcjJ3zyYm9umPSSeyLHcy7589gU7jbVpQbWvm1eL9LB01iap5L/DG6B/xadUlZg0YHDRD59Wj+wPS2ixOe8hZOh4kLE4HzwwZKae6tR3a+afqCjlQWTB4pDwUtNXjYfv5LwLS4zaXn2L4xhUUbFsLwGvHDgY8P/9g0ItN9Vxsqg86RwA5oPHX5GgEQW5K8HXvZ2dFKa6qZ/hRimukeUZcsHPy9Od3WqHDKMVV9Qw/SnFVPcOPUlyV4unDDfy/kiQNAHKAnwuCMABYCOyTJCkD2Of7HGACkOH79zjwJniDI+BFYBgwFHjRHyCFm1D319xmhcXudmOI0jJ30APy8NAVYwt4M+8hAJ7Y8yd+sf9/ua9bCtvOfS4HQOvzZ/Jy0V551QS8QUacPrrdWT3n6muZNWAw75QeZVnu5IChnT9PuUc+tl9CEusmzMAQpeWPZ0/wSJtz2gZbK8YU0GBvCVl3Y3E6iBMNxBuig+bofJthoEp6vSrFVfUMP0pxjTTPiEtja3W777RCh1GKq+oZfpTiqnqGH6W4KsUTQJKkKqDK97FFEIQvgBRgMjDKd9ha4CPgWd/2dyVJkoAiQRC6CIKQ7Dt2jyRJdQCCIOwBxgMbwu186/11ezzYXE5yktOos9sQo6JIMpqxOB1YnHY8koROE8X2L08zIrUPb+Y9REVjHS8V7fEGNkd2MjwlnTXjp3nn9LShbdH/rfU+TQ4793TtxsM7/sCRqxWcra/hPx+cKB8bHaWVjz1bV8PCw9tZPW4qE+7qz1zfStHZ+ho5Fc3qcvLMwQ/xIIWs4RGjonj60HZWjC34WmlqX/d+dmaU4qp6hh+luEaaZ8QFOyoqKioq/7gIgpAO3A8UA919gRDANaC77+MU4HKb0674trW3/TvH5nZi1Imsz5+BzeXi0Z0b6WGM4fVRk6iz25i/fws9jDH858iJPLpzI5snzeaBDcvlWhm42Up67fjpxIh6yupr2X7+NIWZ2Ww881lQ8LEsdzJvlxbz6L1D6GGMAW7W0fiP1YDche2V4n1y+tv6/Jny6s/m8lNsLj+FVqOhat4LXLNZ5KCqbRD0/pkTTO9/PyvGFmB1OX1DQAW5hbSKiorKd4Ea7KioqKioRASCIJiBPwH/JklSkyDcnL0iSZIkCIIUxsd6HG8KHCmpKfyt5GTbnXwvK5tzFRfondqL0ltSLXqlpGJ3OGi2Wqm+UUtDQwMSEq2ShCiKLD77KXF2N79+MJdTpz9HqxFY0Ote6uw2BFcr8Y5Wzty4zjN9BuFyuuTrJickcqWulv8pPsDJG9eZmtyHHyQm89mpEm7U1bDZYec/Bg7nnsl3c6zkJLUXL/F9jZGDx4/yRMZ9nKm8gsPTSn1tLZ+WnOC59EHYrFZ+YDTySvE+pAYLXbQiP4gyU3bmDC/1HUydvYUPay/TXYwm2mzi9LlyFvbOpsSURIO9ha2ffszDOSNpbbbyYLde/OHgHhrsLXQxRJOdmEx2v3uoqq6mb/pdlJ45HfB8uiYk0CUuDofDiUfyUFlVFXAf7xuYxflLF7mrVxqNlqaAr0FKck8Aog0G6hoaqKuvk/eJepEBGf24cPkSPZK6cfZcecB1777rLhoaG0nu3oPzFys6KrrMAAAgAElEQVSw2WzyvtjYWHp270F9YwMGvYGLlwNTA++9pz+XK6/SN/0uTnx+Ck9rq7yvW1ISxuhonC4XV69Vcb26+uZLRqPh/oFZnKu4QFpKKqe+OB1w3bTUXjhdTuJiYrlWU01jY6O8Lzo6mr539eFqVRUJXbpw7sL5gHMz7+5L9Y1aeqekcuZcOQ6HQ94X3yWepK4JNNtsaAQNVyqvyvsaLU24W1upuHyJvul3Bb7GgeQePdBGadHptDRZLNTeuCHv0+l0ZPUfwLmKC6T0SOaLsrMB597VO51mazNJXRO5fPUKluZmeV+M2UyvlFRqbtRiNpm5cLEi4Nz+mf24eq3K+5r54nTA1z6xa1diY2Jwudy4W91UXbsWcO73sgdxruIC6b3SKPn8VMC+1J4peCQPZqORmht11DfUy/v0ej339M3g4tUrdOuaSNmX5wLO7XtXH+oaGkhJTubchfO0tNwcxhsXF0ePpG40WppwupxB93Fg/wFcunqFvul38dmpUqQ2f8Do3q0bBr0BQQBbSwvVNTXyPk1UFPfdO5BzFRfo1TOFz898EXDd3r3SsDvsxMd1ofL6NZqamuR9RqORPr3Tqbp+jS5xcXx54ULAua2trXx5sYK7eqVxuvwsTsfNtu8J8QkkdOlCi90OwNWqyoBzB907kAuXL9EnrTcnTpUG7OuZnIxG0KDXizQ0NnKj7ub7UifqyLrH+5rp2b0HZ8rLAs7tk55Ok8VC96RuVFy+hNVqlb/2MTExpPre86GIuGAnKTHxTit0GKW4qp7hRymuqmf4UYqrUjz9CIKgwxvovCdJ0mbf5uuCICRLklTlS1Pz/3Z5FejV5vRU37ar3Ex782//KNTjSZK0ClgFMGTIEOl72YOCjvEXz4baZ3fYMZtMmE0mSOtNk8OOIAiYdCJHPtrE4h+MQ6MX+Y+KEwHd0zRaHQP79OW/PzvCr3LGBKzUvDvkhzy6c4O8qvKZ5QbDrem8NfYh1uz9E8vvySPJaOJyUwP/ceFEwApPr5gudOnibXHdPSGBnEHf4+Wivcy7O5tfl/41IP3t49ZmnujXj27pvZm/fwvn7BYS4+NZkjsJnUbD/3xcwrNDc0mPS+CypYFkcxzNhuiAgagAw63pvHfvQPk+Zd0zINStBu+iEz2SurV7j/ve1YdePUMvwsXGxJDeq1fQ9rt7p7f79YmLiQXgnr4ZIa9pjPYOW+0aH1zS5Xe6796BIc9NSU4mpYf3X3vnhnK61ftW/M8x1LlmkwmAe9tp1Ws2mQHo1uZ9f7nyKtqoqA45dYmNIy0lNWj77c6Nj4sDIKPP3SGv6f96xt/mvZXVfwCXK6+2+7VP7tY9aFtHno/ZZOautLSg7X3Serd7bmyM94XaPyMz5DVNRiOtra0hXf1O9w/MatcpoUt8yF/oO/J82ivk750a+jXT9p4O7Nc/5Ln+59s9KekbOcXFxMqP39Fzu8R6XzP97u4b5Hk7vlWDAkEQKgRBKBUE4YQgCMd8275295tw4n/DKgGluKqe4Ucprqpn+FGKq1I8wfvzBXgH+EKSpNfb7NoG+DuqzQa2ttn+qO/nUg7Q6Et3+wswThCEeN/PrnG+bWGn7f1t9Xgw6URMOlGurZnab1DI7mnVtmYeGTCYfvFJAZ3Z1ufPbHfoaHdTDMtHT+Hlor24PB6505r/uk8d2IrV5WTpqElUzl3EirEFxBuieTPvIWJNZpaPmRLQTGBVXiEmnYgHieWjp1A5dxFLRuTzctFeupliWDG2AEerm7l7NvGL/f+L1eUkVm8I6WbUiWG/n50dpbiqnuFHKa6R5hmOlZ1cSZLaTjTzd79ZIgjCQt/nzxLY/WYY3u43w8Lw+AG0tlky7uwoxVX1DD9KcVU9w49SXJXi6eOfgFlAqSAIJ3zb/h1YAvxREISfAReBH/v27QDygXOADfhXAEmS6gRBWAx86jvuJX+zgnDT9v5aXd4UEY1vZWf5mCntdk/rHRtPwba1rBk/jVjRwMWmeqqtFgatW8rfZv1bQBOCgoyBPDd0NABmUSQjPhGTTgx5XbOoRxAEBEEgRtRz4NI58vv0Z87uD7i7R0+59sbidPB2aXHAoNB5e/8kz99pctiZvWtjwByg9898xojUPiEbJNhczrDU7Cjp9aoUV9Uz/CjFNdI8v4s0tq/V/aZN8WhYcLqcX31QJ0Eprqpn+FGKq+oZfpTiqhRPAEmSjgBCO7vHhDheAn7ezrV+D/w+fHahaXt/jTpvob7V5eRiUz1Gra7d7mll9TUUVV0iVjRQsG0ty0dPYdHHf8Ht8eDxeFda/M0Mns8Zy/z9WwK6oUmSxCcz5vPq0f1yMwJ/VzYxKoraFiuJ0SZG9vKmFr0+YiK/K/+M4RtXcGT6kyw8vEN2ajsotNrWzLLcyXxw9iTr82di0olct1oQgO0XznD8+lXZrW1a3rdpN93e/ezsKMVV9Qw/SnGNNM9vG+xIwG5f0edKXw7z1+1+ExDsfNuiz4pLl4g1x8hFn34MBgOZd/dlYtdUTjTX8beSk/wyzZtP+7eSk6TojdxnTmBvXSWny84G7PthYi+ON9VSVX2dbHM84xJSAs53ud1MSerNlpqLLOh1b8C+yuvXGGiKx+Fp5XLlVWpqby6CNTVb6Nm9B+cqLpCa3JPTZ88EPJ/0tDRsLS10jU/gSlUlFsvNdqImk4n0Xmlcr6kmNiaG8xUVAefek5FJ5fVr37ro8+Tnp2i0NHHt+nXvF7ETF31eunIFSZI6fdEngCRJ2FpaOnXRp8vlkr/2nb3o89KVy/Jr1E9nLPrs1zeDLysu0COpW6cu+gRvcbLVZiM1uSc36r+TxY1/aK5dv07P7j0AsLmc1LZYSY9L4JdHD7BibAF1dhur8gp5fM+mgNqaV4r3eVdE3C7W589k4xefsWDwg7w19iEEQeCdkmLWjp9OlEbDI7520uANTB7fs4klIyay8PB2VowpQIPANZuFlXmFmEU9N1qsQY9XfuUSL+TkoUEImL/jxz9/543RP+Llor1U25op7JfNlK1rWD56CrsunGHpqEmkxyVg9zmbwtRuur372dlRiqvqGX6U4hppnoL3D1zfDEEQUiRJuioIQjdgD/D/ANskSerS5ph6SZLiBUH4EFji+wscgiDsA56VJOlYe9cfMmSIdOxYu7tD8reSk7ctiOr6uxcBuPHz34T8+Hb7OvLxVx33dVw7C6pn+FGKq+oZfpTiequnIAjHJUkacgeVOi3f9meVR5J4Ys+feG3kv7CqpIjHs3Mw6kTcnlacra2YRT1Wl5MGu50T1VcYkdqHGFFPs8uBs7WVx3Z/QFHVJT6ZMR+9VsuTezezedJseq5cHNCaWqvRUDl3Ed3e/A3DU9JZnz8TSZK813bYWXh4e2ADgZR0FvS6l9cvfy4fO6tNAwT/MUtHTUIjCGwqK+GRAYPZdu4Uzx3ZJT+GAGENbL7qfnZ2lOKqeoYfpbgq1bO9n1Pf6juPJElXff9XA1vwTpy+7ut6Qwe736ioqKioqPzDYnU5uWazYNSJzBowmFUlRdS2WGmwt9DosDNl6xr6vrOEE9VXGJqcxuxdG+m5cjGP7tyIs7WVbkYzk+4eQJRGQ7IpliUjJlLZ3EROcmA3qZzkNJqcdgoyBlJUdQmTTmTWzg1oBIGM+MTQtTy+Gh+jTkejw8HqcVMDmhUsy53Mq0f389SBrTyencOpmiqeO7JLPt+kEzGL+u800FFRUVG5Hd84jU0QBBOg8U2rNuHtWvMSN7vfLCG4+818QRA24m1M0Bjueh2fWNgv+Z2hFFfVM/woxVX1DD9KcVWKp1Lx3V+P5EGr0fiGiTp5fM8mlo+egs3lJD0ugYrGOjmYGZHaJ6B985GrFTy5bzPrJszAI0kBjQGWj5kSMg3undKj/GrYGPr5Bn3609vWjJ8WskbI6nKRk5yGxeFg/v7N9DDGyGlply0NvFy0Vx4oatKJpMTEBZwfrgYEHb2fikAprqpn+FGKa4R5fpuane7AFt/QNi3wB0mSdgmC8Clfo/tNuPleVvZ3cdnvBKW4qp7hRymuqmf4UYqrUjyViv/+2lwuBASiBA0GnY4exhj0Wi3zd28JCFxidPp2W0ubdCKVzU10M5rldtLz923hjdE/ChmYHL56gbXjp9Ngb5GvESsaWJY7OWBuz7LcyWwqK2FlXiFvlxbLgdCm8lKGp6SzZER+QJODsvpaMuOT0Go0YW9A0NH7qQSU4qp6hh+luEaa5zdeV5Yk6bwkSYN8/+6VJOkV3/YbkiSNkSQpQ5Kksf62nZKXn0uSdLckSVm3q9X5NpyruPDVB3USlOKqeoYfpbiqnuFHKa5K8VQq/vtr0OrwIOH2eKhorOPZobnM2f1BwByc+fu2EKs3UFZfGzI1ray+hvn7t/B8zlh5e1HVJXrFdCHJaOaJPX/ie+v+Ww5MiqouEas38FLRnjbXqGVTWQlrx0+nat4i1oyfRq+YLkyITyEx2sRrxw4GPG5R1SU5sPGns20/fxqry0nVvBd4L38midHGv1v6mpJer0pxVT3Dj1JcI80z4pJoQ01j7awoxVX1DD9KcVU9w49SXJXiqVT899fhdst1LfsulpMelxBy9cbqcrL9/GmW5U4OqJlZmVfI68cPycFNQYa3E2hOchrNTgeSJHHNZgm4Xk5yGlXWJrZ9eVoeEpoZn8hjWcN45tCHJK34DT/Z9T437DZWnC+R22Dfeg2ry+kbKDqRTWUlzBowGLPobaP9967TUdLrVSmuqmf4UYprpHlGXLBzazvqzoxSXFXP8KMUV9Uz/CjFVSmeSsV/fwUBmp0Omp0OxvTOoMoaurFAg72Faffcx6ayEm8DgrmLWDt+Oqdqqlgw+EEq5y6iyWHnhZxxDE9JZ/W4qdhcTmwuJ6vyCgMCpBVjCogTo6mc571GjKjnYlM9sXoDCwaP5LmhuazMK8SkFflpj77oo7RB11iVV8jBy1/6UtcSmZOdQ2K06Y41IlDS61Uprqpn+FGKa6R5fhdDRVXawe52YdDq5I9VVFRUVP6xaZUktBoNYpSWdFHPOyXFQY0Flo+eIqecvfSDf6a7KQaL08HhK+fJSkoOqLNZPW4qK8c+hMPdSk9zDIIgcKPFKtfunK2r5tef7Kba1sza8dP54OxJJmcM5OmPtsnXWJlXSGK0CQHYfbGMt66VsfgH/yzPyLG6nBi1OnLT+mL0zcyJEUW145qKikqnRP3O9HfEoNXR9Xcv0vV3L8pBj4qKiorKPyYeyYNJJwJQ22KlytrEmN4ZHK26xLsTplM5dxFvjP4RBq2WalszGgTcHg+vHzuIRoARqX146sDWgPqeObs/wKDV8f7Zz7jYVE+Tw87az49h1InM3bOJ4RtXsLn8FEVVl4gR9YzpnRFUIzR3zyaanQ4uNtWjAaptzbRKEjaXk9ePHUQAojQaX6ra3z9lTUVFReXroK7sqKioqKio3AFsLhfVtmaSjGbm7tlEN6OZlXmFRGk0PLrzZovpgoyBLB01iSSjmZUnP6EwM5uVJ4tYMGRkyPqeGL2eaf3uwyx6a4Em9hlAYrRJblAA3tS4i0317dYIxeoNaDUaxvbOYGpaL2L1ehZ89Ge2fXmaBUNGfuf3RkVFRSVcRNyfYnqlpN5phQ6jFFfVM/woxVX1DD9KcVWKp1LplZKKUSdyvuEGJt/gzs3lp6htsZJqjmPJiIlUP/EiR6Y/CcADG5Zj0olM7DOApw5s5dWjB7hutbTbOADgndKjlNfXsPDwdqwuR1DNTXdjDFaXM+Q1brRYeXjHHxBizTywYTlGrcjm8lPy7JzOhpJer0pxVT3Dj1JcI80z4lZ23K3uO63QYZTiqnqGH6W4qp7hRymuSvFUKu5WN80uB0OSe2Fx2uWBnlpBQ22LlYWHtwfMu+kXn0STw05mfKK8EuP2eFg+egrz928JqO9ptNu5b91StBoNTw95kOVjpmDS6VkyIp/M+CQsTgcxop5mlwOTTh9UI7QqrxCtRkMPYwzdDWa5NbW/8cHfa3bO10FJr1eluKqe4UcprpHmGXHBTozJfKcVOoxSXFXP8KMUV9Uz/CjFVSmeSiXGZEan0aKLigpoSqCLiuJf//K+nMJ25GoFTx3Yytrx03F5Wml2OeTAqKc5lif3bpaDmLL6Gl4u2suKsQWAb5XH6eT9MyeY2Kc/wzeuQKvReDu3Oe3EigYqmy0YtFEsHTWJ3rHx3jodQcDhdvPs0FwsdjurfA0L3sufiVGn65T1OUp6vSrFVfUMP0pxjTTPiAt2qm/UYjaZ7rRGh1CKq+oZfpTiqnqGH6W4KsVTqVTfqCWxe3ccrW4+vPAFj9w7mHUTZsgpbW3pYYwhSqMhVm/A6nKyPn8GNTYrgiDw7NBcXj26X67HGZ6SLq/CLMudjFEnUpiZzSvF+wDf/B2XgyNXLtA3PonUmDge2fEHObjyX2N9/ky6mWIor79AUndvS2mzqP+73Z+vi5Jer0pxVT3Dj1JcI82z8/155lvS0NBwpxU6jFJcVc/woxRX1TP8KMVVKZ5KpaGhAZNORKvRsGHiw1icDmbt3BBUQ1OQMZDnc8byyI4/kPzWSzyy4w/YXC42l5eS/NZLPP3RNl7IyaMwI0seMpoRn8j6/JlsKivB5nKyqawkYIBoeV0tWUnJbD9/OmRwVVR1SW4xbbVYOuVKzq0o6fWqFFfVM/woxTXSPDv/dzAVFRUVFZUIQ0KitsXKihMf4/Z45PbPUYLA6nFT5WYCzw0dzfz9WwJaQz++ZxMT+/SXP5+/fwv/d9QPWTIin0V/3cVD29ZSY2vm0XuH4JEk5g56gKp5L7A+fyZdo02YRD2bykoozMzmsqUhZIOCZqcDUyeszVFRUVH5ukRcGpuKioqKikpnp1WSeHzPJpaPnoJZ1FNUdYmCjIF4JAmjVifX0AiCEHLlJTM+KeBzo1bHqD++JQ8W9Xg8ANz9zhLAm5r2xugfYXM5yYxPIjlrGG+XFlNeXxvU5GBVXiEmnTokVEVFJTKIuGDHYDDcaYUOoxRX1TP8KMVV9Qw/SnFViqdSMUcbKaq6RNdoI81Oh9w04K2Tn/CzrKEYdSIF29ayPn+m3JDAj7c7Wk3A55Y213in9CizBgzm3c+PodVo5ABm3enjvHbsoPz5jP73k2yKpdpqYd2EGZhFPTaXM6AJgVJeB0rxBOW4qp7hRymukeYZcX+2yby7751W6DBKcVU9w49SXFXP8KMUV6V4KpWevXrxzJCRNLuczNq5gZ4rFxMrGnjt2EH+z6HtuFpbWZ8/kyhBYPnoKQEzclbmFbL9/Bfy58vHTOHt0mJqW6zEigbmDnqAGFHP49k5VM1bxJIR+ZTWVPFY1jAq5y5iyYiJrDt9HK2gQZIkTKIesyiiEQTMoj5gRUcprwOleIJyXFXP8KMU10jzjLhg5/LVq3daocMoxVX1DD9KcVU9w49SXJXiqVSuX7vGY1nD5Fodt8dDWX0NOclpbC4/xUtFe7jRYmXfxXLMosiKsQVUzXuBpaMm8edzn1OQkUXVvBdYO346yaZY5mQPIzpKh93tnTvRKklECRpeP3aI4RtXMG37ezxz6EMuNtXTLyGJOdk5xOkNCALE3BLgtEUprwOleIJyXFXP8KMU10jzjLhgJ6lrwp1W6DBKcVU9w49SXFXP8KMUV6V4KpWkhK7E6g0B9TivHz/EstzJDE9JZ8HgkTx1YCs/+csfiRENaBCQJInuxhim97+f9LgEmp0OzKIeq8uJxyPRigdBgJUnP6HvO0uYueM9HhkwmOeG5qLVaKi2NWMW9bS4XJh1IgbtV8/MUcrrQCmeoBxX1TP8KMU10jwjLtgp+/LLO63QYZTiqnqGH6W4qp7hRymuSvFUKucqzge1md5cfopNZSWsHT+dfglJciBUXl9Di9vF0uOHqGlplttQz9q5gRstVkw6kSiNhmitDjFKy6tHD8id2ubu2cTj2TlUzl3E0lGT0Gk0GLRaojQd+/GvlNeBUjxBOa6qZ/hRimukeUZcsKMU7G7XbT9XUVFRUYlcogQNUYLAqrzCgHqcWQMGY3e7qGiskwOh148fIj0ugYl9BvDUga1BbahtLicZv/8tYpSW8vragMcpqrqEWdQjCALdjGbi9IYOBzoqKioqkUDEdWNTCgatjr9cLCPv8GYAbvz8N3fYSEVFRUXl74VH8mBxOkiMNrFm/DRiRQNXmhvRCAJdjeb/v707D6+rvu88/v5os6zFkixvsixb8iIbLxiIQ6AhKSWBx1kassAEMoHJ0kDSSQJhypTOpEkm0zyTeZq0kzS0ZQ0koZCEkISn9cOSQgNJy2rwDsaLvMibvEveJNvf+eMcXV/J91qSOdK9v8v39Tx+fO4993fO5x4dna/O9jt84clf8L0/upKbnv41j65fzbcuOURr3biM3VBXlY3iooapHOrp5l82rO4z/qKGqRzu6aaqbBRVZaNG8is651xe8MM7zjnn3Ag7Cfx49ct0dh/jnhUvsKVzP19+6lfMv/+7HDx2lB2HO/nW8//K9y/7MNtu/EtKi4ro7D6W9QGgP7jsIxQhrp1zfp8zRXddcTUV/nBQ59xbWMGd2WmdMSPXEQbtwZ0bch1hUEJZpqHkhHCyes7khZI1lJyhOmfGLMYcO8zdK57nk3Pfxk9Wv8y33/UBWuvGcezEce68/CpuePJhLnzg+yy97mb+9DeP8IGWOdxx+VXc+OTDfR4AWlZcTFVZGU9tfoOLJk/L+sycsxHKehBKTggnq+dMXihZCy1nwe3sdOzZS1VlVa5jDMp5VWNpP3Y41zEGFMoyDSUnhJPVcyYvlKyh5AzVto5dtE5r5tKXfsuYslH8yYJ3UF02irX7dvMvG1Zz48KLUjstEF2u9rv2Nl7cuZVvv+v9tNaN5/DxHoolTpw0xpSVc+nUmRQhyktKUs/MebNCWQ9CyQnhZPWcyQsla6HlLLjL2JoaG3MdYdB+s3dbriMMSijLNJScEE5Wz5m8ULKGkjNUkyc1pC5L+4vfPcatz/wza/d10Fo3jhvOvQghOnuOAdDVfYxbF/0hEPXYdslDf89HH72fEyej+35u/rdf89FH7wegsqws0Q4IQlkPQskJ4WT1nMkLJWuh5Sy4nZ2169f1eZ3PvZxdO3F6riMMSv9lmq9CyQnhZPWcyQslayg5Q7VpUxvFReKOuDe2R9ev5rZnl7Clcz8VpWUc6unmT3/zSKqL6evSnpfTey/OCTvJ137/OLsOd3HXFVdTOQz35oSyHoSSE8LJ6jmTF0rWQstZcJexHT16tM/r8pJS6m//OpB/PZ6NKyvPdYRB6b9M81UoOSGcrJ4zeaFkDSVnqLq7j3H85EmqS0fx4/ddS0VpGZ3dRxlTVs6hnm5uePJhftfeBpDqYvr+xddwy6I/pKv7GBWlZWw+uI9/uPxjHOrppvJN3puTTSjrQSg5IZysnjN5oWQttJwFd2bHOeecy3cnzLhnxQts7txHRWkZe44c4lOP/ZTJd/xvKkvLMnYxXR3fg3Pn8uc4cryH8RVVgFFdNmpYdnScc64Q+NYxT6RfbnckbTifL8Nzzjl3doolrp+3iNueXcIb+3anzuQcP3mStfs6MnYxvengPg52H+X6eYuoLC31nRznnBuEgttK1tbW5jrCoL1++EBquPdyu/rbv87otOHyktw/HyGUZRpKTggnq+dMXihZQ8kZqrraOsaNruCB93+C2WPH9zmT8zcvP8P3/ujKPs/L+cFlH6GufDTlxSWMG10xYjs5oawHoeSEcLJ6zuSFkrXQchbcPTsT6sflOsKgvXxw94CfOXq8J7XDkz48kkJZpqHkhHCyes7khZI1lJyhmlA/jiIVUVU2iq64V7bee3QeeWMls+vGp7qePtTTnepSOsme1gabMwSh5IRwsnrO5IWStdByFtyZnc5DXbmOMGhTywfuGzz9jE+6kby8LZRlGkpOCCer50xeKFlDyRmq9OVbUVrKXVdc3edMzvXzFlFVVkaRRHXZqMS7lD6bnPkslJwQTlbPmbxQshZazoI7s1NSHM5XOnzy+JA+n6ue5UJZpqHkhHCyes7khZI1lJzDQdJi4HtAMXC3mX076XmkL98iFaUuaasoLeNwTzcVw9S72lCFsh6EkhPCyeo5kxdK1kLLmfstacK2tG/NdYRBu2JsGA9tCmWZhpITwsnqOZMXStZQciZNUjFwO/A+YC5wraS5Sc+n//LtvaStSIr/z4/yHMp6EEpOCCer50xeKFkLLWd+bE2dc865/HAhsM7MNphZN/AQcGWOMznnnDtLYZyncqfp31lBrjovcM65AtMIbEl7vRV4R/8PSboBuAGgcUojS5cvSx/JBQvOZV3bRqZNaWLF6lV92jY1TuHosWN0HTrErj272b9/f2pceXk5rTNmsqW9nfH1Y1m7fn2ftq0zZtCxZy9NjY2sXb+uz0P1amtrmVA/js5DXZQUl5x21HPB3Hls2rqFmc0tLF2xHMxS4yZNnEhZaRnFxcV0HeqiY3fUgc6BzoMsW72KhXPnsa5tI1MaJrP69df6TLd56lQOHzlCfd1Ytm7fRmdnZ2pcZWUlzU1T2dmxizHV1Wxoa+vTds6sVrbt3MHM5hZWvLaanu5T96PWjx1LbU0Nx451c9JOsm379j5tz5u/gA2bN9HSNJUDnQf7/AwaGyYDMLq8nL3797N3397UuLJRZcydNZuNWzYzafwEXl/3Rp/pzmhpYf+BAzRMnMSGTW0cPnw4NW7MmDFMnjiJfQf2Uz6qnE1b+j4Pad6cc9iyrZ2ZzS28umolJ0+cSI2bMH48FaNH093TQ/uO7ezctSs1TkVFnD9/AevaNjK1cQor16zuM92pU5ro7ummpnoMOzp2ceDAqd5cR48ezcyW6bRv387Y2lrWbdzQp23rjJns2rObaY1TeG3dGxw7diw1rq62jvH1Y+k6fJgiFbF1W3tq3JDVPOYAABIXSURBVIHOgxw/cYK2LZujdSZ9HQcaJk2ipLiE0tISDnZ2snvPntS40tJSFpwzl3VtG2mc1MCata/3adsyrZmuQ12Mrx/HlvatdHaduveiuqqKpsYpdOzZTVVlFRs3tfVpe07rbNp3bI/WmTWr+/zsx9XXM6a6mp6e4xw/cZztO3b0aXvBuQtZ17aR5qapLF+1ss+4KZMbOWknqaqooGPPXvbt35caN2rUKObMnMWm9q1MqB/H2vXr+rSd2TKdvfv309jQwLqNGzhy5EhqXE1NDZPGT+BA50G6e7pPW47zz5nL5vatzGxu4ZWVK7CTJ1PjJk6YQPmociQ4fOQIuzo6UuOKios5b9581rVtpGlyI6teW9NnutOapnL02FHqamrZtnMHBw8eTI2rqKhg+rRmtu/cQW1NDes3buzT9sSJE6zf1EZL01RWv/E63ce6U+PG1o1lbG0tR+JtT/v2bX3aLpw3n41bNjN96jReXbmiz7jJDQ0UqYhRo8rYf+AAe/ae+r0sLStlwZxonZk8cRKvvbG2T9vpzc0c7Oxk4vgJtG3ZzKFDh1I/++rqaqbEv/OZ+M5OoNLv3wFov/GrqWHf8XHOueFlZncCdwIsWrTILjh34WmfmdncAkR/YPW3pX0rVZWVVFVWwtRpp41vmTo1a9uqyqhzm7mtszNmq6qsBGB8fX32TAvOzdgWoK6mhqbJ0WXWS5cvY+HceQN+n7G1dQDMapmecZpTG6dkbds73QVzslwtWB39N2n8hKxta6rHZJw2wJjqapqbmk57f8a05qyZaqrHADBn5qyM06wYPRqA+rq6rJnOmzc/Y9uy0lIaJzXQOKkha9ts3yU9d3+93zHzOhOtE/Nmz8nYtnedmjDuVO9WS5cvo6S4eFCZasfUpH7G6c7Utq6mBoBZ02dknGbvOlh3pnXmnLksXb4sa7aGCROHlKlXVWVV6ncw3fT4dzVT2zHV0Yp6zqzWjNOsrKhgx86dZ/wdOH/+gqyZxtbWZfyDfjDfp/cz/U2bknmdWbp8WWo9mz/7nIxte7/vxPHjzypTTfWY1PwH27Z2TLTOzJ4xM5XzTPPoVXA7OwvijXIIbt+6ZuAPDdJwdl4QyjINJSeEk9VzJi+UrKHkHAbtQHoFnhK/l6hQlq/nTF4oWT1n8kLJWmg5C+6enU1btwz8oTyxuP70oyBJSO+W+kiW4f6fO5NQlmkoOSGcrJ4zeaFkDSXnMHgRmCWpRVIZcA3waNIzCWX5es7khZLVcyYvlKyFlrPgzuxkO1WXj37ZsWlYptv/LE+m4d7XgxHKMg0lJ4ST1XMmL5SsoeRMmpkdl/RF4HGirqfvNbNVAzQbslCWr+dMXihZPWfyQslaaDlH/MyOpMWSXpe0TtJtSU9/6YrlSU9y2NzSlNvThNnOAPU/4xPKMg0lJ4ST1XMmL5SsoeQcDma2xMxazWyGmX1rOOYRyvL1nMkLJavnTF4oWQst54ju7IzI8wvSepfJd0VSTuffewao/vavMzptON3R4z3hLNNQckI4WT1n8kLJGkrOUIWyfD1n8kLJ6jmTF0rWAss50md2/PkFASjPsuMDfc/6DObeoP73CZ3pDJJzzjnnnHNJGul7dgb1/AKXP8pLSnl801ouf/YRoG8X14O5NyjTfUK9r9OndeR4D6Pj7rLTh880rv9wps/3f53eLXf6cP822brvztbeOeecc87lH9kInqqSdBWw2Mz+JH59HfAOM/ti2mdSD2oDZgOvnzahMxsH7E4g7kgIJavnTF4oWT1n8kLJ2j/nNDM7/YEKDkkdwFB7nAl1PchXoeSEcLJ6zuSFkjXUnBnr1Eif2Rnw+QXpD2o7G5JeMrNFZ9t+JIWS1XMmL5SsnjN5oWQNJWc+OJudwFCWr+dMXihZPWfyQslaaDlH+p6dEXl+gXPOOeecc86N6JmdkXp+gXPOOeecc86N+ENFzWwJsGQYZ3HWl8DlQChZPWfyQsnqOZMXStZQcoYqlOXrOZMXSlbPmbxQshZUzhHtoMA555xzzjnnRspI37PjnHPOOeeccyOioHZ2JC2W9LqkdZJuy3WebCTdK2mXpJW5znImkpokPS1ptaRVkm7KdaZMJJVLekHSsjjn/8p1pjORVCzpFUn/nOssZyKpTdIKSa9KeinXebKRVCvpYUmvSVoj6eJcZ+pP0ux4Ofb+Oyjp5lznykbSV+LfpZWSHpRUnutMhcLrVLK8Tg0Pr1PJ8jqVvKHUqYK5jE1SMbAWuJzoYaUvAtea2eqcBstA0ruBLuBHZjY/13mykdQANJjZUknVwMvAh/NtmUoSUGlmXZJKgd8BN5nZczmOlpGkW4BFwBgz+2Cu82QjqQ1YZGZ53de+pPuBZ83s7riXxwoz25/rXNnE26p2omeMDfXZLMNOUiPR79BcMzsi6WfAEjO7L7fJwud1Knlep4aH16lkeZ1K1lDrVCGd2bkQWGdmG8ysG3gIuDLHmTIys2eAvbnOMRAz225mS+PhTmAN0JjbVKezSFf8sjT+l5d78ZKmAB8A7s51lkIgqQZ4N3APgJl153MBib0HWJ+PBSRNCTBaUglQAWzLcZ5C4XUqYV6nkud1Kllep4bNoOtUIe3sNAJb0l5vJQ83eKGS1AycDzyf2ySZxafcXwV2AU+aWV7mBP4f8N+Bk7kOMggGPCHpZUk35DpMFi1AB/DD+JKLuyVV5jrUAK4BHsx1iGzMrB34DrAZ2A4cMLMncpuqYHidGkZepxLjdSpZXqcSNtQ6VUg7O26YSKoCfgHcbGYHc50nEzM7YWbnAVOACyXl3WUXkj4I7DKzl3OdZZAuMbMLgPcB/zW+rCXflAAXAP9gZucDh4B8vg+iDPgQ8PNcZ8lGUh3R2YYWYDJQKemTuU3l3Jl5nUqG16lh4XUqYUOtU4W0s9MONKW9nhK/596E+NriXwAPmNkjuc4zkPjU8NPA4lxnyeCdwIfia4wfAi6T9JPcRsouPnKCme0Cfkl0CU6+2QpsTTtC+jBRUclX7wOWmtnOXAc5g/cCG82sw8x6gEeAP8hxpkLhdWoYeJ1KlNep5HmdSt6Q6lQh7ey8CMyS1BLvlV4DPJrjTEGLb6i8B1hjZn+T6zzZSBovqTYeHk108+9ruU11OjP7CzObYmbNROvnU2aWl0fMJVXGN/sSn26/Asi7XpnMbAewRdLs+K33AHl1Y3I/15LHlwbENgMXSaqItwHvIboPwr15XqcS5nUqWV6nkud1algMqU6VjFisYWZmxyV9EXgcKAbuNbNVOY6VkaQHgUuBcZK2Al83s3tymyqjdwLXASvi64wB/oeZLclhpkwagPvj3kOKgJ+ZWV53lxmAicAvo20IJcA/mdljuY2U1ZeAB+I/HjcAn85xnoziYnw5cGOus5yJmT0v6WFgKXAceIVwnqad17xODQuvU29dXqcSVqh1qmC6nnbOOeecc865dIV0GZtzzjnnnHPOpfjOjnPOOeecc64g+c6Oc84555xzriD5zo5zzjnnnHOuIPnOjnPOFTBJ90raJWlQXbJK+k+SVktaJemfhjufc865t7bhrlPeG5tzzhWw+IniXcCPzOyMT2yXNAv4GXCZme2TNCF+WJ9zzjk3LIa7TvmZHTcgSV39Xn9K0g9ykOM+SRslvSppqaSLs3zum5Lem8D8Rkv6raRiSc2STNJfpY0fJ6mnd1lI+ryk69OyXhUP/5ukRfHwkt4HyyUt08+l37w/I2mFpOWSVkq6MsM0viGpPV7Gb0h6RNLcQcw7fT5tksYl8H2+KOkzb3Y6b3Vm9gywN/09STMkPSbpZUnPSpoTj/occLuZ7Yvb+o6OC4LXKa9Tg5i316k8Ndx1ynd2XGhuNbPzgNuAO/qPlFRsZl8zs98kMK/PAI+Y2Yn49UbgA2njrwZSDwQ0s380sx+daYJm9n4z259AtiGRNAX4n8AlZnYucBGwPMvH/9bMzjOzWcBPgackjR/GbMVZRt1L9CA2l7w7gS+Z2duAPwP+Pn6/FWiV9HtJz0lanLOEzoXL69RZ8Drl+kmsTvnOjntT4iNJT8VHYf5V0tT4/dQRo/h1V/x/g6Rn4iMyKyW9K37/Ckn/ER8J+7mkqgFm/QwwM27bJun/SloKXN3vaNXbJf27pGWSXpBUHR8B+2tJL8a5sz0p+D8Dv057fRhY03tkCPg40anU3u/4DUl/NsDySh1NknRLvAxWSro5bXmukXSXomtRn5A0Oh73ZUXXqC6X9NAAy6e/CUAn0WlizKzLzDYO1MjMfgo8AXwizvAeSa/ER97ulTRqgO/7q/iozCpJN6S93yXpu5KWARdL+nbad/tOPO/DQJukC4f4Xd0ZxL9bfwD8XNET5+8gero7RE8hnwVcClwL3DVcR3idGylep7xODfB9vU7lmaTrlO/suMEYHW/0X41Xum+mjfs74P74KMwDwPcHmNYngMfjo14LgVfjjepXgfea2QXAS8AtA0znj4EVaa/3mNkFZpbauEoqIzric5OZLQTeCxwBPgscMLO3A28HPiepJX3icdvpZtbWb74PAddIagJOANsGyJmRpLcBnwbeQXT06nOSzo9HzyI6RTsP2A98LH7/NuD8eFl/foizXAbsBDZK+qGkPx5C26XAHEnlwH3Ax81sAdEG5wsDtP1MfFRmEfBlSfXx+5XA8/HPZQ3wEWBe/N3+Kq39S8C7hpDVDawI2B8fFe39d048bivwqJn1xH9krCVaH53Ld16nTvE65XUqdInWKd/ZcYNxJH2FA76WNu5ioLcnjB8DlwwwrReBT0v6BrDAzDqJNqJzgd/HReq/ANOytP/r+DM3EBWDXj/N8NnZwHYzexHAzA6a2XHgCuD6eDrPA/Wc/osyjmgD3t9jwOXANVnmOViXAL80s0Nm1gU8wqmN5UYzezUefhlojoeXAw9I+iRwPMM0s/U2YvElDouBq4g2DH8b/wwGQ/H/s+Nsa+PX9wPvHqDtl+OjYs8BTZxazieAX8TDB4CjwD2SPkp0ZLLXLmDyIHO6QTCzg0R/TFwNoMjCePSviI6WEf9x1wpsyEVO54bI69QpXqe8TgUt6TrlOztuuBwnXr8kFQFlkLoJ7d1AO3CfohslBTyZVqjmmtlns0z31vgzl5tZeheFh4aQTUTXgfbOr8XMnuj3mSNAef+GZtZNtGH/b8DDQ5jnUBxLGz5BdGQKouuwbwcuAF6UVNKv3R6grt97Y4HdEFUSM3vBzP4PURH8GINzPtFRrSGRdCnRUcqL4yNjr3BqmR7tvcY8LuwXEi3PDxIV6l7lRD8Ld5YkPQj8BzBb0lZJnyW69OWzcYFfBfTeBPw4sEfSauBpot+3PbnI7dwI8Dp19rxOneJ16k0a7jrlOzvuzfp3og0SRCvms/FwG/C2ePhDQCmApGnATjO7C7ibaIP4HPBOSb3XNldKak0g2+tAg6S3x9Otjje8jwNfkNSbqVVSZXpDi3r5KI5Piff3XeDPzWxvhnGD9SzwYUkV8bw/wqlld5q4EDeZ2dPAnwM1QP/rxV8kWo6T4jaLgFHAFkmTJV2Q9tnzgE0DhZT0MaIjjA8SLc/m3p8TcB3w2zM0rwH2mdlhRb2oXJRlHlVAjZktAb5CdNlIr1ZgUP3uu8zM7FozazCzUjObYmb3mNlGM1tsZgvjP9q+GX/WzOyW+L0F6ZfbOBcwr1Nnx+vUqXl4nRpGw12n+u9xOzdUXwJ+KOlWoIPo+l6Au4Bfx3vkj3HqiNalwK2SeohuQrzezDokfQp4UKduJPwq0Wnss2Zm3ZI+DvydopsnjxAdwbmb6JT7UkmKc384wySeIDqN36fHHDNbRVrvNmeZbamk+4AX4rfuNrNXJDVnaVIM/ERSDdERv+9bv95yzGynpJuAJXHR6QKuNbOTccH8jqTJRKfiO8h+PfVX4ksQKok24JeZWQeApE8T3TBYQlS0/vEMX/Mx4POS1hAVoOeyfK6aaF0pj79b+nXw7wS+cYZ5OOfcQLxOnV02r1OneJ0KmD9U1Lks4iNMXzGz63Kd5a1I0Y2wt/jyd865zLxO5ZbXqTD4ZWzOZWFmS4Gnlb1/fTe8xgF/mesQzjmXr7xO5ZzXqQD4mR3nnHPOOedcQfIzO84555xzzrmC5Ds7zjnnnHPOuYLkOzvOOeecc865guQ7O84555xzzrmC5Ds7zjnnnHPOuYL0/wFmAEr7o1QWyAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1008x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGGRu-6WxiBP",
        "outputId": "f050d2b8-9d1d-444d-e5be-f5470f838d76"
      },
      "source": [
        "# Elimina Features non utili\n",
        "df = df.drop('id', axis=1)\n",
        "df = df.drop('zipcode', axis=1)\n",
        "df = df.drop('date', axis=1)\n",
        "\n",
        "# Features finali\n",
        "X = df.drop('price', axis=1)\n",
        "\n",
        "# Label\n",
        "y = df['price']\n",
        "\n",
        "# Eliminare dal dataset la prima riga e salvarla come nuovo record per la predizione finale\n",
        "house_to_predict = df.drop('price', axis=1).iloc[0]\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "# Definizione dello scaler per normalizzare i dati\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Normalizzare i dati sulla dinamica del dataset di Training (per non avere data leakage!)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print('Max: ', X_train.max())\n",
        "print('Min: ', X_train.min())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15129, 17)\n",
            "(6484, 17)\n",
            "(15129,)\n",
            "(6484,)\n",
            "Max:  1.0000000000000002\n",
            "Min:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5IElciXyMOA"
      },
      "source": [
        "# Definizione e implementazione del modello FFNN con Keras\n",
        "def createModel(nn1, nn2, nn3, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(nn1, activation='swish'))\n",
        "    model.add(Dense(nn2, activation='swish'))\n",
        "    model.add(Dense(nn3, activation='swish'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=tf.optimizers.Adam(lr=learning_rate), loss='mse')\n",
        "    return model\n",
        "\n",
        "def gridTrain(train_x, train_y):\n",
        "    number_of_neurons_input_layer = [16, 32, 64]\n",
        "    number_of_neurons_first_hidden_layer = [64, 128, 256]\n",
        "    number_of_neurons_second_hidden_layer = [256, 512]\n",
        "    learning_rate = [0.05, 0.005, 0.0005, 0.00005]\n",
        "    batch_size = [32, 64, 128, 256, 512]\n",
        "    epochs = [100, 200, 300, 500]\n",
        "\n",
        "    parameters = dict(nn1=number_of_neurons_input_layer,\n",
        "                      nn2=number_of_neurons_first_hidden_layer,\n",
        "                      nn3=number_of_neurons_second_hidden_layer,\n",
        "                      learning_rate=learning_rate,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs)\n",
        "\n",
        "    with(tf.device('/GPU:1')):\n",
        "        modelCV = KerasRegressor(build_fn=createModel)\n",
        "        grid = RandomizedSearchCV(modelCV, parameters, n_jobs=1, verbose=50, scoring='r2')\n",
        "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                    patience=10,\n",
        "                                                    restore_best_weights=True)\n",
        "\n",
        "        # filepathCheckPoint=\"/content/gdrive/My Drive/MaBiDa_2021/models_checkpoint/weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "        save = tf.keras.callbacks.ModelCheckpoint(filepathCheckPoint, monitor='val_loss',\n",
        "                                                  mode='auto',\n",
        "                                                  save_best_only=True,\n",
        "                                                  save_weights_only=False, verbose=1)\n",
        "\n",
        "        grid.fit(train_x, train_y, validation_split=0.15, callbacks=[callback, save])\n",
        "\n",
        "        print(\"\\n\\n Best Model After Randomized-Search Cross Validation: \")\n",
        "        print(grid.best_score_, grid.best_params_)\n",
        "        print(\"\\n\\n\")\n",
        "        means = grid.cv_results_['mean_test_score']\n",
        "        parameters = grid.cv_results_['params']\n",
        "        for mean, parameter in zip(means, parameters):\n",
        "            print(mean, parameter)\n",
        "\n",
        "    return grid.best_estimator_.model, grid.best_params_, means, parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IUQnT6YXBDUK",
        "outputId": "e79c2404-cb26-4125-b16b-b234af6e182f"
      },
      "source": [
        "model, params, means, parameters  = gridTrain(X_train, y_train.values)\n",
        "model.save(pathSaveModel, save_format='h5')\n",
        "for mean, parammeter in zip(means, parameters):\n",
        "    print(\"\\nMean Test Score: \", mean, \" for Parameters: \", parameters)\n",
        "\n",
        "losses = pd.DataFrame(model.history.history)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(losses['loss'], label=\"Training Loss\")\n",
        "plt.plot(losses['val_loss'], label=\"Validation Loss\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.grid(color='#b8bfbd', linestyle='-.', linewidth=0.7)\n",
        "plt.show(block=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 12225602560.00000\n",
            "Epoch 320/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 83072576137.3659 - val_loss: 66862669824.0000\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 12225602560.00000\n",
            "Epoch 321/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 82861180078.8293 - val_loss: 66634407936.0000\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 12225602560.00000\n",
            "Epoch 322/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 86850570090.1463 - val_loss: 66398789632.0000\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 12225602560.00000\n",
            "Epoch 323/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 78761714663.0244 - val_loss: 66164121600.0000\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 12225602560.00000\n",
            "Epoch 324/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 78030912636.8781 - val_loss: 65923530752.0000\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 12225602560.00000\n",
            "Epoch 325/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 90085280093.6585 - val_loss: 65681080320.0000\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 12225602560.00000\n",
            "Epoch 326/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 79171170004.2927 - val_loss: 65436565504.0000\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 12225602560.00000\n",
            "Epoch 327/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 75783577799.8049 - val_loss: 65209790464.0000\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 12225602560.00000\n",
            "Epoch 328/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 80499447158.6341 - val_loss: 64967462912.0000\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 12225602560.00000\n",
            "Epoch 329/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 81310924300.4878 - val_loss: 64722378752.0000\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 12225602560.00000\n",
            "Epoch 330/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 84624164114.7317 - val_loss: 64463405056.0000\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 12225602560.00000\n",
            "Epoch 331/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 73221956433.1707 - val_loss: 64233689088.0000\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 12225602560.00000\n",
            "Epoch 332/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 83205906756.6829 - val_loss: 63991603200.0000\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 12225602560.00000\n",
            "Epoch 333/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 80371234116.6829 - val_loss: 63751606272.0000\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 12225602560.00000\n",
            "Epoch 334/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 82419214985.3659 - val_loss: 63508496384.0000\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 12225602560.00000\n",
            "Epoch 335/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 80156216844.4878 - val_loss: 63256670208.0000\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 12225602560.00000\n",
            "Epoch 336/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 69556119801.7561 - val_loss: 63017922560.0000\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 12225602560.00000\n",
            "Epoch 337/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 76056090374.2439 - val_loss: 62771441664.0000\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 12225602560.00000\n",
            "Epoch 338/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 83192911522.3415 - val_loss: 62517821440.0000\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 12225602560.00000\n",
            "Epoch 339/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 82745269572.6829 - val_loss: 62271700992.0000\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 12225602560.00000\n",
            "Epoch 340/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 84542230128.3902 - val_loss: 62028652544.0000\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 12225602560.00000\n",
            "Epoch 341/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 78173472668.0976 - val_loss: 61788942336.0000\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 12225602560.00000\n",
            "Epoch 342/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 74020706304.0000 - val_loss: 61534007296.0000\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 12225602560.00000\n",
            "Epoch 343/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 78771402252.4878 - val_loss: 61286588416.0000\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 12225602560.00000\n",
            "Epoch 344/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 74041483513.7561 - val_loss: 61042135040.0000\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 12225602560.00000\n",
            "Epoch 345/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 87574788046.0488 - val_loss: 60791488512.0000\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 12225602560.00000\n",
            "Epoch 346/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 72327116650.1463 - val_loss: 60555935744.0000\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 12225602560.00000\n",
            "Epoch 347/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 71515999956.2927 - val_loss: 60311621632.0000\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 12225602560.00000\n",
            "Epoch 348/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 71010779185.9512 - val_loss: 60070387712.0000\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 12225602560.00000\n",
            "Epoch 349/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 69608375520.7805 - val_loss: 59828477952.0000\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 12225602560.00000\n",
            "Epoch 350/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 70673033365.8537 - val_loss: 59585540096.0000\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 12225602560.00000\n",
            "Epoch 351/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 73886076253.6585 - val_loss: 59347959808.0000\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 12225602560.00000\n",
            "Epoch 352/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 73396540640.7805 - val_loss: 59102199808.0000\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 12225602560.00000\n",
            "Epoch 353/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 73936469666.3415 - val_loss: 58866257920.0000\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 12225602560.00000\n",
            "Epoch 354/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 74679703901.6585 - val_loss: 58626031616.0000\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 12225602560.00000\n",
            "Epoch 355/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 75239815717.4634 - val_loss: 58384887808.0000\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 12225602560.00000\n",
            "Epoch 356/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 80496643047.0244 - val_loss: 58148110336.0000\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 12225602560.00000\n",
            "Epoch 357/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 71858845995.7073 - val_loss: 57915559936.0000\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 12225602560.00000\n",
            "Epoch 358/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 71309775647.2195 - val_loss: 57677815808.0000\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 12225602560.00000\n",
            "Epoch 359/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 77199395265.5610 - val_loss: 57434025984.0000\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 12225602560.00000\n",
            "Epoch 360/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 74808789591.4146 - val_loss: 57201270784.0000\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 12225602560.00000\n",
            "Epoch 361/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 74726091651.1219 - val_loss: 56964395008.0000\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 12225602560.00000\n",
            "Epoch 362/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68736612501.8537 - val_loss: 56733896704.0000\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 12225602560.00000\n",
            "Epoch 363/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 73695666076.0976 - val_loss: 56505253888.0000\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 12225602560.00000\n",
            "Epoch 364/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 70455053037.2683 - val_loss: 56275873792.0000\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 12225602560.00000\n",
            "Epoch 365/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 66034633503.2195 - val_loss: 56056868864.0000\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 12225602560.00000\n",
            "Epoch 366/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68535311484.8781 - val_loss: 55830953984.0000\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 12225602560.00000\n",
            "Epoch 367/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 69946333084.0976 - val_loss: 55603163136.0000\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 12225602560.00000\n",
            "Epoch 368/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 63910119324.0976 - val_loss: 55382159360.0000\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 12225602560.00000\n",
            "Epoch 369/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 69021707438.8293 - val_loss: 55163449344.0000\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 12225602560.00000\n",
            "Epoch 370/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 73953373683.5122 - val_loss: 54942896128.0000\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 12225602560.00000\n",
            "Epoch 371/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 64508033398.6341 - val_loss: 54732652544.0000\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 12225602560.00000\n",
            "Epoch 372/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 65414093449.3659 - val_loss: 54518546432.0000\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 12225602560.00000\n",
            "Epoch 373/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 70441972111.6098 - val_loss: 54312394752.0000\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 12225602560.00000\n",
            "Epoch 374/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61146694780.8781 - val_loss: 54103433216.0000\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 12225602560.00000\n",
            "Epoch 375/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 65345236042.9268 - val_loss: 53897977856.0000\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 12225602560.00000\n",
            "Epoch 376/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 66499003666.7317 - val_loss: 53693853696.0000\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 12225602560.00000\n",
            "Epoch 377/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60316794780.0976 - val_loss: 53495156736.0000\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 12225602560.00000\n",
            "Epoch 378/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 63885672497.9512 - val_loss: 53302321152.0000\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 12225602560.00000\n",
            "Epoch 379/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 65832065723.3171 - val_loss: 53105606656.0000\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 12225602560.00000\n",
            "Epoch 380/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 65432465008.3902 - val_loss: 52908990464.0000\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 12225602560.00000\n",
            "Epoch 381/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 63459764573.6585 - val_loss: 52721569792.0000\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 12225602560.00000\n",
            "Epoch 382/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 61162982724.6829 - val_loss: 52535967744.0000\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 12225602560.00000\n",
            "Epoch 383/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 67849195320.1951 - val_loss: 52346585088.0000\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 12225602560.00000\n",
            "Epoch 384/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 75235821568.0000 - val_loss: 52159631360.0000\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 12225602560.00000\n",
            "Epoch 385/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 64122791686.2439 - val_loss: 51990691840.0000\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 12225602560.00000\n",
            "Epoch 386/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61597897503.2195 - val_loss: 51817955328.0000\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 12225602560.00000\n",
            "Epoch 387/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 62818087861.0732 - val_loss: 51640270848.0000\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 12225602560.00000\n",
            "Epoch 388/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61797652080.3902 - val_loss: 51468230656.0000\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 12225602560.00000\n",
            "Epoch 389/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 67793517443.1219 - val_loss: 51295866880.0000\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 12225602560.00000\n",
            "Epoch 390/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 71159162730.1463 - val_loss: 51124690944.0000\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 12225602560.00000\n",
            "Epoch 391/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 69706290350.8293 - val_loss: 50962268160.0000\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 12225602560.00000\n",
            "Epoch 392/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60789040802.3415 - val_loss: 50796392448.0000\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 12225602560.00000\n",
            "Epoch 393/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61731093928.5854 - val_loss: 50639998976.0000\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 12225602560.00000\n",
            "Epoch 394/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 64721248905.3659 - val_loss: 50479980544.0000\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 12225602560.00000\n",
            "Epoch 395/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 52817884809.3659 - val_loss: 50333671424.0000\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 12225602560.00000\n",
            "Epoch 396/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 59807561178.5366 - val_loss: 50178297856.0000\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 12225602560.00000\n",
            "Epoch 397/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 65844278946.3415 - val_loss: 50023047168.0000\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 12225602560.00000\n",
            "Epoch 398/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 60648153187.9024 - val_loss: 49879093248.0000\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 12225602560.00000\n",
            "Epoch 399/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 68928013836.4878 - val_loss: 49732055040.0000\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 12225602560.00000\n",
            "Epoch 400/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 67440667073.5610 - val_loss: 49587417088.0000\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 12225602560.00000\n",
            "Epoch 401/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61679568246.6341 - val_loss: 49450074112.0000\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 12225602560.00000\n",
            "Epoch 402/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 67396567989.0732 - val_loss: 49312899072.0000\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 12225602560.00000\n",
            "Epoch 403/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 57505974272.0000 - val_loss: 49178640384.0000\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 12225602560.00000\n",
            "Epoch 404/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61469898752.0000 - val_loss: 49039462400.0000\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 12225602560.00000\n",
            "Epoch 405/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 59412221952.0000 - val_loss: 48911814656.0000\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 12225602560.00000\n",
            "Epoch 406/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 56000331076.6829 - val_loss: 48787075072.0000\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 12225602560.00000\n",
            "Epoch 407/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56458536710.2439 - val_loss: 48658685952.0000\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 12225602560.00000\n",
            "Epoch 408/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 63479017322.1463 - val_loss: 48530911232.0000\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 12225602560.00000\n",
            "Epoch 409/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 57188053541.4634 - val_loss: 48414285824.0000\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 12225602560.00000\n",
            "Epoch 410/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61668194903.4146 - val_loss: 48297099264.0000\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 12225602560.00000\n",
            "Epoch 411/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 64344217050.5366 - val_loss: 48170926080.0000\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 12225602560.00000\n",
            "Epoch 412/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 58175887609.7561 - val_loss: 48057208832.0000\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 12225602560.00000\n",
            "Epoch 413/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 58780745403.3171 - val_loss: 47950966784.0000\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 12225602560.00000\n",
            "Epoch 414/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56390283963.3171 - val_loss: 47843930112.0000\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 12225602560.00000\n",
            "Epoch 415/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60932484545.5610 - val_loss: 47730753536.0000\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 12225602560.00000\n",
            "Epoch 416/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56586558638.8293 - val_loss: 47620894720.0000\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 12225602560.00000\n",
            "Epoch 417/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 66427495299.1219 - val_loss: 47517700096.0000\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 12225602560.00000\n",
            "Epoch 418/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 56790463413.0732 - val_loss: 47416573952.0000\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 12225602560.00000\n",
            "Epoch 419/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 57279632808.5854 - val_loss: 47312277504.0000\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 12225602560.00000\n",
            "Epoch 420/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54937132756.2927 - val_loss: 47212617728.0000\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 12225602560.00000\n",
            "Epoch 421/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 59458322981.4634 - val_loss: 47108591616.0000\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 12225602560.00000\n",
            "Epoch 422/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56735492995.1219 - val_loss: 47016050688.0000\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 12225602560.00000\n",
            "Epoch 423/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 57616593845.0732 - val_loss: 46921236480.0000\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 12225602560.00000\n",
            "Epoch 424/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 60789667989.8537 - val_loss: 46826496000.0000\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 12225602560.00000\n",
            "Epoch 425/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56241378029.2683 - val_loss: 46731927552.0000\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 12225602560.00000\n",
            "Epoch 426/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 62210940928.0000 - val_loss: 46645362688.0000\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 12225602560.00000\n",
            "Epoch 427/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56304447987.5122 - val_loss: 46555000832.0000\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 12225602560.00000\n",
            "Epoch 428/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 58690549909.8537 - val_loss: 46470328320.0000\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 12225602560.00000\n",
            "Epoch 429/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 57327615675.3171 - val_loss: 46387302400.0000\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 12225602560.00000\n",
            "Epoch 430/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 57618709329.1707 - val_loss: 46299615232.0000\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 12225602560.00000\n",
            "Epoch 431/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56974117413.4634 - val_loss: 46216089600.0000\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 12225602560.00000\n",
            "Epoch 432/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 62564485769.3659 - val_loss: 46135934976.0000\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 12225602560.00000\n",
            "Epoch 433/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 59490436320.7805 - val_loss: 46057754624.0000\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 12225602560.00000\n",
            "Epoch 434/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 58451446284.4878 - val_loss: 45979996160.0000\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 12225602560.00000\n",
            "Epoch 435/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 55277316695.4146 - val_loss: 45906915328.0000\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 12225602560.00000\n",
            "Epoch 436/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56585292475.3171 - val_loss: 45829230592.0000\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 12225602560.00000\n",
            "Epoch 437/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 54906847831.4146 - val_loss: 45754748928.0000\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 12225602560.00000\n",
            "Epoch 438/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 54348498694.2439 - val_loss: 45677387776.0000\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 12225602560.00000\n",
            "Epoch 439/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60762392426.1463 - val_loss: 45613608960.0000\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 12225602560.00000\n",
            "Epoch 440/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 57045616090.5366 - val_loss: 45541527552.0000\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 12225602560.00000\n",
            "Epoch 441/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 52854150718.4390 - val_loss: 45470265344.0000\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 12225602560.00000\n",
            "Epoch 442/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 50886862148.6829 - val_loss: 45400809472.0000\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 12225602560.00000\n",
            "Epoch 443/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 55842844222.4390 - val_loss: 45335506944.0000\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 12225602560.00000\n",
            "Epoch 444/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56360006830.8293 - val_loss: 45262282752.0000\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 12225602560.00000\n",
            "Epoch 445/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 50491588408.1951 - val_loss: 45200093184.0000\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 12225602560.00000\n",
            "Epoch 446/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 58975765129.3659 - val_loss: 45136105472.0000\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 12225602560.00000\n",
            "Epoch 447/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60010963943.0244 - val_loss: 45069012992.0000\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 12225602560.00000\n",
            "Epoch 448/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 58831193912.1951 - val_loss: 45007028224.0000\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 12225602560.00000\n",
            "Epoch 449/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 52652993411.1219 - val_loss: 44947464192.0000\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 12225602560.00000\n",
            "Epoch 450/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56418013533.6585 - val_loss: 44885766144.0000\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 12225602560.00000\n",
            "Epoch 451/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56741238983.8049 - val_loss: 44828065792.0000\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 12225602560.00000\n",
            "Epoch 452/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 59897191698.7317 - val_loss: 44766879744.0000\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 12225602560.00000\n",
            "Epoch 453/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 52069825760.7805 - val_loss: 44708409344.0000\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 12225602560.00000\n",
            "Epoch 454/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 52198989024.7805 - val_loss: 44646739968.0000\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 12225602560.00000\n",
            "Epoch 455/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50788858880.0000 - val_loss: 44590632960.0000\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 12225602560.00000\n",
            "Epoch 456/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54056270173.6585 - val_loss: 44533096448.0000\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 12225602560.00000\n",
            "Epoch 457/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 52494776270.0488 - val_loss: 44476624896.0000\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 12225602560.00000\n",
            "Epoch 458/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54482197628.8781 - val_loss: 44419391488.0000\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 12225602560.00000\n",
            "Epoch 459/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 49466234230.6341 - val_loss: 44374024192.0000\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 12225602560.00000\n",
            "Epoch 460/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 51254307914.9268 - val_loss: 44312809472.0000\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 12225602560.00000\n",
            "Epoch 461/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 51681462571.7073 - val_loss: 44258611200.0000\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 12225602560.00000\n",
            "Epoch 462/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 50148943422.4390 - val_loss: 44208001024.0000\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 12225602560.00000\n",
            "Epoch 463/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50334307003.3171 - val_loss: 44154273792.0000\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 12225602560.00000\n",
            "Epoch 464/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50109629340.0976 - val_loss: 44099661824.0000\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 12225602560.00000\n",
            "Epoch 465/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 49569613873.9512 - val_loss: 44048805888.0000\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 12225602560.00000\n",
            "Epoch 466/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 51872081120.7805 - val_loss: 43995500544.0000\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 12225602560.00000\n",
            "Epoch 467/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50344900008.5854 - val_loss: 43946295296.0000\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 12225602560.00000\n",
            "Epoch 468/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 51590490336.7805 - val_loss: 43894161408.0000\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 12225602560.00000\n",
            "Epoch 469/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 48494149881.7561 - val_loss: 43846201344.0000\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 12225602560.00000\n",
            "Epoch 470/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54782721648.3902 - val_loss: 43792896000.0000\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 12225602560.00000\n",
            "Epoch 471/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 59501245689.7561 - val_loss: 43750359040.0000\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 12225602560.00000\n",
            "Epoch 472/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 50355725237.0732 - val_loss: 43697549312.0000\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 12225602560.00000\n",
            "Epoch 473/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 45866518478.0488 - val_loss: 43651203072.0000\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 12225602560.00000\n",
            "Epoch 474/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 51756803846.2439 - val_loss: 43603525632.0000\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 12225602560.00000\n",
            "Epoch 475/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 48925265670.2439 - val_loss: 43562287104.0000\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 12225602560.00000\n",
            "Epoch 476/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56557164893.6585 - val_loss: 43514249216.0000\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 12225602560.00000\n",
            "Epoch 477/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 60302230552.9756 - val_loss: 43461935104.0000\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 12225602560.00000\n",
            "Epoch 478/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 49653459443.5122 - val_loss: 43419926528.0000\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 12225602560.00000\n",
            "Epoch 479/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 52660221227.7073 - val_loss: 43372445696.0000\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 12225602560.00000\n",
            "Epoch 480/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 52606556010.1463 - val_loss: 43331641344.0000\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 12225602560.00000\n",
            "Epoch 481/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 48635741009.1707 - val_loss: 43285426176.0000\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 12225602560.00000\n",
            "Epoch 482/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 54342505896.5854 - val_loss: 43241459712.0000\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 12225602560.00000\n",
            "Epoch 483/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 48854964873.3659 - val_loss: 43200397312.0000\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 12225602560.00000\n",
            "Epoch 484/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50957100007.0244 - val_loss: 43158372352.0000\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 12225602560.00000\n",
            "Epoch 485/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 47057114636.4878 - val_loss: 43120263168.0000\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 12225602560.00000\n",
            "Epoch 486/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 49367172320.7805 - val_loss: 43077898240.0000\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 12225602560.00000\n",
            "Epoch 487/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 51215878493.6585 - val_loss: 43032608768.0000\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 12225602560.00000\n",
            "Epoch 488/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 48346286654.4390 - val_loss: 42991955968.0000\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 12225602560.00000\n",
            "Epoch 489/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 56933094574.8293 - val_loss: 42949140480.0000\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 12225602560.00000\n",
            "Epoch 490/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 54660519086.8293 - val_loss: 42911686656.0000\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 12225602560.00000\n",
            "Epoch 491/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54083949443.1219 - val_loss: 42869338112.0000\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 12225602560.00000\n",
            "Epoch 492/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 44642153696.7805 - val_loss: 42828849152.0000\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 12225602560.00000\n",
            "Epoch 493/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 51934619398.2439 - val_loss: 42795024384.0000\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 12225602560.00000\n",
            "Epoch 494/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 49433205685.0732 - val_loss: 42751696896.0000\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 12225602560.00000\n",
            "Epoch 495/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 50402791923.5122 - val_loss: 42716639232.0000\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 12225602560.00000\n",
            "Epoch 496/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 48853399601.9512 - val_loss: 42677157888.0000\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 12225602560.00000\n",
            "Epoch 497/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 49739202410.1463 - val_loss: 42641162240.0000\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 12225602560.00000\n",
            "Epoch 498/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 51156085085.6585 - val_loss: 42607407104.0000\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 12225602560.00000\n",
            "Epoch 499/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 52281603396.6829 - val_loss: 42568126464.0000\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 12225602560.00000\n",
            "Epoch 500/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 48299800426.1463 - val_loss: 42530598912.0000\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 12225602560.00000\n",
            "[CV 3/5; 10/10] END batch_size=128, epochs=500, learning_rate=5e-05, nn1=32, nn2=64, nn3=256;, score=0.598 total time= 2.5min\n",
            "[CV 4/5; 10/10] START batch_size=128, epochs=500, learning_rate=5e-05, nn1=32, nn2=64, nn3=256\n",
            "Epoch 1/500\n",
            "81/81 [==============================] - 1s 5ms/step - loss: 429295922900.2927 - val_loss: 390700302336.0000\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 12225602560.00000\n",
            "Epoch 2/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 435307858669.2683 - val_loss: 390699909120.0000\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 12225602560.00000\n",
            "Epoch 3/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 409878798535.8049 - val_loss: 390699155456.0000\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 12225602560.00000\n",
            "Epoch 4/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 430990243490.3415 - val_loss: 390697779200.0000\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 12225602560.00000\n",
            "Epoch 5/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 429518884364.4878 - val_loss: 390695452672.0000\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 12225602560.00000\n",
            "Epoch 6/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 417503797847.4146 - val_loss: 390691487744.0000\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 12225602560.00000\n",
            "Epoch 7/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 404659796817.1707 - val_loss: 390685327360.0000\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 12225602560.00000\n",
            "Epoch 8/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 428231088777.3658 - val_loss: 390675693568.0000\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 12225602560.00000\n",
            "Epoch 9/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 416046618474.1464 - val_loss: 390661767168.0000\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 12225602560.00000\n",
            "Epoch 10/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 424948910429.6585 - val_loss: 390642761728.0000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 12225602560.00000\n",
            "Epoch 11/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 419250067306.1464 - val_loss: 390618316800.0000\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 12225602560.00000\n",
            "Epoch 12/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 418741074369.5610 - val_loss: 390587940864.0000\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 12225602560.00000\n",
            "Epoch 13/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 412974174507.7073 - val_loss: 390550519808.0000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 12225602560.00000\n",
            "Epoch 14/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 432255608981.8536 - val_loss: 390505299968.0000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 12225602560.00000\n",
            "Epoch 15/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 436749083623.0244 - val_loss: 390451167232.0000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 12225602560.00000\n",
            "Epoch 16/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 423776832237.2683 - val_loss: 390387236864.0000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 12225602560.00000\n",
            "Epoch 17/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 423349759300.6829 - val_loss: 390312591360.0000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 12225602560.00000\n",
            "Epoch 18/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 417301702805.8536 - val_loss: 390226870272.0000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 12225602560.00000\n",
            "Epoch 19/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 412168585216.0000 - val_loss: 390129778688.0000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 12225602560.00000\n",
            "Epoch 20/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 429470195911.8049 - val_loss: 390019776512.0000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 12225602560.00000\n",
            "Epoch 21/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 424220029827.1219 - val_loss: 389897355264.0000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 12225602560.00000\n",
            "Epoch 22/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 435109199872.0000 - val_loss: 389761892352.0000\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 12225602560.00000\n",
            "Epoch 23/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 436517577703.0244 - val_loss: 389611585536.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 12225602560.00000\n",
            "Epoch 24/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 422036000668.0975 - val_loss: 389447286784.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 12225602560.00000\n",
            "Epoch 25/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 417338666308.6829 - val_loss: 389267488768.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 12225602560.00000\n",
            "Epoch 26/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 425878278044.0975 - val_loss: 389070127104.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 12225602560.00000\n",
            "Epoch 27/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 425609528095.2195 - val_loss: 388856446976.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 12225602560.00000\n",
            "Epoch 28/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 416782799147.7073 - val_loss: 388625367040.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 12225602560.00000\n",
            "Epoch 29/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 429421132625.1707 - val_loss: 388374560768.0000\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 12225602560.00000\n",
            "Epoch 30/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 415526308988.8781 - val_loss: 388103667712.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 12225602560.00000\n",
            "Epoch 31/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 417864326568.5854 - val_loss: 387813670912.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 12225602560.00000\n",
            "Epoch 32/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 422872252416.0000 - val_loss: 387501391872.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 12225602560.00000\n",
            "Epoch 33/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 422963351052.4878 - val_loss: 387166699520.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 12225602560.00000\n",
            "Epoch 34/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 413541504074.9268 - val_loss: 386810052608.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 12225602560.00000\n",
            "Epoch 35/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 416726717115.3171 - val_loss: 386430730240.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 12225602560.00000\n",
            "Epoch 36/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 418434345109.8536 - val_loss: 386025914368.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 12225602560.00000\n",
            "Epoch 37/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 411686912799.2195 - val_loss: 385595736064.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 12225602560.00000\n",
            "Epoch 38/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 417777375881.3658 - val_loss: 385143734272.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 12225602560.00000\n",
            "Epoch 39/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 413737559015.0244 - val_loss: 384661127168.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 12225602560.00000\n",
            "Epoch 40/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 404972622123.7073 - val_loss: 384155156480.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 12225602560.00000\n",
            "Epoch 41/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 425420159425.5610 - val_loss: 383620415488.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 12225602560.00000\n",
            "Epoch 42/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 410479853967.6097 - val_loss: 383058903040.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 12225602560.00000\n",
            "Epoch 43/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 413308943584.7805 - val_loss: 382467571712.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 12225602560.00000\n",
            "Epoch 44/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 396107849927.8049 - val_loss: 381847175168.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 12225602560.00000\n",
            "Epoch 45/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 414391351745.5610 - val_loss: 381197221888.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 12225602560.00000\n",
            "Epoch 46/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 403213093213.6585 - val_loss: 380516171776.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 12225602560.00000\n",
            "Epoch 47/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 443439633083.3171 - val_loss: 379806875648.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 12225602560.00000\n",
            "Epoch 48/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 400497367289.7561 - val_loss: 379064352768.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 12225602560.00000\n",
            "Epoch 49/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 403319953657.7561 - val_loss: 378287652864.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 12225602560.00000\n",
            "Epoch 50/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 415926956531.5122 - val_loss: 377480675328.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 12225602560.00000\n",
            "Epoch 51/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 393303297148.8781 - val_loss: 376645550080.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 12225602560.00000\n",
            "Epoch 52/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 410984822034.7317 - val_loss: 375769628672.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 12225602560.00000\n",
            "Epoch 53/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 407696940056.9756 - val_loss: 374861955072.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 12225602560.00000\n",
            "Epoch 54/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 395329361770.1464 - val_loss: 373924626432.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 12225602560.00000\n",
            "Epoch 55/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 387325269466.5366 - val_loss: 372947779584.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 12225602560.00000\n",
            "Epoch 56/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 399925735124.2927 - val_loss: 371935281152.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 12225602560.00000\n",
            "Epoch 57/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 401867112847.6097 - val_loss: 370888933376.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 12225602560.00000\n",
            "Epoch 58/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 387032450222.8293 - val_loss: 369810309120.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 12225602560.00000\n",
            "Epoch 59/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 394040843688.5854 - val_loss: 368690429952.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 12225602560.00000\n",
            "Epoch 60/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 390409285332.2927 - val_loss: 367536111616.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 12225602560.00000\n",
            "Epoch 61/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 390099201748.2927 - val_loss: 366343782400.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 12225602560.00000\n",
            "Epoch 62/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 400301680789.8536 - val_loss: 365111902208.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 12225602560.00000\n",
            "Epoch 63/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 407492688721.1707 - val_loss: 363845681152.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 12225602560.00000\n",
            "Epoch 64/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 392300507235.9025 - val_loss: 362540498944.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 12225602560.00000\n",
            "Epoch 65/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 392980240234.1464 - val_loss: 361196224512.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 12225602560.00000\n",
            "Epoch 66/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 375331867772.8781 - val_loss: 359819509760.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 12225602560.00000\n",
            "Epoch 67/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 375287963847.8049 - val_loss: 358400557056.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 12225602560.00000\n",
            "Epoch 68/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 382518235435.7073 - val_loss: 356943659008.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 12225602560.00000\n",
            "Epoch 69/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 373788634436.6829 - val_loss: 355440164864.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 12225602560.00000\n",
            "Epoch 70/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 378113331899.3171 - val_loss: 353900101632.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 12225602560.00000\n",
            "Epoch 71/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 375228382033.1707 - val_loss: 352317374464.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 12225602560.00000\n",
            "Epoch 72/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 377413478150.2439 - val_loss: 350703124480.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 12225602560.00000\n",
            "Epoch 73/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 369586378202.5366 - val_loss: 349044604928.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 12225602560.00000\n",
            "Epoch 74/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 383253973741.2683 - val_loss: 347345879040.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 12225602560.00000\n",
            "Epoch 75/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 386094758737.1707 - val_loss: 345611206656.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 12225602560.00000\n",
            "Epoch 76/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 371177855625.3658 - val_loss: 343832887296.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 12225602560.00000\n",
            "Epoch 77/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 378058590557.6585 - val_loss: 342007349248.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 12225602560.00000\n",
            "Epoch 78/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 376269854969.7561 - val_loss: 340158611456.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 12225602560.00000\n",
            "Epoch 79/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 406368576886.6342 - val_loss: 338252464128.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 12225602560.00000\n",
            "Epoch 80/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 351432927381.8536 - val_loss: 336321576960.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 12225602560.00000\n",
            "Epoch 81/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 348513454030.0488 - val_loss: 334340292608.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 12225602560.00000\n",
            "Epoch 82/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 354942402560.0000 - val_loss: 332313591808.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 12225602560.00000\n",
            "Epoch 83/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 365052360753.9512 - val_loss: 330262970368.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 12225602560.00000\n",
            "Epoch 84/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 354046923850.9268 - val_loss: 328157528064.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 12225602560.00000\n",
            "Epoch 85/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 341315873317.4634 - val_loss: 326027182080.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 12225602560.00000\n",
            "Epoch 86/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 349339920933.4634 - val_loss: 323838345216.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 12225602560.00000\n",
            "Epoch 87/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 368783067086.0488 - val_loss: 321622802432.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 12225602560.00000\n",
            "Epoch 88/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 348420043251.5122 - val_loss: 319370100736.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 12225602560.00000\n",
            "Epoch 89/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 341048871261.6585 - val_loss: 317082107904.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 12225602560.00000\n",
            "Epoch 90/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 354326064053.0732 - val_loss: 314745487360.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 12225602560.00000\n",
            "Epoch 91/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 339329534701.2683 - val_loss: 312365678592.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 12225602560.00000\n",
            "Epoch 92/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 347158980757.8536 - val_loss: 309957984256.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 12225602560.00000\n",
            "Epoch 93/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 333905402904.9756 - val_loss: 307511853056.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 12225602560.00000\n",
            "Epoch 94/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 338578436595.5122 - val_loss: 305030365184.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 12225602560.00000\n",
            "Epoch 95/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 323857672741.4634 - val_loss: 302512996352.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 12225602560.00000\n",
            "Epoch 96/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 328733036444.0975 - val_loss: 299956961280.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 12225602560.00000\n",
            "Epoch 97/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 332994995974.2439 - val_loss: 297362391040.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 12225602560.00000\n",
            "Epoch 98/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 336319329554.7317 - val_loss: 294736101376.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 12225602560.00000\n",
            "Epoch 99/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 313106795245.2683 - val_loss: 292070096896.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 12225602560.00000\n",
            "Epoch 100/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 306987888340.2927 - val_loss: 289376534528.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 12225602560.00000\n",
            "Epoch 101/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 307404103080.5854 - val_loss: 286641192960.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 12225602560.00000\n",
            "Epoch 102/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 307486691427.9025 - val_loss: 283887796224.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 12225602560.00000\n",
            "Epoch 103/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 309425696068.6829 - val_loss: 281094520832.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 12225602560.00000\n",
            "Epoch 104/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 324773069199.6097 - val_loss: 278256091136.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 12225602560.00000\n",
            "Epoch 105/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 290092379860.2927 - val_loss: 275418841088.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 12225602560.00000\n",
            "Epoch 106/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 305270549828.6829 - val_loss: 272533110784.0000\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 12225602560.00000\n",
            "Epoch 107/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 311718528574.4390 - val_loss: 269621166080.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 12225602560.00000\n",
            "Epoch 108/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 294638749246.4390 - val_loss: 266664558592.0000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 12225602560.00000\n",
            "Epoch 109/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 313468612308.2927 - val_loss: 263693320192.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 12225602560.00000\n",
            "Epoch 110/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 288204005575.8049 - val_loss: 260714217472.0000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 12225602560.00000\n",
            "Epoch 111/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 282513094905.7561 - val_loss: 257698283520.0000\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 12225602560.00000\n",
            "Epoch 112/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 294325125120.0000 - val_loss: 254652710912.0000\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 12225602560.00000\n",
            "Epoch 113/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 277211323217.1707 - val_loss: 251595440128.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 12225602560.00000\n",
            "Epoch 114/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 283428318932.2927 - val_loss: 248503517184.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 12225602560.00000\n",
            "Epoch 115/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 270200308311.4146 - val_loss: 245413150720.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 12225602560.00000\n",
            "Epoch 116/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 277771050208.7805 - val_loss: 242282315776.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 12225602560.00000\n",
            "Epoch 117/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 261929349020.0976 - val_loss: 239145926656.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 12225602560.00000\n",
            "Epoch 118/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 271481973884.8781 - val_loss: 235984584704.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 12225602560.00000\n",
            "Epoch 119/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 255647337397.0732 - val_loss: 232832483328.0000\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 12225602560.00000\n",
            "Epoch 120/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 257697180996.6829 - val_loss: 229631033344.0000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 12225602560.00000\n",
            "Epoch 121/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 239083006101.8537 - val_loss: 226451931136.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 12225602560.00000\n",
            "Epoch 122/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 264535667187.5122 - val_loss: 223240863744.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 12225602560.00000\n",
            "Epoch 123/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 236249818686.4390 - val_loss: 220037480448.0000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 12225602560.00000\n",
            "Epoch 124/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 233880328241.9512 - val_loss: 216816631808.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 12225602560.00000\n",
            "Epoch 125/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 247628718879.2195 - val_loss: 213590507520.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 12225602560.00000\n",
            "Epoch 126/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 236662885101.2683 - val_loss: 210348785664.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 12225602560.00000\n",
            "Epoch 127/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 240802730283.7073 - val_loss: 207119466496.0000\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 12225602560.00000\n",
            "Epoch 128/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 228217243797.8537 - val_loss: 203888738304.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 12225602560.00000\n",
            "Epoch 129/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 229503081097.3658 - val_loss: 200659894272.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 12225602560.00000\n",
            "Epoch 130/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 228489953879.4146 - val_loss: 197433573376.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 12225602560.00000\n",
            "Epoch 131/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 215152230799.6097 - val_loss: 194224209920.0000\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 12225602560.00000\n",
            "Epoch 132/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 198545313292.4878 - val_loss: 191012339712.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 12225602560.00000\n",
            "Epoch 133/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 222817784906.9268 - val_loss: 187813445632.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 12225602560.00000\n",
            "Epoch 134/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 212003836603.3171 - val_loss: 184623759360.0000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 12225602560.00000\n",
            "Epoch 135/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 209068144689.9512 - val_loss: 181434351616.0000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 12225602560.00000\n",
            "Epoch 136/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 198767507106.3415 - val_loss: 178269782016.0000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 12225602560.00000\n",
            "Epoch 137/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 195704760270.0488 - val_loss: 175119138816.0000\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 12225602560.00000\n",
            "Epoch 138/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 194523474269.6585 - val_loss: 171996282880.0000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 12225602560.00000\n",
            "Epoch 139/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 185968541096.5854 - val_loss: 168893284352.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 12225602560.00000\n",
            "Epoch 140/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 193785174165.8537 - val_loss: 165812879360.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 12225602560.00000\n",
            "Epoch 141/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 197908945045.8537 - val_loss: 162750906368.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 12225602560.00000\n",
            "Epoch 142/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 178597031236.6829 - val_loss: 159706824704.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 12225602560.00000\n",
            "Epoch 143/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 185705701775.6097 - val_loss: 156704686080.0000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 12225602560.00000\n",
            "Epoch 144/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 174390605873.9512 - val_loss: 153728253952.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 12225602560.00000\n",
            "Epoch 145/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 191736116298.9268 - val_loss: 150790668288.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 12225602560.00000\n",
            "Epoch 146/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 161674667382.6342 - val_loss: 147919437824.0000\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 12225602560.00000\n",
            "Epoch 147/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 167221592863.2195 - val_loss: 145033625600.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 12225602560.00000\n",
            "Epoch 148/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 168133281941.8537 - val_loss: 142239137792.0000\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 12225602560.00000\n",
            "Epoch 149/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 166505099963.3171 - val_loss: 139446484992.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 12225602560.00000\n",
            "Epoch 150/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 148781693077.8537 - val_loss: 136727126016.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 12225602560.00000\n",
            "Epoch 151/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 159972734726.2439 - val_loss: 134024994816.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 12225602560.00000\n",
            "Epoch 152/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 155556350901.0732 - val_loss: 131380994048.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 12225602560.00000\n",
            "Epoch 153/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 153994993564.0976 - val_loss: 128809893888.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 12225602560.00000\n",
            "Epoch 154/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 149053164668.8781 - val_loss: 126277541888.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 12225602560.00000\n",
            "Epoch 155/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 143654648856.9756 - val_loss: 123802083328.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 12225602560.00000\n",
            "Epoch 156/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 133183157223.0244 - val_loss: 121391349760.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 12225602560.00000\n",
            "Epoch 157/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 135200227927.4146 - val_loss: 119029522432.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 12225602560.00000\n",
            "Epoch 158/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 129421445169.9512 - val_loss: 116732821504.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 12225602560.00000\n",
            "Epoch 159/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 131042508000.7805 - val_loss: 114496151552.0000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 12225602560.00000\n",
            "Epoch 160/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 140193526459.3171 - val_loss: 112303931392.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 12225602560.00000\n",
            "Epoch 161/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 123185347409.1707 - val_loss: 110215618560.0000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 12225602560.00000\n",
            "Epoch 162/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 124940946856.5854 - val_loss: 108146589696.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 12225602560.00000\n",
            "Epoch 163/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 137519604910.8293 - val_loss: 106168377344.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 12225602560.00000\n",
            "Epoch 164/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 115213993134.8293 - val_loss: 104270790656.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 12225602560.00000\n",
            "Epoch 165/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 125395237013.8537 - val_loss: 102445244416.0000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 12225602560.00000\n",
            "Epoch 166/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 111710753966.8293 - val_loss: 100675313664.0000\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 12225602560.00000\n",
            "Epoch 167/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 114840196071.0244 - val_loss: 98986270720.0000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 12225602560.00000\n",
            "Epoch 168/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 112509209425.1707 - val_loss: 97340522496.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 12225602560.00000\n",
            "Epoch 169/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 108088292526.8293 - val_loss: 95799828480.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 12225602560.00000\n",
            "Epoch 170/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 120369475883.7073 - val_loss: 94327341056.0000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 12225602560.00000\n",
            "Epoch 171/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 118424942641.9512 - val_loss: 92931981312.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 12225602560.00000\n",
            "Epoch 172/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 103809241387.7073 - val_loss: 91612299264.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 12225602560.00000\n",
            "Epoch 173/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 110734735709.6585 - val_loss: 90358685696.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 12225602560.00000\n",
            "Epoch 174/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 108450960658.7317 - val_loss: 89179136000.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 12225602560.00000\n",
            "Epoch 175/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 109221051616.7805 - val_loss: 88073936896.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 12225602560.00000\n",
            "Epoch 176/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 107080032655.6098 - val_loss: 87017635840.0000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 12225602560.00000\n",
            "Epoch 177/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 104765075955.5122 - val_loss: 86065364992.0000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 12225602560.00000\n",
            "Epoch 178/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 100393933149.6585 - val_loss: 85174861824.0000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 12225602560.00000\n",
            "Epoch 179/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 97579129231.6098 - val_loss: 84334739456.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 12225602560.00000\n",
            "Epoch 180/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 105395299402.9268 - val_loss: 83564265472.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 12225602560.00000\n",
            "Epoch 181/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 95300352149.8537 - val_loss: 82867273728.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 12225602560.00000\n",
            "Epoch 182/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 94772273951.2195 - val_loss: 82219597824.0000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 12225602560.00000\n",
            "Epoch 183/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 101216200154.5366 - val_loss: 81646387200.0000\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 12225602560.00000\n",
            "Epoch 184/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 104327689290.9268 - val_loss: 81113710592.0000\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 12225602560.00000\n",
            "Epoch 185/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 89048711767.4146 - val_loss: 80639565824.0000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 12225602560.00000\n",
            "Epoch 186/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 98438988275.5122 - val_loss: 80204824576.0000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 12225602560.00000\n",
            "Epoch 187/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 101614477312.0000 - val_loss: 79834431488.0000\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 12225602560.00000\n",
            "Epoch 188/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 94164914176.0000 - val_loss: 79493783552.0000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 12225602560.00000\n",
            "Epoch 189/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 96467880885.0732 - val_loss: 79187755008.0000\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 12225602560.00000\n",
            "Epoch 190/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 90665295272.5854 - val_loss: 78927642624.0000\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 12225602560.00000\n",
            "Epoch 191/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 81787155381.0732 - val_loss: 78704861184.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 12225602560.00000\n",
            "Epoch 192/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 98125829294.8293 - val_loss: 78490632192.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 12225602560.00000\n",
            "Epoch 193/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 89777769696.7805 - val_loss: 78310547456.0000\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 12225602560.00000\n",
            "Epoch 194/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 93448207335.0244 - val_loss: 78154088448.0000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 12225602560.00000\n",
            "Epoch 195/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 87933903147.7073 - val_loss: 78016585728.0000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 12225602560.00000\n",
            "Epoch 196/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 89762814301.6585 - val_loss: 77895860224.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 12225602560.00000\n",
            "Epoch 197/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 89967580434.7317 - val_loss: 77792575488.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 12225602560.00000\n",
            "Epoch 198/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 104275764099.1219 - val_loss: 77694296064.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 12225602560.00000\n",
            "Epoch 199/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 93966186446.0488 - val_loss: 77609394176.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 12225602560.00000\n",
            "Epoch 200/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 93133543074.3415 - val_loss: 77528842240.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 12225602560.00000\n",
            "Epoch 201/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 94691208616.5854 - val_loss: 77453385728.0000\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 12225602560.00000\n",
            "Epoch 202/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 97461749310.4390 - val_loss: 77382041600.0000\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 12225602560.00000\n",
            "Epoch 203/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 84920077636.6829 - val_loss: 77313155072.0000\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 12225602560.00000\n",
            "Epoch 204/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 86502222922.9268 - val_loss: 77244555264.0000\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 12225602560.00000\n",
            "Epoch 205/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 99495491434.1463 - val_loss: 77175840768.0000\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 12225602560.00000\n",
            "Epoch 206/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 95191008880.3902 - val_loss: 77106282496.0000\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 12225602560.00000\n",
            "Epoch 207/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 96224800468.2927 - val_loss: 77038542848.0000\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 12225602560.00000\n",
            "Epoch 208/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 87035793607.8049 - val_loss: 76969246720.0000\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 12225602560.00000\n",
            "Epoch 209/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 86693049319.0244 - val_loss: 76897878016.0000\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 12225602560.00000\n",
            "Epoch 210/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 90032419165.6585 - val_loss: 76827082752.0000\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 12225602560.00000\n",
            "Epoch 211/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 91662168113.9512 - val_loss: 76755877888.0000\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 12225602560.00000\n",
            "Epoch 212/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 95196343820.4878 - val_loss: 76682477568.0000\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 12225602560.00000\n",
            "Epoch 213/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 96504382139.3171 - val_loss: 76602318848.0000\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 12225602560.00000\n",
            "Epoch 214/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 95357544497.9512 - val_loss: 76528107520.0000\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 12225602560.00000\n",
            "Epoch 215/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 96882414267.3171 - val_loss: 76449054720.0000\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 12225602560.00000\n",
            "Epoch 216/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 92973667702.6341 - val_loss: 76365578240.0000\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 12225602560.00000\n",
            "Epoch 217/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 93415504321.5610 - val_loss: 76285116416.0000\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 12225602560.00000\n",
            "Epoch 218/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 91066947733.8537 - val_loss: 76206260224.0000\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 12225602560.00000\n",
            "Epoch 219/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 82663978958.0488 - val_loss: 76126085120.0000\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 12225602560.00000\n",
            "Epoch 220/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 82469014053.4634 - val_loss: 76045033472.0000\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 12225602560.00000\n",
            "Epoch 221/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 101273498648.9756 - val_loss: 75956232192.0000\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 12225602560.00000\n",
            "Epoch 222/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 86969953404.8781 - val_loss: 75870855168.0000\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 12225602560.00000\n",
            "Epoch 223/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 91596391099.3171 - val_loss: 75788599296.0000\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 12225602560.00000\n",
            "Epoch 224/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 94558475039.2195 - val_loss: 75699216384.0000\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 12225602560.00000\n",
            "Epoch 225/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 86375818714.5366 - val_loss: 75607588864.0000\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 12225602560.00000\n",
            "Epoch 226/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 93914245719.4146 - val_loss: 75519557632.0000\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 12225602560.00000\n",
            "Epoch 227/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 87255871787.7073 - val_loss: 75421655040.0000\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 12225602560.00000\n",
            "Epoch 228/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 90850142357.8537 - val_loss: 75336998912.0000\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 12225602560.00000\n",
            "Epoch 229/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 91994053357.2683 - val_loss: 75243454464.0000\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 12225602560.00000\n",
            "Epoch 230/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 89876709176.1951 - val_loss: 75157544960.0000\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 12225602560.00000\n",
            "Epoch 231/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 85720595281.1707 - val_loss: 75057029120.0000\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 12225602560.00000\n",
            "Epoch 232/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 86824380715.7073 - val_loss: 74964459520.0000\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 12225602560.00000\n",
            "Epoch 233/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 87063787270.2439 - val_loss: 74872717312.0000\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 12225602560.00000\n",
            "Epoch 234/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 84810106330.5366 - val_loss: 74776625152.0000\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 12225602560.00000\n",
            "Epoch 235/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 82208672543.2195 - val_loss: 74690961408.0000\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 12225602560.00000\n",
            "Epoch 236/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 89579972358.2439 - val_loss: 74588815360.0000\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 12225602560.00000\n",
            "Epoch 237/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 95489566969.7561 - val_loss: 74492387328.0000\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 12225602560.00000\n",
            "Epoch 238/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 94550964373.8537 - val_loss: 74391199744.0000\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 12225602560.00000\n",
            "Epoch 239/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 90855614963.5122 - val_loss: 74292158464.0000\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 12225602560.00000\n",
            "Epoch 240/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 87469077479.0244 - val_loss: 74192412672.0000\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 12225602560.00000\n",
            "Epoch 241/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 89066903751.8049 - val_loss: 74087849984.0000\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 12225602560.00000\n",
            "Epoch 242/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 85946853276.0976 - val_loss: 73987588096.0000\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 12225602560.00000\n",
            "Epoch 243/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 92083655854.8293 - val_loss: 73881370624.0000\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 12225602560.00000\n",
            "Epoch 244/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 99208011076.6829 - val_loss: 73774915584.0000\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 12225602560.00000\n",
            "Epoch 245/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 88748623222.6341 - val_loss: 73666019328.0000\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 12225602560.00000\n",
            "Epoch 246/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 90508193991.8049 - val_loss: 73562349568.0000\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 12225602560.00000\n",
            "Epoch 247/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 84437424227.9024 - val_loss: 73461596160.0000\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 12225602560.00000\n",
            "Epoch 248/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 86541315096.9756 - val_loss: 73355837440.0000\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 12225602560.00000\n",
            "Epoch 249/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 88252622198.6341 - val_loss: 73237569536.0000\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 12225602560.00000\n",
            "Epoch 250/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 91437589778.7317 - val_loss: 73132974080.0000\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 12225602560.00000\n",
            "Epoch 251/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 90421513840.3902 - val_loss: 73008906240.0000\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 12225602560.00000\n",
            "Epoch 252/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 90905893513.3659 - val_loss: 72894930944.0000\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 12225602560.00000\n",
            "Epoch 253/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 90026220469.0732 - val_loss: 72782036992.0000\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 12225602560.00000\n",
            "Epoch 254/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 83273634441.3659 - val_loss: 72664735744.0000\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 12225602560.00000\n",
            "Epoch 255/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 81035821855.2195 - val_loss: 72547688448.0000\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 12225602560.00000\n",
            "Epoch 256/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 79325085895.8049 - val_loss: 72430157824.0000\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 12225602560.00000\n",
            "Epoch 257/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 89617974746.5366 - val_loss: 72313036800.0000\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 12225602560.00000\n",
            "Epoch 258/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 91519476910.8293 - val_loss: 72181317632.0000\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 12225602560.00000\n",
            "Epoch 259/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 96532294181.4634 - val_loss: 72068669440.0000\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 12225602560.00000\n",
            "Epoch 260/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 88150644935.8049 - val_loss: 71949262848.0000\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 12225602560.00000\n",
            "Epoch 261/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 89565552640.0000 - val_loss: 71817928704.0000\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 12225602560.00000\n",
            "Epoch 262/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 80959653038.8293 - val_loss: 71702839296.0000\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 12225602560.00000\n",
            "Epoch 263/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 93835087122.7317 - val_loss: 71581212672.0000\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 12225602560.00000\n",
            "Epoch 264/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 76057476370.7317 - val_loss: 71451467776.0000\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 12225602560.00000\n",
            "Epoch 265/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 91680812956.0976 - val_loss: 71324549120.0000\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 12225602560.00000\n",
            "Epoch 266/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 89467989216.7805 - val_loss: 71195459584.0000\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 12225602560.00000\n",
            "Epoch 267/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 81533342495.2195 - val_loss: 71057219584.0000\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 12225602560.00000\n",
            "Epoch 268/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 84777179985.1707 - val_loss: 70930391040.0000\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 12225602560.00000\n",
            "Epoch 269/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 82093471893.8537 - val_loss: 70799785984.0000\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 12225602560.00000\n",
            "Epoch 270/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 84380662609.1707 - val_loss: 70671228928.0000\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 12225602560.00000\n",
            "Epoch 271/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 90040237280.7805 - val_loss: 70524968960.0000\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 12225602560.00000\n",
            "Epoch 272/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 87808525986.3415 - val_loss: 70394085376.0000\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 12225602560.00000\n",
            "Epoch 273/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 83410292835.9024 - val_loss: 70258368512.0000\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 12225602560.00000\n",
            "Epoch 274/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 85515224688.3902 - val_loss: 70122864640.0000\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 12225602560.00000\n",
            "Epoch 275/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 77854851521.5610 - val_loss: 69982486528.0000\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 12225602560.00000\n",
            "Epoch 276/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 79209570453.8537 - val_loss: 69836726272.0000\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 12225602560.00000\n",
            "Epoch 277/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 83142968494.8293 - val_loss: 69697732608.0000\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 12225602560.00000\n",
            "Epoch 278/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 86960521315.9024 - val_loss: 69550481408.0000\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 12225602560.00000\n",
            "Epoch 279/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 81530544427.7073 - val_loss: 69412315136.0000\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 12225602560.00000\n",
            "Epoch 280/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 77711673993.3659 - val_loss: 69263802368.0000\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 12225602560.00000\n",
            "Epoch 281/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 89812452876.4878 - val_loss: 69125472256.0000\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 12225602560.00000\n",
            "Epoch 282/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 89794440416.7805 - val_loss: 68986257408.0000\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 12225602560.00000\n",
            "Epoch 283/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 89883402240.0000 - val_loss: 68834304000.0000\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 12225602560.00000\n",
            "Epoch 284/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 84706644517.4634 - val_loss: 68684697600.0000\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 12225602560.00000\n",
            "Epoch 285/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 88136041946.5366 - val_loss: 68540305408.0000\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 12225602560.00000\n",
            "Epoch 286/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 75158098869.0732 - val_loss: 68401373184.0000\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 12225602560.00000\n",
            "Epoch 287/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 83709258876.8781 - val_loss: 68248207360.0000\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 12225602560.00000\n",
            "Epoch 288/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 85760369139.5122 - val_loss: 68100362240.0000\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 12225602560.00000\n",
            "Epoch 289/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 75465338730.1463 - val_loss: 67954458624.0000\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 12225602560.00000\n",
            "Epoch 290/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 74458251313.9512 - val_loss: 67807428608.0000\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 12225602560.00000\n",
            "Epoch 291/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 80089022813.6585 - val_loss: 67655344128.0000\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 12225602560.00000\n",
            "Epoch 292/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 85339386904.9756 - val_loss: 67506167808.0000\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 12225602560.00000\n",
            "Epoch 293/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 76517296527.6098 - val_loss: 67351371776.0000\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 12225602560.00000\n",
            "Epoch 294/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 82982804629.8537 - val_loss: 67196026880.0000\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 12225602560.00000\n",
            "Epoch 295/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 77766726680.9756 - val_loss: 67040751616.0000\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 12225602560.00000\n",
            "Epoch 296/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 84412864861.6585 - val_loss: 66888269824.0000\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 12225602560.00000\n",
            "Epoch 297/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 82150352146.7317 - val_loss: 66726100992.0000\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 12225602560.00000\n",
            "Epoch 298/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 76704526535.8049 - val_loss: 66571280384.0000\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 12225602560.00000\n",
            "Epoch 299/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 77648615074.3415 - val_loss: 66424983552.0000\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 12225602560.00000\n",
            "Epoch 300/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 76070201543.8049 - val_loss: 66269274112.0000\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 12225602560.00000\n",
            "Epoch 301/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 79263772771.9024 - val_loss: 66109091840.0000\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 12225602560.00000\n",
            "Epoch 302/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 80249269722.5366 - val_loss: 65949274112.0000\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 12225602560.00000\n",
            "Epoch 303/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 80171434483.5122 - val_loss: 65790009344.0000\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 12225602560.00000\n",
            "Epoch 304/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 85483565755.3171 - val_loss: 65624612864.0000\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 12225602560.00000\n",
            "Epoch 305/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 74035756156.8781 - val_loss: 65475248128.0000\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 12225602560.00000\n",
            "Epoch 306/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 79980054178.3415 - val_loss: 65318301696.0000\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 12225602560.00000\n",
            "Epoch 307/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 73829605925.4634 - val_loss: 65146843136.0000\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 12225602560.00000\n",
            "Epoch 308/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 78344677525.8537 - val_loss: 64988622848.0000\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 12225602560.00000\n",
            "Epoch 309/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 71993130708.2927 - val_loss: 64826707968.0000\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 12225602560.00000\n",
            "Epoch 310/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 75997068812.4878 - val_loss: 64658010112.0000\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 12225602560.00000\n",
            "Epoch 311/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 73715398456.1951 - val_loss: 64490848256.0000\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 12225602560.00000\n",
            "Epoch 312/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 77324449692.0976 - val_loss: 64333000704.0000\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 12225602560.00000\n",
            "Epoch 313/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 75622146747.3171 - val_loss: 64170721280.0000\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 12225602560.00000\n",
            "Epoch 314/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 82673407850.1463 - val_loss: 63999995904.0000\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 12225602560.00000\n",
            "Epoch 315/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 74581752557.2683 - val_loss: 63828619264.0000\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 12225602560.00000\n",
            "Epoch 316/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 77392198331.3171 - val_loss: 63666503680.0000\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 12225602560.00000\n",
            "Epoch 317/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 75275185976.1951 - val_loss: 63496044544.0000\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 12225602560.00000\n",
            "Epoch 318/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 78463076352.0000 - val_loss: 63330672640.0000\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 12225602560.00000\n",
            "Epoch 319/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 78101976788.2927 - val_loss: 63166197760.0000\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 12225602560.00000\n",
            "Epoch 320/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 70702486303.2195 - val_loss: 62993354752.0000\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 12225602560.00000\n",
            "Epoch 321/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 77121867226.5366 - val_loss: 62831226880.0000\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 12225602560.00000\n",
            "Epoch 322/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 73343020656.3902 - val_loss: 62662619136.0000\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 12225602560.00000\n",
            "Epoch 323/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 73950637031.0244 - val_loss: 62498377728.0000\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 12225602560.00000\n",
            "Epoch 324/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 76577064160.7805 - val_loss: 62340288512.0000\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 12225602560.00000\n",
            "Epoch 325/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 75172916448.7805 - val_loss: 62163062784.0000\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 12225602560.00000\n",
            "Epoch 326/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 75851762613.0732 - val_loss: 61996122112.0000\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 12225602560.00000\n",
            "Epoch 327/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 91421140742.2439 - val_loss: 61830262784.0000\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 12225602560.00000\n",
            "Epoch 328/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 65204485020.0976 - val_loss: 61654302720.0000\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 12225602560.00000\n",
            "Epoch 329/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 70731760065.5610 - val_loss: 61491646464.0000\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 12225602560.00000\n",
            "Epoch 330/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 77710920679.0244 - val_loss: 61320425472.0000\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 12225602560.00000\n",
            "Epoch 331/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 72190849473.5610 - val_loss: 61151264768.0000\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 12225602560.00000\n",
            "Epoch 332/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 76275441863.8049 - val_loss: 60979040256.0000\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 12225602560.00000\n",
            "Epoch 333/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 73372613457.1707 - val_loss: 60808466432.0000\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 12225602560.00000\n",
            "Epoch 334/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 71488900420.6829 - val_loss: 60645187584.0000\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 12225602560.00000\n",
            "Epoch 335/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 76302419818.1463 - val_loss: 60471812096.0000\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 12225602560.00000\n",
            "Epoch 336/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 74081222256.3902 - val_loss: 60302413824.0000\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 12225602560.00000\n",
            "Epoch 337/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 69368249918.4390 - val_loss: 60126367744.0000\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 12225602560.00000\n",
            "Epoch 338/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68207536277.8537 - val_loss: 59963588608.0000\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 12225602560.00000\n",
            "Epoch 339/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 72571949655.4146 - val_loss: 59790946304.0000\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 12225602560.00000\n",
            "Epoch 340/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68610130968.9756 - val_loss: 59620794368.0000\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 12225602560.00000\n",
            "Epoch 341/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 72144109318.2439 - val_loss: 59448754176.0000\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 12225602560.00000\n",
            "Epoch 342/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60830195262.4390 - val_loss: 59282747392.0000\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 12225602560.00000\n",
            "Epoch 343/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 63751557419.7073 - val_loss: 59110440960.0000\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 12225602560.00000\n",
            "Epoch 344/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 71719124992.0000 - val_loss: 58937827328.0000\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 12225602560.00000\n",
            "Epoch 345/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 73969605207.4146 - val_loss: 58772369408.0000\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 12225602560.00000\n",
            "Epoch 346/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 65964902799.6098 - val_loss: 58599690240.0000\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 12225602560.00000\n",
            "Epoch 347/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 65437997305.7561 - val_loss: 58435358720.0000\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 12225602560.00000\n",
            "Epoch 348/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68475288301.2683 - val_loss: 58263687168.0000\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 12225602560.00000\n",
            "Epoch 349/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68229216555.7073 - val_loss: 58098655232.0000\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 12225602560.00000\n",
            "Epoch 350/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 68925580362.9268 - val_loss: 57919889408.0000\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 12225602560.00000\n",
            "Epoch 351/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68026606017.5610 - val_loss: 57752686592.0000\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 12225602560.00000\n",
            "Epoch 352/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 70311287683.1219 - val_loss: 57589374976.0000\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 12225602560.00000\n",
            "Epoch 353/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 67059240310.6341 - val_loss: 57421938688.0000\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 12225602560.00000\n",
            "Epoch 354/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 71591060604.8781 - val_loss: 57250869248.0000\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 12225602560.00000\n",
            "Epoch 355/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 75489268411.3171 - val_loss: 57085104128.0000\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 12225602560.00000\n",
            "Epoch 356/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 67572773713.1707 - val_loss: 56914092032.0000\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 12225602560.00000\n",
            "Epoch 357/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 66244204494.0488 - val_loss: 56750608384.0000\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 12225602560.00000\n",
            "Epoch 358/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 65024422137.7561 - val_loss: 56587751424.0000\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 12225602560.00000\n",
            "Epoch 359/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 74876157052.8781 - val_loss: 56420306944.0000\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 12225602560.00000\n",
            "Epoch 360/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 73817843212.4878 - val_loss: 56258383872.0000\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 12225602560.00000\n",
            "Epoch 361/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 62250824728.9756 - val_loss: 56089006080.0000\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 12225602560.00000\n",
            "Epoch 362/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 67541324974.8293 - val_loss: 55925514240.0000\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 12225602560.00000\n",
            "Epoch 363/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 64180774162.7317 - val_loss: 55765745664.0000\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 12225602560.00000\n",
            "Epoch 364/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 64791356690.7317 - val_loss: 55600300032.0000\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 12225602560.00000\n",
            "Epoch 365/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 66340901663.2195 - val_loss: 55438798848.0000\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 12225602560.00000\n",
            "Epoch 366/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 65308260451.9024 - val_loss: 55274999808.0000\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 12225602560.00000\n",
            "Epoch 367/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 72956141468.0976 - val_loss: 55107661824.0000\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 12225602560.00000\n",
            "Epoch 368/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68917019323.3171 - val_loss: 54954557440.0000\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 12225602560.00000\n",
            "Epoch 369/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 59239503072.7805 - val_loss: 54795829248.0000\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 12225602560.00000\n",
            "Epoch 370/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68191598442.1463 - val_loss: 54639292416.0000\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 12225602560.00000\n",
            "Epoch 371/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 67948375564.4878 - val_loss: 54473494528.0000\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 12225602560.00000\n",
            "Epoch 372/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68711902582.6341 - val_loss: 54316679168.0000\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 12225602560.00000\n",
            "Epoch 373/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 70074955726.0488 - val_loss: 54164008960.0000\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 12225602560.00000\n",
            "Epoch 374/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68439665838.8293 - val_loss: 54008606720.0000\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 12225602560.00000\n",
            "Epoch 375/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 67143534342.2439 - val_loss: 53850677248.0000\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 12225602560.00000\n",
            "Epoch 376/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60404793893.4634 - val_loss: 53703589888.0000\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 12225602560.00000\n",
            "Epoch 377/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 64415654187.7073 - val_loss: 53550477312.0000\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 12225602560.00000\n",
            "Epoch 378/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 57651613346.3415 - val_loss: 53402857472.0000\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 12225602560.00000\n",
            "Epoch 379/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 61090799116.4878 - val_loss: 53254213632.0000\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 12225602560.00000\n",
            "Epoch 380/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 67004180330.1463 - val_loss: 53105422336.0000\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 12225602560.00000\n",
            "Epoch 381/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 59735429919.2195 - val_loss: 52957270016.0000\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 12225602560.00000\n",
            "Epoch 382/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 59891605254.2439 - val_loss: 52809629696.0000\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 12225602560.00000\n",
            "Epoch 383/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 57633997799.0244 - val_loss: 52663365632.0000\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 12225602560.00000\n",
            "Epoch 384/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 62352001673.3659 - val_loss: 52521857024.0000\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 12225602560.00000\n",
            "Epoch 385/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 57462187932.0976 - val_loss: 52379025408.0000\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 12225602560.00000\n",
            "Epoch 386/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 66038979359.2195 - val_loss: 52242960384.0000\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 12225602560.00000\n",
            "Epoch 387/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 58325540764.0976 - val_loss: 52100141056.0000\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 12225602560.00000\n",
            "Epoch 388/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 63095730675.5122 - val_loss: 51960946688.0000\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 12225602560.00000\n",
            "Epoch 389/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 62846617100.4878 - val_loss: 51822780416.0000\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 12225602560.00000\n",
            "Epoch 390/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61379370258.7317 - val_loss: 51686707200.0000\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 12225602560.00000\n",
            "Epoch 391/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 62725053864.5854 - val_loss: 51549200384.0000\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 12225602560.00000\n",
            "Epoch 392/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60842670529.5610 - val_loss: 51415670784.0000\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 12225602560.00000\n",
            "Epoch 393/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 64388100820.2927 - val_loss: 51277692928.0000\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 12225602560.00000\n",
            "Epoch 394/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 58806434091.7073 - val_loss: 51143221248.0000\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 12225602560.00000\n",
            "Epoch 395/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 63615292141.2683 - val_loss: 51013320704.0000\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 12225602560.00000\n",
            "Epoch 396/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 62488784371.5122 - val_loss: 50882437120.0000\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 12225602560.00000\n",
            "Epoch 397/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 71003986169.7561 - val_loss: 50750959616.0000\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 12225602560.00000\n",
            "Epoch 398/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 64637062918.2439 - val_loss: 50623938560.0000\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 12225602560.00000\n",
            "Epoch 399/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 60576414245.4634 - val_loss: 50497138688.0000\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 12225602560.00000\n",
            "Epoch 400/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60543204751.6098 - val_loss: 50374160384.0000\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 12225602560.00000\n",
            "Epoch 401/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 62077284352.0000 - val_loss: 50249474048.0000\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 12225602560.00000\n",
            "Epoch 402/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60778998359.4146 - val_loss: 50131775488.0000\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 12225602560.00000\n",
            "Epoch 403/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 59574264657.1707 - val_loss: 50007158784.0000\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 12225602560.00000\n",
            "Epoch 404/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 55023139065.7561 - val_loss: 49886789632.0000\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 12225602560.00000\n",
            "Epoch 405/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60377231459.9024 - val_loss: 49771073536.0000\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 12225602560.00000\n",
            "Epoch 406/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 59659457461.0732 - val_loss: 49654632448.0000\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 12225602560.00000\n",
            "Epoch 407/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 65062088928.7805 - val_loss: 49535381504.0000\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 12225602560.00000\n",
            "Epoch 408/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60856823957.8537 - val_loss: 49415041024.0000\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 12225602560.00000\n",
            "Epoch 409/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56124820355.1219 - val_loss: 49311137792.0000\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 12225602560.00000\n",
            "Epoch 410/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 58893808065.5610 - val_loss: 49196097536.0000\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 12225602560.00000\n",
            "Epoch 411/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 58419739872.7805 - val_loss: 49085100032.0000\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 12225602560.00000\n",
            "Epoch 412/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 57986917301.0732 - val_loss: 48972943360.0000\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 12225602560.00000\n",
            "Epoch 413/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54246258438.2439 - val_loss: 48870268928.0000\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 12225602560.00000\n",
            "Epoch 414/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 63986007064.9756 - val_loss: 48764272640.0000\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 12225602560.00000\n",
            "Epoch 415/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 64457504518.2439 - val_loss: 48652083200.0000\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 12225602560.00000\n",
            "Epoch 416/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 58151969367.4146 - val_loss: 48550002688.0000\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 12225602560.00000\n",
            "Epoch 417/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 55900428238.0488 - val_loss: 48451194880.0000\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 12225602560.00000\n",
            "Epoch 418/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 64947376227.9024 - val_loss: 48347168768.0000\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 12225602560.00000\n",
            "Epoch 419/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 55046724483.1219 - val_loss: 48251830272.0000\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 12225602560.00000\n",
            "Epoch 420/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 53833913968.3902 - val_loss: 48158171136.0000\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 12225602560.00000\n",
            "Epoch 421/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 64510831091.5122 - val_loss: 48057987072.0000\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 12225602560.00000\n",
            "Epoch 422/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 66312192399.6098 - val_loss: 47955488768.0000\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 12225602560.00000\n",
            "Epoch 423/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 62516148623.6098 - val_loss: 47867072512.0000\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 12225602560.00000\n",
            "Epoch 424/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 57268540990.4390 - val_loss: 47772459008.0000\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 12225602560.00000\n",
            "Epoch 425/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60775207161.7561 - val_loss: 47681720320.0000\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 12225602560.00000\n",
            "Epoch 426/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 56619319895.4146 - val_loss: 47593340928.0000\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 12225602560.00000\n",
            "Epoch 427/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 65553868400.3902 - val_loss: 47504506880.0000\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 12225602560.00000\n",
            "Epoch 428/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54255646120.5854 - val_loss: 47413653504.0000\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 12225602560.00000\n",
            "Epoch 429/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54063443543.4146 - val_loss: 47328604160.0000\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 12225602560.00000\n",
            "Epoch 430/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50610077671.0244 - val_loss: 47245144064.0000\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 12225602560.00000\n",
            "Epoch 431/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61923666219.7073 - val_loss: 47156838400.0000\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 12225602560.00000\n",
            "Epoch 432/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 52520427120.3902 - val_loss: 47072137216.0000\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 12225602560.00000\n",
            "Epoch 433/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56104991993.7561 - val_loss: 46984237056.0000\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 12225602560.00000\n",
            "Epoch 434/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 55713523462.2439 - val_loss: 46901022720.0000\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 12225602560.00000\n",
            "Epoch 435/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 53686823711.2195 - val_loss: 46822420480.0000\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 12225602560.00000\n",
            "Epoch 436/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 58099333969.1707 - val_loss: 46741118976.0000\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 12225602560.00000\n",
            "Epoch 437/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 51940062932.2927 - val_loss: 46657372160.0000\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 12225602560.00000\n",
            "Epoch 438/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 59235983110.2439 - val_loss: 46582411264.0000\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 12225602560.00000\n",
            "Epoch 439/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61885717978.5366 - val_loss: 46503989248.0000\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 12225602560.00000\n",
            "Epoch 440/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 52013959617.5610 - val_loss: 46427512832.0000\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 12225602560.00000\n",
            "Epoch 441/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 58823521105.1707 - val_loss: 46354714624.0000\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 12225602560.00000\n",
            "Epoch 442/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56136132258.3415 - val_loss: 46282014720.0000\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 12225602560.00000\n",
            "Epoch 443/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 58297335508.2927 - val_loss: 46207225856.0000\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 12225602560.00000\n",
            "Epoch 444/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56337448410.5366 - val_loss: 46134222848.0000\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 12225602560.00000\n",
            "Epoch 445/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60987000682.1463 - val_loss: 46062505984.0000\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 12225602560.00000\n",
            "Epoch 446/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 55481854026.9268 - val_loss: 45993279488.0000\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 12225602560.00000\n",
            "Epoch 447/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 59842725238.6341 - val_loss: 45923540992.0000\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 12225602560.00000\n",
            "Epoch 448/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 59923295606.6341 - val_loss: 45852803072.0000\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 12225602560.00000\n",
            "Epoch 449/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 51937728861.6585 - val_loss: 45787615232.0000\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 12225602560.00000\n",
            "Epoch 450/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 52207996828.0976 - val_loss: 45722025984.0000\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 12225602560.00000\n",
            "Epoch 451/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 59823843278.0488 - val_loss: 45657047040.0000\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 12225602560.00000\n",
            "Epoch 452/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 50311258886.2439 - val_loss: 45591396352.0000\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 12225602560.00000\n",
            "Epoch 453/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 50610081042.7317 - val_loss: 45528420352.0000\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 12225602560.00000\n",
            "Epoch 454/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 53406528487.0244 - val_loss: 45466578944.0000\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 12225602560.00000\n",
            "Epoch 455/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 58018709903.6098 - val_loss: 45403529216.0000\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 12225602560.00000\n",
            "Epoch 456/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54226821819.3171 - val_loss: 45341773824.0000\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 12225602560.00000\n",
            "Epoch 457/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 55920131646.4390 - val_loss: 45283254272.0000\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 12225602560.00000\n",
            "Epoch 458/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 53098944062.4390 - val_loss: 45220093952.0000\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 12225602560.00000\n",
            "Epoch 459/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61554187888.3902 - val_loss: 45164154880.0000\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 12225602560.00000\n",
            "Epoch 460/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50816700515.9024 - val_loss: 45106180096.0000\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 12225602560.00000\n",
            "Epoch 461/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50786359196.0976 - val_loss: 45049384960.0000\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 12225602560.00000\n",
            "Epoch 462/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 55062087480.1951 - val_loss: 44989153280.0000\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 12225602560.00000\n",
            "Epoch 463/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 57839377882.5366 - val_loss: 44932444160.0000\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 12225602560.00000\n",
            "Epoch 464/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 48738340689.1707 - val_loss: 44877701120.0000\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 12225602560.00000\n",
            "Epoch 465/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 48793976582.2439 - val_loss: 44824064000.0000\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 12225602560.00000\n",
            "Epoch 466/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 56950802931.5122 - val_loss: 44768563200.0000\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 12225602560.00000\n",
            "Epoch 467/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 51951350159.6098 - val_loss: 44714233856.0000\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 12225602560.00000\n",
            "Epoch 468/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 59338420623.6098 - val_loss: 44659826688.0000\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 12225602560.00000\n",
            "Epoch 469/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 52274405126.2439 - val_loss: 44610138112.0000\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 12225602560.00000\n",
            "Epoch 470/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 54670386350.8293 - val_loss: 44557250560.0000\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 12225602560.00000\n",
            "Epoch 471/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50601026934.6341 - val_loss: 44505767936.0000\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 12225602560.00000\n",
            "Epoch 472/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 55937852241.1707 - val_loss: 44455174144.0000\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 12225602560.00000\n",
            "Epoch 473/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 49353611813.4634 - val_loss: 44404346880.0000\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 12225602560.00000\n",
            "Epoch 474/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 62528329328.3902 - val_loss: 44352536576.0000\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 12225602560.00000\n",
            "Epoch 475/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 47744733408.7805 - val_loss: 44304769024.0000\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 12225602560.00000\n",
            "Epoch 476/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 47638137431.4146 - val_loss: 44259704832.0000\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 12225602560.00000\n",
            "Epoch 477/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54050498834.7317 - val_loss: 44211253248.0000\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 12225602560.00000\n",
            "Epoch 478/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 52426444175.6098 - val_loss: 44163141632.0000\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 12225602560.00000\n",
            "Epoch 479/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 55858278250.1463 - val_loss: 44116447232.0000\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 12225602560.00000\n",
            "Epoch 480/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 53124744666.5366 - val_loss: 44071493632.0000\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 12225602560.00000\n",
            "Epoch 481/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50338567392.7805 - val_loss: 44026064896.0000\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 12225602560.00000\n",
            "Epoch 482/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50536637415.0244 - val_loss: 43985313792.0000\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 12225602560.00000\n",
            "Epoch 483/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 51114581841.1707 - val_loss: 43939381248.0000\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 12225602560.00000\n",
            "Epoch 484/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 48390184110.8293 - val_loss: 43894173696.0000\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 12225602560.00000\n",
            "Epoch 485/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 52328855601.9512 - val_loss: 43849129984.0000\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 12225602560.00000\n",
            "Epoch 486/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 47748029290.1463 - val_loss: 43810512896.0000\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 12225602560.00000\n",
            "Epoch 487/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50875443299.9024 - val_loss: 43767783424.0000\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 12225602560.00000\n",
            "Epoch 488/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 55241688688.3902 - val_loss: 43725303808.0000\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 12225602560.00000\n",
            "Epoch 489/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 51984471614.4390 - val_loss: 43678846976.0000\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 12225602560.00000\n",
            "Epoch 490/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 52159290792.5854 - val_loss: 43638173696.0000\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 12225602560.00000\n",
            "Epoch 491/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 53021644600.1951 - val_loss: 43598843904.0000\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 12225602560.00000\n",
            "Epoch 492/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 47296594819.1219 - val_loss: 43560755200.0000\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 12225602560.00000\n",
            "Epoch 493/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56290700712.5854 - val_loss: 43515723776.0000\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 12225602560.00000\n",
            "Epoch 494/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 55173190331.3171 - val_loss: 43478192128.0000\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 12225602560.00000\n",
            "Epoch 495/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 53624690238.4390 - val_loss: 43435954176.0000\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 12225602560.00000\n",
            "Epoch 496/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 47898171142.2439 - val_loss: 43396055040.0000\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 12225602560.00000\n",
            "Epoch 497/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 54014625991.8049 - val_loss: 43357929472.0000\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 12225602560.00000\n",
            "Epoch 498/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 48073714163.5122 - val_loss: 43319087104.0000\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 12225602560.00000\n",
            "Epoch 499/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 50111210171.3171 - val_loss: 43282116608.0000\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 12225602560.00000\n",
            "Epoch 500/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 47987235265.5610 - val_loss: 43242012672.0000\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 12225602560.00000\n",
            "[CV 4/5; 10/10] END batch_size=128, epochs=500, learning_rate=5e-05, nn1=32, nn2=64, nn3=256;, score=0.613 total time= 2.5min\n",
            "[CV 5/5; 10/10] START batch_size=128, epochs=500, learning_rate=5e-05, nn1=32, nn2=64, nn3=256\n",
            "Epoch 1/500\n",
            "81/81 [==============================] - 1s 5ms/step - loss: 465052638232.9756 - val_loss: 455235534848.0000\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 12225602560.00000\n",
            "Epoch 2/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 417148169540.6829 - val_loss: 455235239936.0000\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 12225602560.00000\n",
            "Epoch 3/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 440355681954.3415 - val_loss: 455234617344.0000\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 12225602560.00000\n",
            "Epoch 4/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 428136138302.4390 - val_loss: 455233339392.0000\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 12225602560.00000\n",
            "Epoch 5/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 413842811429.4634 - val_loss: 455231209472.0000\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 12225602560.00000\n",
            "Epoch 6/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 430879113615.6097 - val_loss: 455227703296.0000\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 12225602560.00000\n",
            "Epoch 7/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 420568323146.9268 - val_loss: 455222001664.0000\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 12225602560.00000\n",
            "Epoch 8/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 436929347983.6097 - val_loss: 455213219840.0000\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 12225602560.00000\n",
            "Epoch 9/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 413579956923.3171 - val_loss: 455200538624.0000\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 12225602560.00000\n",
            "Epoch 10/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 439892632950.6342 - val_loss: 455183204352.0000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 12225602560.00000\n",
            "Epoch 11/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 412273827240.5854 - val_loss: 455161315328.0000\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 12225602560.00000\n",
            "Epoch 12/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 423654984429.2683 - val_loss: 455134543872.0000\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 12225602560.00000\n",
            "Epoch 13/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 432797320766.4390 - val_loss: 455102824448.0000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 12225602560.00000\n",
            "Epoch 14/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 429679619396.6829 - val_loss: 455065141248.0000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 12225602560.00000\n",
            "Epoch 15/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 430961625837.2683 - val_loss: 455020871680.0000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 12225602560.00000\n",
            "Epoch 16/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 422229020971.7073 - val_loss: 454969524224.0000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 12225602560.00000\n",
            "Epoch 17/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 425420388401.9512 - val_loss: 454910050304.0000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 12225602560.00000\n",
            "Epoch 18/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 417820733939.5122 - val_loss: 454842155008.0000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 12225602560.00000\n",
            "Epoch 19/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 435553489994.9268 - val_loss: 454765117440.0000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 12225602560.00000\n",
            "Epoch 20/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 430592493917.6585 - val_loss: 454678839296.0000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 12225602560.00000\n",
            "Epoch 21/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 436736383225.7561 - val_loss: 454582992896.0000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 12225602560.00000\n",
            "Epoch 22/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 435445778382.0488 - val_loss: 454476038144.0000\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 12225602560.00000\n",
            "Epoch 23/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 435723279784.5854 - val_loss: 454359220224.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 12225602560.00000\n",
            "Epoch 24/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 422175390944.7805 - val_loss: 454230310912.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 12225602560.00000\n",
            "Epoch 25/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 428247288956.8781 - val_loss: 454090358784.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 12225602560.00000\n",
            "Epoch 26/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 421650329699.9025 - val_loss: 453937823744.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 12225602560.00000\n",
            "Epoch 27/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 411501158200.1951 - val_loss: 453772279808.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 12225602560.00000\n",
            "Epoch 28/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 435285519285.0732 - val_loss: 453593137152.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 12225602560.00000\n",
            "Epoch 29/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 418192784209.1707 - val_loss: 453399969792.0000\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 12225602560.00000\n",
            "Epoch 30/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 430737037961.3658 - val_loss: 453192417280.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 12225602560.00000\n",
            "Epoch 31/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 431790074405.4634 - val_loss: 452967825408.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 12225602560.00000\n",
            "Epoch 32/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 433636413340.0975 - val_loss: 452729110528.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 12225602560.00000\n",
            "Epoch 33/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 424618549847.4146 - val_loss: 452474699776.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 12225602560.00000\n",
            "Epoch 34/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 422273002021.4634 - val_loss: 452202496000.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 12225602560.00000\n",
            "Epoch 35/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 424825100937.3658 - val_loss: 451912466432.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 12225602560.00000\n",
            "Epoch 36/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 422674816824.1951 - val_loss: 451604119552.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 12225602560.00000\n",
            "Epoch 37/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 429967139415.4146 - val_loss: 451275358208.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 12225602560.00000\n",
            "Epoch 38/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 404314601921.5610 - val_loss: 450930180096.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 12225602560.00000\n",
            "Epoch 39/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 417434540281.7561 - val_loss: 450564849664.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 12225602560.00000\n",
            "Epoch 40/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 414257341814.6342 - val_loss: 450179301376.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 12225602560.00000\n",
            "Epoch 41/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 401135451360.7805 - val_loss: 449773404160.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 12225602560.00000\n",
            "Epoch 42/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 412975279029.0732 - val_loss: 449346240512.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 12225602560.00000\n",
            "Epoch 43/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 412497388918.6342 - val_loss: 448897286144.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 12225602560.00000\n",
            "Epoch 44/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 417318021269.8536 - val_loss: 448426541056.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 12225602560.00000\n",
            "Epoch 45/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 444832896074.9268 - val_loss: 447934038016.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 12225602560.00000\n",
            "Epoch 46/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 395599343715.9025 - val_loss: 447420104704.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 12225602560.00000\n",
            "Epoch 47/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 442266479141.4634 - val_loss: 446878384128.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 12225602560.00000\n",
            "Epoch 48/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 408767315968.0000 - val_loss: 446313988096.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 12225602560.00000\n",
            "Epoch 49/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 408351398537.3658 - val_loss: 445727703040.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 12225602560.00000\n",
            "Epoch 50/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 405162651747.9025 - val_loss: 445117136896.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 12225602560.00000\n",
            "Epoch 51/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 406953370998.6342 - val_loss: 444478717952.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 12225602560.00000\n",
            "Epoch 52/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 429205448054.6342 - val_loss: 443817918464.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 12225602560.00000\n",
            "Epoch 53/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 419278207025.9512 - val_loss: 443129397248.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 12225602560.00000\n",
            "Epoch 54/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 419402439704.9756 - val_loss: 442414923776.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 12225602560.00000\n",
            "Epoch 55/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 413712611777.5610 - val_loss: 441674465280.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 12225602560.00000\n",
            "Epoch 56/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 432101190581.0732 - val_loss: 440908120064.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 12225602560.00000\n",
            "Epoch 57/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 413429186560.0000 - val_loss: 440111431680.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 12225602560.00000\n",
            "Epoch 58/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 420488626176.0000 - val_loss: 439288954880.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 12225602560.00000\n",
            "Epoch 59/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 405355236077.2683 - val_loss: 438440624128.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 12225602560.00000\n",
            "Epoch 60/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 430694019571.5122 - val_loss: 437560246272.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 12225602560.00000\n",
            "Epoch 61/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 398673528782.0488 - val_loss: 436652376064.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 12225602560.00000\n",
            "Epoch 62/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 408062188968.5854 - val_loss: 435715014656.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 12225602560.00000\n",
            "Epoch 63/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 399607232162.3415 - val_loss: 434753306624.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 12225602560.00000\n",
            "Epoch 64/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 431400895263.2195 - val_loss: 433746804736.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 12225602560.00000\n",
            "Epoch 65/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 390762288602.5366 - val_loss: 432726540288.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 12225602560.00000\n",
            "Epoch 66/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 407985475484.0975 - val_loss: 431667576832.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 12225602560.00000\n",
            "Epoch 67/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 392199826756.6829 - val_loss: 430582038528.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 12225602560.00000\n",
            "Epoch 68/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 391331649785.7561 - val_loss: 429467009024.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 12225602560.00000\n",
            "Epoch 69/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 393867165696.0000 - val_loss: 428316950528.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 12225602560.00000\n",
            "Epoch 70/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 389933848026.5366 - val_loss: 427137695744.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 12225602560.00000\n",
            "Epoch 71/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 399614148208.3903 - val_loss: 425925115904.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 12225602560.00000\n",
            "Epoch 72/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 382718626541.2683 - val_loss: 424681897984.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 12225602560.00000\n",
            "Epoch 73/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 388496395388.8781 - val_loss: 423403749376.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 12225602560.00000\n",
            "Epoch 74/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 388952677800.5854 - val_loss: 422098796544.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 12225602560.00000\n",
            "Epoch 75/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 405386027207.8049 - val_loss: 420754685952.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 12225602560.00000\n",
            "Epoch 76/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 392556782167.4146 - val_loss: 419384197120.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 12225602560.00000\n",
            "Epoch 77/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 381534689529.7561 - val_loss: 417972158464.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 12225602560.00000\n",
            "Epoch 78/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 386146479029.0732 - val_loss: 416531316736.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 12225602560.00000\n",
            "Epoch 79/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 384901537192.5854 - val_loss: 415062654976.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 12225602560.00000\n",
            "Epoch 80/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 365245017812.2927 - val_loss: 413559488512.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 12225602560.00000\n",
            "Epoch 81/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 395314097876.2927 - val_loss: 412016410624.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 12225602560.00000\n",
            "Epoch 82/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 387593356862.4390 - val_loss: 410439614464.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 12225602560.00000\n",
            "Epoch 83/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 365757027003.3171 - val_loss: 408835489792.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 12225602560.00000\n",
            "Epoch 84/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 378159570744.1951 - val_loss: 407190339584.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 12225602560.00000\n",
            "Epoch 85/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 381200298883.1219 - val_loss: 405513502720.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 12225602560.00000\n",
            "Epoch 86/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 371059694616.9756 - val_loss: 403812024320.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 12225602560.00000\n",
            "Epoch 87/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 370796919233.5610 - val_loss: 402062442496.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 12225602560.00000\n",
            "Epoch 88/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 368745910971.3171 - val_loss: 400277307392.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 12225602560.00000\n",
            "Epoch 89/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 386585993415.8049 - val_loss: 398459437056.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 12225602560.00000\n",
            "Epoch 90/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 372639103425.5610 - val_loss: 396607488000.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 12225602560.00000\n",
            "Epoch 91/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 379498577120.7805 - val_loss: 394737025024.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 12225602560.00000\n",
            "Epoch 92/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 375868347466.9268 - val_loss: 392817508352.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 12225602560.00000\n",
            "Epoch 93/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 372201642633.3658 - val_loss: 390870073344.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 12225602560.00000\n",
            "Epoch 94/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 372091498995.5122 - val_loss: 388883513344.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 12225602560.00000\n",
            "Epoch 95/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 350724639069.6585 - val_loss: 386869788672.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 12225602560.00000\n",
            "Epoch 96/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 357460788798.4390 - val_loss: 384815202304.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 12225602560.00000\n",
            "Epoch 97/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 345260627818.1464 - val_loss: 382728765440.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 12225602560.00000\n",
            "Epoch 98/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 343668110410.9268 - val_loss: 380609757184.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 12225602560.00000\n",
            "Epoch 99/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 360639459028.2927 - val_loss: 378449199104.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 12225602560.00000\n",
            "Epoch 100/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 365784084979.5122 - val_loss: 376266686464.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 12225602560.00000\n",
            "Epoch 101/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 340958472741.4634 - val_loss: 374039838720.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 12225602560.00000\n",
            "Epoch 102/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 354612208215.4146 - val_loss: 371785400320.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 12225602560.00000\n",
            "Epoch 103/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 366384154574.0488 - val_loss: 369497899008.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 12225602560.00000\n",
            "Epoch 104/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 351486818753.5610 - val_loss: 367178612736.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 12225602560.00000\n",
            "Epoch 105/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 329238407317.8536 - val_loss: 364823805952.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 12225602560.00000\n",
            "Epoch 106/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 310435384444.8781 - val_loss: 362444226560.0000\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 12225602560.00000\n",
            "Epoch 107/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 329997979148.4878 - val_loss: 360014774272.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 12225602560.00000\n",
            "Epoch 108/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 319288018894.0488 - val_loss: 357580046336.0000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 12225602560.00000\n",
            "Epoch 109/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 321365719489.5610 - val_loss: 355104358400.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 12225602560.00000\n",
            "Epoch 110/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 332938673776.3903 - val_loss: 352593641472.0000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 12225602560.00000\n",
            "Epoch 111/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 342399647344.3903 - val_loss: 350046191616.0000\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 12225602560.00000\n",
            "Epoch 112/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 339270774484.2927 - val_loss: 347470299136.0000\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 12225602560.00000\n",
            "Epoch 113/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 321482582165.8536 - val_loss: 344874516480.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 12225602560.00000\n",
            "Epoch 114/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 325837529287.8049 - val_loss: 342247440384.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 12225602560.00000\n",
            "Epoch 115/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 317053503038.4390 - val_loss: 339590643712.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 12225602560.00000\n",
            "Epoch 116/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 294795545525.0732 - val_loss: 336915726336.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 12225602560.00000\n",
            "Epoch 117/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 313620593688.9756 - val_loss: 334201225216.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 12225602560.00000\n",
            "Epoch 118/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 315970855911.0244 - val_loss: 331453956096.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 12225602560.00000\n",
            "Epoch 119/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 313632375183.6097 - val_loss: 328680505344.0000\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 12225602560.00000\n",
            "Epoch 120/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 301997379084.4878 - val_loss: 325899419648.0000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 12225602560.00000\n",
            "Epoch 121/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 292443756593.9512 - val_loss: 323070787584.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 12225602560.00000\n",
            "Epoch 122/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 274024697356.4878 - val_loss: 320242024448.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 12225602560.00000\n",
            "Epoch 123/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 301615888034.3415 - val_loss: 317370925056.0000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 12225602560.00000\n",
            "Epoch 124/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 284479311372.4878 - val_loss: 314485964800.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 12225602560.00000\n",
            "Epoch 125/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 295945161453.2683 - val_loss: 311577313280.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 12225602560.00000\n",
            "Epoch 126/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 282216722531.9025 - val_loss: 308650049536.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 12225602560.00000\n",
            "Epoch 127/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 295549318418.7317 - val_loss: 305702666240.0000\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 12225602560.00000\n",
            "Epoch 128/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 266253338773.8537 - val_loss: 302751055872.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 12225602560.00000\n",
            "Epoch 129/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 266422328344.9756 - val_loss: 299744100352.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 12225602560.00000\n",
            "Epoch 130/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 270674917425.9512 - val_loss: 296753168384.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 12225602560.00000\n",
            "Epoch 131/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 275104942629.4634 - val_loss: 293723373568.0000\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 12225602560.00000\n",
            "Epoch 132/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 277259855222.6342 - val_loss: 290692038656.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 12225602560.00000\n",
            "Epoch 133/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 247612136672.7805 - val_loss: 287650086912.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 12225602560.00000\n",
            "Epoch 134/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 259129136202.9268 - val_loss: 284566388736.0000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 12225602560.00000\n",
            "Epoch 135/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 259145755373.2683 - val_loss: 281495306240.0000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 12225602560.00000\n",
            "Epoch 136/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 254131253347.9024 - val_loss: 278418227200.0000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 12225602560.00000\n",
            "Epoch 137/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 266777347446.6342 - val_loss: 275319619584.0000\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 12225602560.00000\n",
            "Epoch 138/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 256006136807.0244 - val_loss: 272201678848.0000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 12225602560.00000\n",
            "Epoch 139/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 251984799494.2439 - val_loss: 269104660480.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 12225602560.00000\n",
            "Epoch 140/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 240099568165.4634 - val_loss: 265969238016.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 12225602560.00000\n",
            "Epoch 141/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 251093556248.9756 - val_loss: 262841532416.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 12225602560.00000\n",
            "Epoch 142/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 229571214560.7805 - val_loss: 259713564672.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 12225602560.00000\n",
            "Epoch 143/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 240097005867.7073 - val_loss: 256584089600.0000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 12225602560.00000\n",
            "Epoch 144/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 236095533955.1219 - val_loss: 253437100032.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 12225602560.00000\n",
            "Epoch 145/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 232427079430.2439 - val_loss: 250312556544.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 12225602560.00000\n",
            "Epoch 146/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 213531599247.6097 - val_loss: 247185506304.0000\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 12225602560.00000\n",
            "Epoch 147/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 239383720235.7073 - val_loss: 244038926336.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 12225602560.00000\n",
            "Epoch 148/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 226283238874.5366 - val_loss: 240915202048.0000\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 12225602560.00000\n",
            "Epoch 149/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 218235818883.1219 - val_loss: 237787119616.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 12225602560.00000\n",
            "Epoch 150/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 218542157524.2927 - val_loss: 234669883392.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 12225602560.00000\n",
            "Epoch 151/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 216615419704.1951 - val_loss: 231552909312.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 12225602560.00000\n",
            "Epoch 152/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 197340262100.2927 - val_loss: 228462084096.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 12225602560.00000\n",
            "Epoch 153/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 211843821667.9024 - val_loss: 225377157120.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 12225602560.00000\n",
            "Epoch 154/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 206684133026.3415 - val_loss: 222307991552.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 12225602560.00000\n",
            "Epoch 155/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 202117494734.0488 - val_loss: 219244134400.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 12225602560.00000\n",
            "Epoch 156/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 201299487968.7805 - val_loss: 216202952704.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 12225602560.00000\n",
            "Epoch 157/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 193926410539.7073 - val_loss: 213189525504.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 12225602560.00000\n",
            "Epoch 158/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 190417140161.5610 - val_loss: 210193547264.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 12225602560.00000\n",
            "Epoch 159/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 180015523790.0488 - val_loss: 207207677952.0000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 12225602560.00000\n",
            "Epoch 160/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 177918728042.1463 - val_loss: 204251922432.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 12225602560.00000\n",
            "Epoch 161/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 191980402538.1463 - val_loss: 201327198208.0000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 12225602560.00000\n",
            "Epoch 162/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 189233152599.4146 - val_loss: 198400425984.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 12225602560.00000\n",
            "Epoch 163/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 171687250169.7561 - val_loss: 195532832768.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 12225602560.00000\n",
            "Epoch 164/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 179144398897.9512 - val_loss: 192693616640.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 12225602560.00000\n",
            "Epoch 165/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 177508640318.4390 - val_loss: 189873078272.0000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 12225602560.00000\n",
            "Epoch 166/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 175771814437.4634 - val_loss: 187081637888.0000\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 12225602560.00000\n",
            "Epoch 167/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 154609109641.3658 - val_loss: 184356700160.0000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 12225602560.00000\n",
            "Epoch 168/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 155531470498.3415 - val_loss: 181638692864.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 12225602560.00000\n",
            "Epoch 169/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 147229920081.1707 - val_loss: 178969772032.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 12225602560.00000\n",
            "Epoch 170/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 147878522680.1951 - val_loss: 176315760640.0000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 12225602560.00000\n",
            "Epoch 171/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 157104944202.9268 - val_loss: 173715046400.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 12225602560.00000\n",
            "Epoch 172/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 181619742969.7561 - val_loss: 171178557440.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 12225602560.00000\n",
            "Epoch 173/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 151473342214.2439 - val_loss: 168688582656.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 12225602560.00000\n",
            "Epoch 174/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 155120680860.0976 - val_loss: 166227984384.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 12225602560.00000\n",
            "Epoch 175/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 136586416227.9024 - val_loss: 163849175040.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 12225602560.00000\n",
            "Epoch 176/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 141059278298.5366 - val_loss: 161465860096.0000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 12225602560.00000\n",
            "Epoch 177/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 135526048643.1219 - val_loss: 159181684736.0000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 12225602560.00000\n",
            "Epoch 178/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 136909389124.6829 - val_loss: 156908290048.0000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 12225602560.00000\n",
            "Epoch 179/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 133713605507.1219 - val_loss: 154717880320.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 12225602560.00000\n",
            "Epoch 180/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 133515474894.0488 - val_loss: 152587730944.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 12225602560.00000\n",
            "Epoch 181/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 133083708440.9756 - val_loss: 150481420288.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 12225602560.00000\n",
            "Epoch 182/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 131704838593.5610 - val_loss: 148455882752.0000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 12225602560.00000\n",
            "Epoch 183/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 132481871971.9024 - val_loss: 146477793280.0000\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 12225602560.00000\n",
            "Epoch 184/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 122358370204.0976 - val_loss: 144569843712.0000\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 12225602560.00000\n",
            "Epoch 185/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 125337015670.6341 - val_loss: 142714535936.0000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 12225602560.00000\n",
            "Epoch 186/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 139592172718.8293 - val_loss: 140912082944.0000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 12225602560.00000\n",
            "Epoch 187/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 129461127217.9512 - val_loss: 139185127424.0000\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 12225602560.00000\n",
            "Epoch 188/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 119159484815.6098 - val_loss: 137517449216.0000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 12225602560.00000\n",
            "Epoch 189/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 111571782780.8781 - val_loss: 135928446976.0000\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 12225602560.00000\n",
            "Epoch 190/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 124931672713.3659 - val_loss: 134360244224.0000\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 12225602560.00000\n",
            "Epoch 191/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 123427945696.7805 - val_loss: 132894212096.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 12225602560.00000\n",
            "Epoch 192/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 109111197396.2927 - val_loss: 131496329216.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 12225602560.00000\n",
            "Epoch 193/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 102013003726.0488 - val_loss: 130124300288.0000\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 12225602560.00000\n",
            "Epoch 194/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 115879962873.7561 - val_loss: 128834183168.0000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 12225602560.00000\n",
            "Epoch 195/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 113066872432.3902 - val_loss: 127599992832.0000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 12225602560.00000\n",
            "Epoch 196/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 106179688298.1463 - val_loss: 126423285760.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 12225602560.00000\n",
            "Epoch 197/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 120305433824.7805 - val_loss: 125318922240.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 12225602560.00000\n",
            "Epoch 198/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 109098835168.7805 - val_loss: 124286361600.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 12225602560.00000\n",
            "Epoch 199/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 99318005160.5854 - val_loss: 123316011008.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 12225602560.00000\n",
            "Epoch 200/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 113488190089.3659 - val_loss: 122369802240.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 12225602560.00000\n",
            "Epoch 201/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 105862715541.8537 - val_loss: 121499631616.0000\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 12225602560.00000\n",
            "Epoch 202/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 99034243321.7561 - val_loss: 120694546432.0000\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 12225602560.00000\n",
            "Epoch 203/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 102732681216.0000 - val_loss: 119926792192.0000\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 12225602560.00000\n",
            "Epoch 204/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 96584027660.4878 - val_loss: 119216513024.0000\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 12225602560.00000\n",
            "Epoch 205/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 103831113728.0000 - val_loss: 118561972224.0000\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 12225602560.00000\n",
            "Epoch 206/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 107939257868.4878 - val_loss: 117941616640.0000\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 12225602560.00000\n",
            "Epoch 207/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 103301296627.5122 - val_loss: 117377138688.0000\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 12225602560.00000\n",
            "Epoch 208/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 92115993025.5610 - val_loss: 116874371072.0000\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 12225602560.00000\n",
            "Epoch 209/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 102226753486.0488 - val_loss: 116386709504.0000\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 12225602560.00000\n",
            "Epoch 210/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 89433141947.3171 - val_loss: 115949207552.0000\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 12225602560.00000\n",
            "Epoch 211/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 92880635804.0976 - val_loss: 115534782464.0000\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 12225602560.00000\n",
            "Epoch 212/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 104465431776.7805 - val_loss: 115135520768.0000\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 12225602560.00000\n",
            "Epoch 213/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 101926857553.1707 - val_loss: 114802204672.0000\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 12225602560.00000\n",
            "Epoch 214/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 91577909947.3171 - val_loss: 114477793280.0000\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 12225602560.00000\n",
            "Epoch 215/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 99108076768.7805 - val_loss: 114180554752.0000\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 12225602560.00000\n",
            "Epoch 216/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 90638667676.0976 - val_loss: 113917222912.0000\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 12225602560.00000\n",
            "Epoch 217/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 97630119985.9512 - val_loss: 113664524288.0000\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 12225602560.00000\n",
            "Epoch 218/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 107042106692.6829 - val_loss: 113429094400.0000\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 12225602560.00000\n",
            "Epoch 219/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 93182978147.9024 - val_loss: 113206624256.0000\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 12225602560.00000\n",
            "Epoch 220/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 105342349012.2927 - val_loss: 113006174208.0000\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 12225602560.00000\n",
            "Epoch 221/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 91561708469.0732 - val_loss: 112798162944.0000\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 12225602560.00000\n",
            "Epoch 222/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 95228401713.9512 - val_loss: 112618455040.0000\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 12225602560.00000\n",
            "Epoch 223/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 98988618976.7805 - val_loss: 112443244544.0000\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 12225602560.00000\n",
            "Epoch 224/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 112085390111.2195 - val_loss: 112270368768.0000\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 12225602560.00000\n",
            "Epoch 225/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 94966726755.9024 - val_loss: 112112631808.0000\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 12225602560.00000\n",
            "Epoch 226/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 112763871132.0976 - val_loss: 111954567168.0000\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 12225602560.00000\n",
            "Epoch 227/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 93532993136.3902 - val_loss: 111796707328.0000\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 12225602560.00000\n",
            "Epoch 228/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 86741645911.4146 - val_loss: 111651086336.0000\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 12225602560.00000\n",
            "Epoch 229/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 91245379484.0976 - val_loss: 111511814144.0000\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 12225602560.00000\n",
            "Epoch 230/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 93131485134.0488 - val_loss: 111364022272.0000\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 12225602560.00000\n",
            "Epoch 231/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 93174472004.6829 - val_loss: 111219941376.0000\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 12225602560.00000\n",
            "Epoch 232/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 99415587465.3659 - val_loss: 111075557376.0000\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 12225602560.00000\n",
            "Epoch 233/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 92447139889.9512 - val_loss: 110947950592.0000\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 12225602560.00000\n",
            "Epoch 234/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 91171981661.6585 - val_loss: 110816059392.0000\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 12225602560.00000\n",
            "Epoch 235/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 98702269664.7805 - val_loss: 110668021760.0000\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 12225602560.00000\n",
            "Epoch 236/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 88013804618.9268 - val_loss: 110540374016.0000\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 12225602560.00000\n",
            "Epoch 237/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 89173904608.7805 - val_loss: 110396088320.0000\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 12225602560.00000\n",
            "Epoch 238/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 91836331982.0488 - val_loss: 110259560448.0000\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 12225602560.00000\n",
            "Epoch 239/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 92687121932.4878 - val_loss: 110119002112.0000\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 12225602560.00000\n",
            "Epoch 240/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 101344927744.0000 - val_loss: 109983752192.0000\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 12225602560.00000\n",
            "Epoch 241/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 89538085163.7073 - val_loss: 109854892032.0000\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 12225602560.00000\n",
            "Epoch 242/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 88256952544.7805 - val_loss: 109703102464.0000\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 12225602560.00000\n",
            "Epoch 243/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 95109247075.9024 - val_loss: 109559865344.0000\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 12225602560.00000\n",
            "Epoch 244/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 94576301480.5854 - val_loss: 109418356736.0000\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 12225602560.00000\n",
            "Epoch 245/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 99995581564.8781 - val_loss: 109281878016.0000\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 12225602560.00000\n",
            "Epoch 246/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 94554748578.3415 - val_loss: 109144244224.0000\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 12225602560.00000\n",
            "Epoch 247/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 86940622398.4390 - val_loss: 109005668352.0000\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 12225602560.00000\n",
            "Epoch 248/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 93861461616.3902 - val_loss: 108844720128.0000\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 12225602560.00000\n",
            "Epoch 249/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 91439720697.7561 - val_loss: 108702556160.0000\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 12225602560.00000\n",
            "Epoch 250/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 89163037421.2683 - val_loss: 108566036480.0000\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 12225602560.00000\n",
            "Epoch 251/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 102496184669.6585 - val_loss: 108407046144.0000\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 12225602560.00000\n",
            "Epoch 252/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 92016090386.7317 - val_loss: 108263374848.0000\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 12225602560.00000\n",
            "Epoch 253/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 87831568483.9024 - val_loss: 108098666496.0000\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 12225602560.00000\n",
            "Epoch 254/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 90776422849.5610 - val_loss: 107951841280.0000\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 12225602560.00000\n",
            "Epoch 255/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 102683959296.0000 - val_loss: 107795406848.0000\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 12225602560.00000\n",
            "Epoch 256/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 96351123905.5610 - val_loss: 107637366784.0000\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 12225602560.00000\n",
            "Epoch 257/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 109143089851.3171 - val_loss: 107478507520.0000\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 12225602560.00000\n",
            "Epoch 258/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 93809801365.8537 - val_loss: 107314962432.0000\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 12225602560.00000\n",
            "Epoch 259/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 96495692525.2683 - val_loss: 107148132352.0000\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 12225602560.00000\n",
            "Epoch 260/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 87154157168.3902 - val_loss: 106969964544.0000\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 12225602560.00000\n",
            "Epoch 261/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 85736676227.1219 - val_loss: 106797400064.0000\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 12225602560.00000\n",
            "Epoch 262/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 94617491406.0488 - val_loss: 106623426560.0000\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 12225602560.00000\n",
            "Epoch 263/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 99980271715.9024 - val_loss: 106455646208.0000\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 12225602560.00000\n",
            "Epoch 264/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 84226534724.6829 - val_loss: 106279911424.0000\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 12225602560.00000\n",
            "Epoch 265/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 90368118484.2927 - val_loss: 106082541568.0000\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 12225602560.00000\n",
            "Epoch 266/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 93208537387.7073 - val_loss: 105907412992.0000\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 12225602560.00000\n",
            "Epoch 267/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 83988372455.0244 - val_loss: 105708052480.0000\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 12225602560.00000\n",
            "Epoch 268/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 88729704497.9512 - val_loss: 105505955840.0000\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 12225602560.00000\n",
            "Epoch 269/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 83354237877.0732 - val_loss: 105308340224.0000\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 12225602560.00000\n",
            "Epoch 270/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 83761684729.7561 - val_loss: 105110675456.0000\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 12225602560.00000\n",
            "Epoch 271/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 99444298926.8293 - val_loss: 104908431360.0000\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 12225602560.00000\n",
            "Epoch 272/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 84259768569.7561 - val_loss: 104711929856.0000\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 12225602560.00000\n",
            "Epoch 273/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 85397998916.6829 - val_loss: 104504270848.0000\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 12225602560.00000\n",
            "Epoch 274/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 84409777376.7805 - val_loss: 104281096192.0000\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 12225602560.00000\n",
            "Epoch 275/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 93105307448.1951 - val_loss: 104071905280.0000\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 12225602560.00000\n",
            "Epoch 276/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 91221449053.6585 - val_loss: 103864557568.0000\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 12225602560.00000\n",
            "Epoch 277/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 88106109277.6585 - val_loss: 103647346688.0000\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 12225602560.00000\n",
            "Epoch 278/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 84580733727.2195 - val_loss: 103422443520.0000\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 12225602560.00000\n",
            "Epoch 279/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 86573315596.4878 - val_loss: 103202078720.0000\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 12225602560.00000\n",
            "Epoch 280/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 93039698419.5122 - val_loss: 102981427200.0000\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 12225602560.00000\n",
            "Epoch 281/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 88717957369.7561 - val_loss: 102756007936.0000\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 12225602560.00000\n",
            "Epoch 282/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 89345934810.5366 - val_loss: 102519373824.0000\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 12225602560.00000\n",
            "Epoch 283/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 98142602115.1219 - val_loss: 102271762432.0000\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 12225602560.00000\n",
            "Epoch 284/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 95198945579.7073 - val_loss: 102046539776.0000\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 12225602560.00000\n",
            "Epoch 285/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 90578278949.4634 - val_loss: 101793325056.0000\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 12225602560.00000\n",
            "Epoch 286/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 93577477594.5366 - val_loss: 101561532416.0000\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 12225602560.00000\n",
            "Epoch 287/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 90970326390.6341 - val_loss: 101300740096.0000\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 12225602560.00000\n",
            "Epoch 288/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 91050629869.2683 - val_loss: 101081915392.0000\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 12225602560.00000\n",
            "Epoch 289/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 85347591342.8293 - val_loss: 100810252288.0000\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 12225602560.00000\n",
            "Epoch 290/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 76433543168.0000 - val_loss: 100573872128.0000\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 12225602560.00000\n",
            "Epoch 291/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 78368392816.3902 - val_loss: 100307238912.0000\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 12225602560.00000\n",
            "Epoch 292/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 87323585511.0244 - val_loss: 100054450176.0000\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 12225602560.00000\n",
            "Epoch 293/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 94256947599.6098 - val_loss: 99792265216.0000\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 12225602560.00000\n",
            "Epoch 294/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 98071967594.1463 - val_loss: 99525312512.0000\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 12225602560.00000\n",
            "Epoch 295/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 79725890634.9268 - val_loss: 99284934656.0000\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 12225602560.00000\n",
            "Epoch 296/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 78280986723.9024 - val_loss: 99030622208.0000\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 12225602560.00000\n",
            "Epoch 297/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 92055430518.6341 - val_loss: 98736144384.0000\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 12225602560.00000\n",
            "Epoch 298/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 81308806418.7317 - val_loss: 98484019200.0000\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 12225602560.00000\n",
            "Epoch 299/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 86704779813.4634 - val_loss: 98229682176.0000\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 12225602560.00000\n",
            "Epoch 300/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 76056372998.2439 - val_loss: 97977196544.0000\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 12225602560.00000\n",
            "Epoch 301/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 85199638078.4390 - val_loss: 97678950400.0000\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 12225602560.00000\n",
            "Epoch 302/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 81745104446.4390 - val_loss: 97432862720.0000\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 12225602560.00000\n",
            "Epoch 303/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 78354574211.1219 - val_loss: 97148518400.0000\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 12225602560.00000\n",
            "Epoch 304/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 97235521036.4878 - val_loss: 96862838784.0000\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 12225602560.00000\n",
            "Epoch 305/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 96374622557.6585 - val_loss: 96581419008.0000\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 12225602560.00000\n",
            "Epoch 306/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 81992947412.2927 - val_loss: 96305618944.0000\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 12225602560.00000\n",
            "Epoch 307/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 74695631647.2195 - val_loss: 96030474240.0000\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 12225602560.00000\n",
            "Epoch 308/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 90743627276.4878 - val_loss: 95733194752.0000\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 12225602560.00000\n",
            "Epoch 309/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 80904203538.7317 - val_loss: 95456108544.0000\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 12225602560.00000\n",
            "Epoch 310/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 94264479494.2439 - val_loss: 95157641216.0000\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 12225602560.00000\n",
            "Epoch 311/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 84996723087.6098 - val_loss: 94872993792.0000\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 12225602560.00000\n",
            "Epoch 312/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 77453621347.9024 - val_loss: 94587125760.0000\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 12225602560.00000\n",
            "Epoch 313/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 77818509162.1463 - val_loss: 94279057408.0000\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 12225602560.00000\n",
            "Epoch 314/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 92498974570.1463 - val_loss: 93999202304.0000\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 12225602560.00000\n",
            "Epoch 315/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 83100622548.2927 - val_loss: 93692084224.0000\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 12225602560.00000\n",
            "Epoch 316/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 72730714062.0488 - val_loss: 93394804736.0000\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 12225602560.00000\n",
            "Epoch 317/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 81103624142.0488 - val_loss: 93107666944.0000\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 12225602560.00000\n",
            "Epoch 318/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 81600639825.1707 - val_loss: 92788211712.0000\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 12225602560.00000\n",
            "Epoch 319/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 73887333950.4390 - val_loss: 92510609408.0000\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 12225602560.00000\n",
            "Epoch 320/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 82257034165.0732 - val_loss: 92188860416.0000\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 12225602560.00000\n",
            "Epoch 321/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 79113252564.2927 - val_loss: 91866324992.0000\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 12225602560.00000\n",
            "Epoch 322/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 74180682277.4634 - val_loss: 91576893440.0000\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 12225602560.00000\n",
            "Epoch 323/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 86021017600.0000 - val_loss: 91250089984.0000\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 12225602560.00000\n",
            "Epoch 324/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 81293180928.0000 - val_loss: 90966999040.0000\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 12225602560.00000\n",
            "Epoch 325/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 73549476689.1707 - val_loss: 90645200896.0000\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 12225602560.00000\n",
            "Epoch 326/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 74249880101.4634 - val_loss: 90341621760.0000\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 12225602560.00000\n",
            "Epoch 327/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 81484317371.3171 - val_loss: 90019430400.0000\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 12225602560.00000\n",
            "Epoch 328/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 79920963184.3902 - val_loss: 89699082240.0000\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 12225602560.00000\n",
            "Epoch 329/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 72531819145.3659 - val_loss: 89387196416.0000\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 12225602560.00000\n",
            "Epoch 330/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 86617540158.4390 - val_loss: 89068167168.0000\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 12225602560.00000\n",
            "Epoch 331/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 74787433247.2195 - val_loss: 88749154304.0000\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 12225602560.00000\n",
            "Epoch 332/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 75116343296.0000 - val_loss: 88433442816.0000\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 12225602560.00000\n",
            "Epoch 333/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 79201773468.0976 - val_loss: 88109432832.0000\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 12225602560.00000\n",
            "Epoch 334/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 72326599605.0732 - val_loss: 87801880576.0000\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 12225602560.00000\n",
            "Epoch 335/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 67852296591.6098 - val_loss: 87474462720.0000\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 12225602560.00000\n",
            "Epoch 336/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 80975209297.1707 - val_loss: 87129784320.0000\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 12225602560.00000\n",
            "Epoch 337/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 72884732403.5122 - val_loss: 86832930816.0000\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 12225602560.00000\n",
            "Epoch 338/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 75345493016.9756 - val_loss: 86502211584.0000\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 12225602560.00000\n",
            "Epoch 339/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 71398841368.9756 - val_loss: 86192373760.0000\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 12225602560.00000\n",
            "Epoch 340/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 69125255467.7073 - val_loss: 85876539392.0000\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 12225602560.00000\n",
            "Epoch 341/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 65182121234.7317 - val_loss: 85554274304.0000\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 12225602560.00000\n",
            "Epoch 342/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 71799752654.0488 - val_loss: 85243633664.0000\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 12225602560.00000\n",
            "Epoch 343/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68419656029.6585 - val_loss: 84917100544.0000\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 12225602560.00000\n",
            "Epoch 344/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 72287844302.0488 - val_loss: 84619706368.0000\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 12225602560.00000\n",
            "Epoch 345/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 85648046729.3659 - val_loss: 84292452352.0000\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 12225602560.00000\n",
            "Epoch 346/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 72881856512.0000 - val_loss: 83980861440.0000\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 12225602560.00000\n",
            "Epoch 347/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 80292799213.2683 - val_loss: 83657072640.0000\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 12225602560.00000\n",
            "Epoch 348/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 73483421471.2195 - val_loss: 83356368896.0000\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 12225602560.00000\n",
            "Epoch 349/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 63534362723.9024 - val_loss: 83033276416.0000\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 12225602560.00000\n",
            "Epoch 350/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 81720311108.6829 - val_loss: 82717966336.0000\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 12225602560.00000\n",
            "Epoch 351/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 62899372281.7561 - val_loss: 82405490688.0000\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 12225602560.00000\n",
            "Epoch 352/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 73112404817.1707 - val_loss: 82106564608.0000\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 12225602560.00000\n",
            "Epoch 353/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 71547881072.3902 - val_loss: 81762967552.0000\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 12225602560.00000\n",
            "Epoch 354/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 73394736103.0244 - val_loss: 81458397184.0000\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 12225602560.00000\n",
            "Epoch 355/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 73114468202.1463 - val_loss: 81131634688.0000\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 12225602560.00000\n",
            "Epoch 356/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 69349982208.0000 - val_loss: 80825802752.0000\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 12225602560.00000\n",
            "Epoch 357/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 75417949109.0732 - val_loss: 80522084352.0000\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 12225602560.00000\n",
            "Epoch 358/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 67418083328.0000 - val_loss: 80227778560.0000\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 12225602560.00000\n",
            "Epoch 359/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68501801359.6098 - val_loss: 79889907712.0000\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 12225602560.00000\n",
            "Epoch 360/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 71362573262.0488 - val_loss: 79583133696.0000\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 12225602560.00000\n",
            "Epoch 361/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 73103709358.8293 - val_loss: 79263383552.0000\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 12225602560.00000\n",
            "Epoch 362/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 80136159631.6098 - val_loss: 78971756544.0000\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 12225602560.00000\n",
            "Epoch 363/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 67134721748.2927 - val_loss: 78665228288.0000\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 12225602560.00000\n",
            "Epoch 364/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 69239106834.7317 - val_loss: 78344560640.0000\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 12225602560.00000\n",
            "Epoch 365/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 62186870883.9024 - val_loss: 78071373824.0000\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 12225602560.00000\n",
            "Epoch 366/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 77842614022.2439 - val_loss: 77751705600.0000\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 12225602560.00000\n",
            "Epoch 367/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 72131075596.4878 - val_loss: 77466828800.0000\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 12225602560.00000\n",
            "Epoch 368/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 76116616466.7317 - val_loss: 77162962944.0000\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 12225602560.00000\n",
            "Epoch 369/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 67619474656.7805 - val_loss: 76863561728.0000\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 12225602560.00000\n",
            "Epoch 370/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 69769408012.4878 - val_loss: 76585410560.0000\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 12225602560.00000\n",
            "Epoch 371/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68041545628.0976 - val_loss: 76289933312.0000\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 12225602560.00000\n",
            "Epoch 372/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 71239379143.8049 - val_loss: 75997888512.0000\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 12225602560.00000\n",
            "Epoch 373/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 76070737845.0732 - val_loss: 75702083584.0000\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 12225602560.00000\n",
            "Epoch 374/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 64573299736.9756 - val_loss: 75440209920.0000\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 12225602560.00000\n",
            "Epoch 375/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 65555483872.7805 - val_loss: 75123228672.0000\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 12225602560.00000\n",
            "Epoch 376/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 64124562307.1219 - val_loss: 74843201536.0000\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 12225602560.00000\n",
            "Epoch 377/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68658339889.9512 - val_loss: 74561232896.0000\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 12225602560.00000\n",
            "Epoch 378/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 64766012990.4390 - val_loss: 74287718400.0000\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 12225602560.00000\n",
            "Epoch 379/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 62322136788.2927 - val_loss: 74014457856.0000\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 12225602560.00000\n",
            "Epoch 380/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 58234499022.0488 - val_loss: 73748430848.0000\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 12225602560.00000\n",
            "Epoch 381/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 64610850066.7317 - val_loss: 73458581504.0000\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 12225602560.00000\n",
            "Epoch 382/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 68197125095.0244 - val_loss: 73206931456.0000\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 12225602560.00000\n",
            "Epoch 383/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 67686151242.9268 - val_loss: 72941035520.0000\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 12225602560.00000\n",
            "Epoch 384/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 59127328518.2439 - val_loss: 72686403584.0000\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 12225602560.00000\n",
            "Epoch 385/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 69225048213.8537 - val_loss: 72408891392.0000\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 12225602560.00000\n",
            "Epoch 386/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 71812386566.2439 - val_loss: 72181293056.0000\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 12225602560.00000\n",
            "Epoch 387/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 69373819029.8537 - val_loss: 71905902592.0000\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 12225602560.00000\n",
            "Epoch 388/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 63668660323.9024 - val_loss: 71655825408.0000\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 12225602560.00000\n",
            "Epoch 389/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60161905539.1219 - val_loss: 71412383744.0000\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 12225602560.00000\n",
            "Epoch 390/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 73649896972.4878 - val_loss: 71175020544.0000\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 12225602560.00000\n",
            "Epoch 391/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 64296004533.0732 - val_loss: 70930702336.0000\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 12225602560.00000\n",
            "Epoch 392/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 70698736015.6098 - val_loss: 70698508288.0000\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 12225602560.00000\n",
            "Epoch 393/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 68544027797.8537 - val_loss: 70453436416.0000\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 12225602560.00000\n",
            "Epoch 394/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61112444828.0976 - val_loss: 70232162304.0000\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 12225602560.00000\n",
            "Epoch 395/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 75931672576.0000 - val_loss: 69991940096.0000\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 12225602560.00000\n",
            "Epoch 396/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 66604994110.4390 - val_loss: 69786796032.0000\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 12225602560.00000\n",
            "Epoch 397/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 58096754688.0000 - val_loss: 69546164224.0000\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 12225602560.00000\n",
            "Epoch 398/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 64642155944.5854 - val_loss: 69308801024.0000\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 12225602560.00000\n",
            "Epoch 399/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 62725447929.7561 - val_loss: 69112995840.0000\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 12225602560.00000\n",
            "Epoch 400/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 51937945949.6585 - val_loss: 68897316864.0000\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 12225602560.00000\n",
            "Epoch 401/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 57748232866.3415 - val_loss: 68677201920.0000\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 12225602560.00000\n",
            "Epoch 402/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61224846261.0732 - val_loss: 68456767488.0000\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 12225602560.00000\n",
            "Epoch 403/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 62027869658.5366 - val_loss: 68245708800.0000\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 12225602560.00000\n",
            "Epoch 404/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 66991810809.7561 - val_loss: 68040626176.0000\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 12225602560.00000\n",
            "Epoch 405/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 65000032655.6098 - val_loss: 67850088448.0000\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 12225602560.00000\n",
            "Epoch 406/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 54098200925.6585 - val_loss: 67645415424.0000\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 12225602560.00000\n",
            "Epoch 407/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 57784354716.0976 - val_loss: 67448074240.0000\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 12225602560.00000\n",
            "Epoch 408/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 59486228130.3415 - val_loss: 67221356544.0000\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 12225602560.00000\n",
            "Epoch 409/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 55947703021.2683 - val_loss: 67028869120.0000\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 12225602560.00000\n",
            "Epoch 410/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 65495849359.6098 - val_loss: 66846461952.0000\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 12225602560.00000\n",
            "Epoch 411/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 68879854766.8293 - val_loss: 66675257344.0000\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 12225602560.00000\n",
            "Epoch 412/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60107369297.1707 - val_loss: 66502426624.0000\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 12225602560.00000\n",
            "Epoch 413/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 53907243307.7073 - val_loss: 66306564096.0000\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 12225602560.00000\n",
            "Epoch 414/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 65682225801.3659 - val_loss: 66122674176.0000\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 12225602560.00000\n",
            "Epoch 415/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 53674600348.0976 - val_loss: 65936289792.0000\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 12225602560.00000\n",
            "Epoch 416/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 55991839719.0244 - val_loss: 65774137344.0000\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 12225602560.00000\n",
            "Epoch 417/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60322617493.8537 - val_loss: 65587048448.0000\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 12225602560.00000\n",
            "Epoch 418/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 62790426474.1463 - val_loss: 65405353984.0000\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 12225602560.00000\n",
            "Epoch 419/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60661671586.3415 - val_loss: 65255731200.0000\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 12225602560.00000\n",
            "Epoch 420/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61813861700.6829 - val_loss: 65076920320.0000\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 12225602560.00000\n",
            "Epoch 421/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 63224581294.8293 - val_loss: 64915525632.0000\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 12225602560.00000\n",
            "Epoch 422/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 57177460386.3415 - val_loss: 64759222272.0000\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 12225602560.00000\n",
            "Epoch 423/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56876049832.5854 - val_loss: 64607154176.0000\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 12225602560.00000\n",
            "Epoch 424/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 53320147143.8049 - val_loss: 64447467520.0000\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 12225602560.00000\n",
            "Epoch 425/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 59282186389.8537 - val_loss: 64296820736.0000\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 12225602560.00000\n",
            "Epoch 426/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 62294402522.5366 - val_loss: 64139735040.0000\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 12225602560.00000\n",
            "Epoch 427/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 63100062794.9268 - val_loss: 63966646272.0000\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 12225602560.00000\n",
            "Epoch 428/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 65435332707.9024 - val_loss: 63806365696.0000\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 12225602560.00000\n",
            "Epoch 429/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 53853910740.2927 - val_loss: 63668563968.0000\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 12225602560.00000\n",
            "Epoch 430/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 53675342423.4146 - val_loss: 63523282944.0000\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 12225602560.00000\n",
            "Epoch 431/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61201308047.6098 - val_loss: 63379439616.0000\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 12225602560.00000\n",
            "Epoch 432/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56705162364.8781 - val_loss: 63226068992.0000\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 12225602560.00000\n",
            "Epoch 433/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 65374245088.7805 - val_loss: 63085670400.0000\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 12225602560.00000\n",
            "Epoch 434/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 57345605981.6585 - val_loss: 62939045888.0000\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 12225602560.00000\n",
            "Epoch 435/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 62307108014.8293 - val_loss: 62798721024.0000\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 12225602560.00000\n",
            "Epoch 436/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 54554652921.7561 - val_loss: 62678851584.0000\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 12225602560.00000\n",
            "Epoch 437/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50046582584.1951 - val_loss: 62526840832.0000\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 12225602560.00000\n",
            "Epoch 438/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 60250159603.5122 - val_loss: 62392057856.0000\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 12225602560.00000\n",
            "Epoch 439/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 56114867824.3902 - val_loss: 62263910400.0000\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 12225602560.00000\n",
            "Epoch 440/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 60180254720.0000 - val_loss: 62135496704.0000\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 12225602560.00000\n",
            "Epoch 441/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54206686083.1219 - val_loss: 62015209472.0000\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 12225602560.00000\n",
            "Epoch 442/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 53461185785.7561 - val_loss: 61886447616.0000\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 12225602560.00000\n",
            "Epoch 443/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 67720543306.9268 - val_loss: 61755543552.0000\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 12225602560.00000\n",
            "Epoch 444/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 64438512864.7805 - val_loss: 61626101760.0000\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 12225602560.00000\n",
            "Epoch 445/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 62474397795.9024 - val_loss: 61517938688.0000\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 12225602560.00000\n",
            "Epoch 446/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 64867461819.3171 - val_loss: 61368393728.0000\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 12225602560.00000\n",
            "Epoch 447/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 58215549627.3171 - val_loss: 61267476480.0000\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 12225602560.00000\n",
            "Epoch 448/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 51090945673.3659 - val_loss: 61132935168.0000\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 12225602560.00000\n",
            "Epoch 449/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 58232360110.8293 - val_loss: 61022040064.0000\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 12225602560.00000\n",
            "Epoch 450/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54435407222.6341 - val_loss: 60910014464.0000\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 12225602560.00000\n",
            "Epoch 451/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 62271254827.7073 - val_loss: 60820025344.0000\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 12225602560.00000\n",
            "Epoch 452/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 52927754140.0976 - val_loss: 60700295168.0000\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 12225602560.00000\n",
            "Epoch 453/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 53144314255.6098 - val_loss: 60575870976.0000\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 12225602560.00000\n",
            "Epoch 454/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 53254422028.4878 - val_loss: 60470956032.0000\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 12225602560.00000\n",
            "Epoch 455/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 61543391481.7561 - val_loss: 60358737920.0000\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 12225602560.00000\n",
            "Epoch 456/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54140495672.1951 - val_loss: 60254162944.0000\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 12225602560.00000\n",
            "Epoch 457/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 61666547512.1951 - val_loss: 60171730944.0000\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 12225602560.00000\n",
            "Epoch 458/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54665535388.0976 - val_loss: 60053004288.0000\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 12225602560.00000\n",
            "Epoch 459/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54529150426.5366 - val_loss: 59943116800.0000\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 12225602560.00000\n",
            "Epoch 460/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 50869083660.4878 - val_loss: 59819835392.0000\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 12225602560.00000\n",
            "Epoch 461/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 52169045566.4390 - val_loss: 59729248256.0000\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 12225602560.00000\n",
            "Epoch 462/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 58041620579.9024 - val_loss: 59649720320.0000\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 12225602560.00000\n",
            "Epoch 463/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 48581932206.8293 - val_loss: 59533856768.0000\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 12225602560.00000\n",
            "Epoch 464/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 60019722989.2683 - val_loss: 59437125632.0000\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 12225602560.00000\n",
            "Epoch 465/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 51555193980.8781 - val_loss: 59367395328.0000\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 12225602560.00000\n",
            "Epoch 466/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 47667073823.2195 - val_loss: 59238998016.0000\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 12225602560.00000\n",
            "Epoch 467/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 54364208502.6341 - val_loss: 59148218368.0000\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 12225602560.00000\n",
            "Epoch 468/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54256509927.0244 - val_loss: 59044892672.0000\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 12225602560.00000\n",
            "Epoch 469/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 52754473759.2195 - val_loss: 58965827584.0000\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 12225602560.00000\n",
            "Epoch 470/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 50552104160.7805 - val_loss: 58870734848.0000\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 12225602560.00000\n",
            "Epoch 471/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 66450409871.6098 - val_loss: 58777219072.0000\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 12225602560.00000\n",
            "Epoch 472/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 53733616265.3659 - val_loss: 58700271616.0000\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 12225602560.00000\n",
            "Epoch 473/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 47107467538.7317 - val_loss: 58587471872.0000\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 12225602560.00000\n",
            "Epoch 474/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 52709647085.2683 - val_loss: 58517762048.0000\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 12225602560.00000\n",
            "Epoch 475/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54346943488.0000 - val_loss: 58414510080.0000\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 12225602560.00000\n",
            "Epoch 476/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 54085841145.7561 - val_loss: 58346803200.0000\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 12225602560.00000\n",
            "Epoch 477/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 59593584340.2927 - val_loss: 58246328320.0000\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 12225602560.00000\n",
            "Epoch 478/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 47847621107.5122 - val_loss: 58149928960.0000\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 12225602560.00000\n",
            "Epoch 479/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 48948130666.1463 - val_loss: 58080481280.0000\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 12225602560.00000\n",
            "Epoch 480/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 49747822492.0976 - val_loss: 57992683520.0000\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 12225602560.00000\n",
            "Epoch 481/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 51632919102.4390 - val_loss: 57906700288.0000\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 12225602560.00000\n",
            "Epoch 482/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 49694850522.5366 - val_loss: 57820590080.0000\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 12225602560.00000\n",
            "Epoch 483/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56173846328.1951 - val_loss: 57753772032.0000\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 12225602560.00000\n",
            "Epoch 484/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 63694324161.5610 - val_loss: 57679343616.0000\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 12225602560.00000\n",
            "Epoch 485/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 65759104174.8293 - val_loss: 57624625152.0000\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 12225602560.00000\n",
            "Epoch 486/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 48003616268.4878 - val_loss: 57509756928.0000\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 12225602560.00000\n",
            "Epoch 487/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50920924234.9268 - val_loss: 57437523968.0000\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 12225602560.00000\n",
            "Epoch 488/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 57260385155.1219 - val_loss: 57361735680.0000\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 12225602560.00000\n",
            "Epoch 489/500\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 56321630982.2439 - val_loss: 57302847488.0000\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 12225602560.00000\n",
            "Epoch 490/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 48689731484.0976 - val_loss: 57236582400.0000\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 12225602560.00000\n",
            "Epoch 491/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50885022695.0244 - val_loss: 57126174720.0000\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 12225602560.00000\n",
            "Epoch 492/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 50347110999.4146 - val_loss: 57056641024.0000\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 12225602560.00000\n",
            "Epoch 493/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 51664153150.4390 - val_loss: 56980029440.0000\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 12225602560.00000\n",
            "Epoch 494/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 55272895712.7805 - val_loss: 56927227904.0000\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 12225602560.00000\n",
            "Epoch 495/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 46813922204.0976 - val_loss: 56843415552.0000\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 12225602560.00000\n",
            "Epoch 496/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 55544542732.4878 - val_loss: 56784257024.0000\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 12225602560.00000\n",
            "Epoch 497/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 52590458979.9024 - val_loss: 56704872448.0000\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 12225602560.00000\n",
            "Epoch 498/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 51866532289.5610 - val_loss: 56626098176.0000\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 12225602560.00000\n",
            "Epoch 499/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 56265527995.3171 - val_loss: 56553598976.0000\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 12225602560.00000\n",
            "Epoch 500/500\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 57820904572.8781 - val_loss: 56502599680.0000\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 12225602560.00000\n",
            "[CV 5/5; 10/10] END batch_size=128, epochs=500, learning_rate=5e-05, nn1=32, nn2=64, nn3=256;, score=0.610 total time= 2.5min\n",
            "Epoch 1/300\n",
            "51/51 [==============================] - 1s 6ms/step - loss: 295258847074.4615 - val_loss: 61324357632.0000\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 12225602560.00000\n",
            "Epoch 2/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 64804058663.3846 - val_loss: 39384633344.0000\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 12225602560.00000\n",
            "Epoch 3/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 52753439271.3846 - val_loss: 34391011328.0000\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 12225602560.00000\n",
            "Epoch 4/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 39611840354.4615 - val_loss: 33738153984.0000\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 12225602560.00000\n",
            "Epoch 5/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 36959571062.1538 - val_loss: 32966223872.0000\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 12225602560.00000\n",
            "Epoch 6/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 36397692258.4615 - val_loss: 30672207872.0000\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 12225602560.00000\n",
            "Epoch 7/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 35769675815.3846 - val_loss: 29824948224.0000\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 12225602560.00000\n",
            "Epoch 8/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 31185360068.9231 - val_loss: 28631746560.0000\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 12225602560.00000\n",
            "Epoch 9/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 29593485351.3846 - val_loss: 26582392832.0000\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 12225602560.00000\n",
            "Epoch 10/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 29643628386.4615 - val_loss: 31009421312.0000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 12225602560.00000\n",
            "Epoch 11/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 29645707421.5385 - val_loss: 30032537600.0000\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 12225602560.00000\n",
            "Epoch 12/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 31825796056.6154 - val_loss: 23408216064.0000\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 12225602560.00000\n",
            "Epoch 13/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 25358270464.0000 - val_loss: 22237104128.0000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 12225602560.00000\n",
            "Epoch 14/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 26800702070.1538 - val_loss: 20856483840.0000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 12225602560.00000\n",
            "Epoch 15/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 25046490663.3846 - val_loss: 26634227712.0000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 12225602560.00000\n",
            "Epoch 16/300\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 25251349622.1538 - val_loss: 20637630464.0000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 12225602560.00000\n",
            "Epoch 17/300\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 23527301316.9231 - val_loss: 20382400512.0000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 12225602560.00000\n",
            "Epoch 18/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 24265484209.2308 - val_loss: 22255958016.0000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 12225602560.00000\n",
            "Epoch 19/300\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 25274493755.0769 - val_loss: 20445743104.0000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 12225602560.00000\n",
            "Epoch 20/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 22800371908.9231 - val_loss: 20082393088.0000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 12225602560.00000\n",
            "Epoch 21/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 20882943330.4615 - val_loss: 18075545600.0000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 12225602560.00000\n",
            "Epoch 22/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 19517163677.5385 - val_loss: 17996713984.0000\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 12225602560.00000\n",
            "Epoch 23/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 20554460317.5385 - val_loss: 18687500288.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 12225602560.00000\n",
            "Epoch 24/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 20712826446.7692 - val_loss: 18536806400.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 12225602560.00000\n",
            "Epoch 25/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 17897834180.9231 - val_loss: 17489610752.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 12225602560.00000\n",
            "Epoch 26/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 19730952664.6154 - val_loss: 20400918528.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 12225602560.00000\n",
            "Epoch 27/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 20111562673.2308 - val_loss: 18372573184.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 12225602560.00000\n",
            "Epoch 28/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 18090135611.0769 - val_loss: 18292654080.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 12225602560.00000\n",
            "Epoch 29/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 20320552684.3077 - val_loss: 18076870656.0000\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 12225602560.00000\n",
            "Epoch 30/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 19863546013.5385 - val_loss: 15664868352.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 12225602560.00000\n",
            "Epoch 31/300\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 17587875721.8462 - val_loss: 16734151680.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 12225602560.00000\n",
            "Epoch 32/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 18282681088.0000 - val_loss: 17846886400.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 12225602560.00000\n",
            "Epoch 33/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 18285756022.1538 - val_loss: 32059308032.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 12225602560.00000\n",
            "Epoch 34/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 21593411072.0000 - val_loss: 16294745088.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 12225602560.00000\n",
            "Epoch 35/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 16620618653.5385 - val_loss: 22084911104.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 12225602560.00000\n",
            "Epoch 36/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 18723585536.0000 - val_loss: 16669066240.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 12225602560.00000\n",
            "Epoch 37/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 18421392384.0000 - val_loss: 16300232704.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 12225602560.00000\n",
            "Epoch 38/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 18698530422.1538 - val_loss: 16732539904.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 12225602560.00000\n",
            "Epoch 39/300\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 18765350754.4615 - val_loss: 15535915008.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 12225602560.00000\n",
            "Epoch 40/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 15870188484.9231 - val_loss: 14670230528.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 12225602560.00000\n",
            "Epoch 41/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 16215356632.6154 - val_loss: 15823613952.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 12225602560.00000\n",
            "Epoch 42/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 16993941504.0000 - val_loss: 15584568320.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 12225602560.00000\n",
            "Epoch 43/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 16913413828.9231 - val_loss: 15041297408.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 12225602560.00000\n",
            "Epoch 44/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 15371826491.0769 - val_loss: 14788129792.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 12225602560.00000\n",
            "Epoch 45/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 16051822237.5385 - val_loss: 15293838336.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 12225602560.00000\n",
            "Epoch 46/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 15778611515.0769 - val_loss: 15198613504.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 12225602560.00000\n",
            "Epoch 47/300\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 16667378924.3077 - val_loss: 16288051200.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 12225602560.00000\n",
            "Epoch 48/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 16797103084.3077 - val_loss: 14132779008.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 12225602560.00000\n",
            "Epoch 49/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 14948073491.6923 - val_loss: 14833055744.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 12225602560.00000\n",
            "Epoch 50/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 15627525828.9231 - val_loss: 15311171584.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 12225602560.00000\n",
            "Epoch 51/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 15414984290.4615 - val_loss: 15291759616.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 12225602560.00000\n",
            "Epoch 52/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 15550070390.1538 - val_loss: 15498561536.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 12225602560.00000\n",
            "Epoch 53/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 15115841280.0000 - val_loss: 15258108928.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 12225602560.00000\n",
            "Epoch 54/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 16554025649.2308 - val_loss: 15968730112.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 12225602560.00000\n",
            "Epoch 55/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 15495035707.0769 - val_loss: 15674217472.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 12225602560.00000\n",
            "Epoch 56/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 15529059012.9231 - val_loss: 15736082432.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 12225602560.00000\n",
            "Epoch 57/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 14326855680.0000 - val_loss: 14753679360.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 12225602560.00000\n",
            "Epoch 58/300\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 14745421646.7692 - val_loss: 21196599296.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 12225602560.00000\n",
            "\n",
            "\n",
            " Best Model After Randomized-Search Cross Validation: \n",
            "0.8713311548455905 {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 256}\n",
            "\n",
            "\n",
            "\n",
            "0.3243001372377109 {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 256}\n",
            "0.8713311548455905 {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 256}\n",
            "0.7327773600535006 {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 300, 'batch_size': 256}\n",
            "0.667976465397785 {'nn3': 256, 'nn2': 256, 'nn1': 16, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 512}\n",
            "-0.7948207606872705 {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 300, 'batch_size': 256}\n",
            "0.6812032657386788 {'nn3': 256, 'nn2': 128, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 256}\n",
            "0.738953572066423 {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 0.005, 'epochs': 100, 'batch_size': 128}\n",
            "0.8648024139598108 {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 200, 'batch_size': 512}\n",
            "0.8629864233702864 {'nn3': 256, 'nn2': 64, 'nn1': 16, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 64}\n",
            "0.6089856994331048 {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 128}\n",
            "\n",
            "Mean Test Score:  0.3243001372377109  for Parameters:  [{'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 256, 'nn1': 16, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 128, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 256}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 0.005, 'epochs': 100, 'batch_size': 128}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 16, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 64}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 128}]\n",
            "\n",
            "Mean Test Score:  0.8713311548455905  for Parameters:  [{'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 256, 'nn1': 16, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 128, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 256}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 0.005, 'epochs': 100, 'batch_size': 128}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 16, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 64}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 128}]\n",
            "\n",
            "Mean Test Score:  0.7327773600535006  for Parameters:  [{'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 256, 'nn1': 16, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 128, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 256}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 0.005, 'epochs': 100, 'batch_size': 128}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 16, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 64}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 128}]\n",
            "\n",
            "Mean Test Score:  0.667976465397785  for Parameters:  [{'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 256, 'nn1': 16, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 128, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 256}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 0.005, 'epochs': 100, 'batch_size': 128}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 16, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 64}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 128}]\n",
            "\n",
            "Mean Test Score:  -0.7948207606872705  for Parameters:  [{'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 256, 'nn1': 16, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 128, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 256}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 0.005, 'epochs': 100, 'batch_size': 128}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 16, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 64}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 128}]\n",
            "\n",
            "Mean Test Score:  0.6812032657386788  for Parameters:  [{'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 256, 'nn1': 16, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 128, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 256}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 0.005, 'epochs': 100, 'batch_size': 128}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 16, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 64}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 128}]\n",
            "\n",
            "Mean Test Score:  0.738953572066423  for Parameters:  [{'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 256, 'nn1': 16, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 128, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 256}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 0.005, 'epochs': 100, 'batch_size': 128}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 16, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 64}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 128}]\n",
            "\n",
            "Mean Test Score:  0.8648024139598108  for Parameters:  [{'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 256, 'nn1': 16, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 128, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 256}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 0.005, 'epochs': 100, 'batch_size': 128}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 16, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 64}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 128}]\n",
            "\n",
            "Mean Test Score:  0.8629864233702864  for Parameters:  [{'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 256, 'nn1': 16, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 128, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 256}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 0.005, 'epochs': 100, 'batch_size': 128}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 16, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 64}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 128}]\n",
            "\n",
            "Mean Test Score:  0.6089856994331048  for Parameters:  [{'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 256}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 256, 'nn1': 16, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 300, 'batch_size': 256}, {'nn3': 256, 'nn2': 128, 'nn1': 32, 'learning_rate': 0.0005, 'epochs': 200, 'batch_size': 256}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 0.005, 'epochs': 100, 'batch_size': 128}, {'nn3': 512, 'nn2': 256, 'nn1': 32, 'learning_rate': 0.05, 'epochs': 200, 'batch_size': 512}, {'nn3': 256, 'nn2': 64, 'nn1': 16, 'learning_rate': 0.05, 'epochs': 300, 'batch_size': 64}, {'nn3': 256, 'nn2': 64, 'nn1': 32, 'learning_rate': 5e-05, 'epochs': 500, 'batch_size': 128}]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3jGEzV-BVcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a8a649c-3ea0-44e5-da6b-e155b2b00591"
      },
      "source": [
        "# Computo delle predizioni sul dataset di Test\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "print('MAPE: ', mean_absolute_percentage_error(y_test, predictions))\n",
        "print('Variance Regression Score: ', explained_variance_score(y_test, predictions))\n",
        "\n",
        "# Normalizza le feature del record per la predizione out-of-sample\n",
        "house_to_predict = scaler.transform(house_to_predict.values.reshape(-1, X.shape[1]))\n",
        "\n",
        "# Esegue la predizione out-of-sample e stampa il prezzo predetto\n",
        "print('\\nPredicted Price:', model.predict(house_to_predict)[0, 0])\n",
        "\n",
        "# Stampa del prezzo reale\n",
        "print('\\nOriginal Price:', df.iloc[0]['price'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAPE:  0.14758108760203584\n",
            "Variance Regression Score:  0.8802752071248292\n",
            "\n",
            "Predicted Price: 271296.06\n",
            "\n",
            "Original Price: 221900.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}